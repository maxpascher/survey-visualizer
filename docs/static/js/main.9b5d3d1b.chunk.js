(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{108:function(e){e.exports={metadata:{acronym:"RMI",name:"Robot Motion Intent Survey",title_text:"How to Communicate Robot Motion Intent: A Scoping Review",citation_text:"@inproceedings{Pascher.2023-RMI, \n title={{How to Communicate Robot Motion Intent: A Scoping Review}}, \n author={Max Pascher and Uwe Gruenefeld and\n\t Stefan Schneegass and Jens Gerken}, \n booktitle={Proceedings of the 2023 CHI Conference\n\t on Human Factors in Computing Systems (CHI '23)}, \n year={2023}}",link_to_contribute:"https://github.com/maxpascher/survey-visualizer/issues/new/choose",link_to_code:"https://github.com/TathagataChakraborti/survey-visualizer",primary_link:"https://doi.org/10.1145/3544548.3580857",secondary_links:!1,community_link:!1,info_tile:!1,info_text:"Learn more about the VAM-HRI Workshop Series",info_link:[{link:"https://ojs.aaai.org//index.php/aimagazine/article/view/2822",text:"How it started"},{link:"https://vam-hri.github.io/",text:"How it's going"}]},link_to_server:"path.to.server",default_view:"Taxonomy",min_year:1984,views:[{name:"Taxonomy",disabled:!1,default_tab:"Intent Type",tabs:[{tab_name:"Intent Type",title_text:"Taxonomy View of Intent Type",disabled:!1,fancy_chart:!0,fancy_chart_default_level:2,input_file:{filename:"data/slug.xlsx",relative:!0,active_worksheet:"Master List"},papers_list:{key_map:{slug:0,title:3,abstract:5,authors:2,venue:4,year:1,link:6},rows:{start:5,stop:81}},taxonomy:{rows:{start:2,stop:4},columns:{start:9,stop:16}}},{tab_name:"Intent Information",title_text:"Taxonomy View of Intent Information",disabled:!1,fancy_chart:!0,fancy_chart_default_level:2,input_file:{filename:"data/slug.xlsx",relative:!0,active_worksheet:"Master List"},papers_list:{key_map:{slug:0,title:3,abstract:5,authors:2,venue:4,year:1,link:6},rows:{start:5,stop:81}},taxonomy:{rows:{start:1,stop:4},columns:{start:17,stop:23}}},{tab_name:"Intent Location",title_text:"Taxonomy View of Intent Location",disabled:!1,fancy_chart:!0,fancy_chart_default_level:2,input_file:{filename:"data/slug.xlsx",relative:!0,active_worksheet:"Master List"},papers_list:{key_map:{slug:0,title:3,abstract:5,authors:2,venue:4,year:1,link:6},rows:{start:5,stop:81}},taxonomy:{rows:{start:2,stop:4},columns:{start:24,stop:28}}}]},{name:"Affinity",disabled:!1},{name:"Network",files_directory:{location:"pdfs",relative:!0},match_threshold:.25,disabled:!1},{name:"Insights",disabled:!1}]}},179:function(e){e.exports=[{name:"Intent Type",data:[{UID:5,citations:[],selected:!0,slug:"paper_5",title:"Projecting robot intentions into human environments",abstract:"Trained human co-workers can often easily predict each other's intentions based on prior experience. When collaborating with a robot coworker, however, intentions are hard or impossible to infer. This difficulty of mental introspection makes human-robot collaboration challenging and can lead to dangerous misunderstandings. In this paper, we present a novel, object-aware projection technique that allows robots to visualize task information and intentions on physical objects in the environment. The approach uses modern object tracking methods in order to display information at specific spatial locations taking into account the pose and shape of surrounding objects. As a result, a human co-worker can be informed in a timely manner about the safety of the workspace, the site of next robot manipulation tasks, and next subtasks to perform. A preliminary usability study compares the approach to collaboration approaches based on monitors and printed text. The study indicates that, on average, the user effectiveness and satisfaction is higher with the projection based approach.",authors:"Andersen, Rasmus S.; Madsen, Ole; Moeslund, Thomas B.; Amor, Heni Ben",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745145",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"World Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"},{name:"Robot World Perception",parent:"State"},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:6,citations:[],selected:!0,slug:"paper_6",title:"Designing Multimodal Intent Communication Strategies for Conflict Avoidance in Industrial Human-Robot Teams",abstract:"Robot-to-human intent communication has been proposed as a method of enabling fluent coordination in human-robot teams. Prior research has focused on identifying modalities by which intent information can be accurately communicated, but has not yet studied whether intent communication enables fluent or safer coordination in human-robot teams in which intent communication is only supportive to the team's primary task. To address this question, we conduct a study (N = 29) in a mock collaborative manufacturing scenario in which motion-based and display-based intent communication approaches are evaluated under varying penalties for failing to coordinate safely. Subjective and objective measures of team fluency suggest that although intent communication supports fluent coordination, using a purely motion-based or a purely display-based approach may not be the most effective strategy. Although multimodal intent communication did not significantly improve upon unimodal approaches, merging both motion-based and display-based intent communication seems to combine the strengths of both approaches. Interestingly, results also suggest that contrary to theoretical predictions, the positive effect of intent communication is generally robust to teaming scenarios that require members to operate concurrently.",authors:"Aubert, Miles C.; Bader, Hayden; Hauser, Kris",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ROMAN.2018.8525557",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"},{name:"Attention",parent:"Intent Type"},{name:"Robot-Focused",parent:"Attention"}]},{UID:7,citations:[],selected:!0,slug:"paper_7",title:"Legible light communications for factory robots",abstract:"This work focuses on methods to improve mobile robot legibility in factories using lights. Implementation and evaluation were done at a robotics company that manufactures factory robots that work in human spaces. Three new sets of communicative lights were created and tested on the robots, integrated into the company's software stack and compared to the industry default lights that currently exist on the robots. All three newly designed light sets outperformed the industry default. Insights from this work have been integrated into software releases across North America.",authors:"Bacula, Alexandra; Mercer, Jason; Knight, Heather",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3371382.3378305",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:8,citations:[],selected:!0,slug:"paper_8",title:"Enhancing human understanding of a mobile robot\u2019s state and actions using expressive lights",abstract:"In order to be successfully integrated into human-populated environments, mobile robots need to express relevant information about their state to the outside world. In particular, animated lights are a promising way to express hidden robot state information such that it is visible at a distance. In this work, we present an online study to evaluate the effect of robot communication through expressive lights on people's understanding of the robot's state and actions. In our study, we use the CoBot mobile service robot with our light interface, designed to express relevant robot information to humans. We evaluate three designed light animations on three corresponding scenarios for each, for a total of nine scenarios. Our results suggest that expressive lights can play a significant role in helping people accurately hypothesize about a mobile robot's state and actions from afar when minimal contextual clues are present. We conclude that lights could be generally used as an effective non-verbal communication modality for mobile robots in the absence of, or as a complement to, other modalities.",authors:"Baraka, Kim; Rosenthal, Stephanie; Veloso, Manuela",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745187",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"},{name:"World-Centered",parent:"Instruction"}]},{UID:9,citations:[],selected:!0,slug:"paper_9",title:"A flexible optimization-based method for synthesizing intent-expressive robot arm motion",abstract:"We present an approach to synthesize robot arm trajectories that effectively communicate the robot\u2019s intent to a human collaborator while achieving task goals. Our approach uses nonlinear constrained optimization to encode task requirements and desired motion properties. Our implementation allows for a wide range of constraints and objectives. We introduce a novel objective function to optimize robot arm motions for intent-expressiveness that works in a range of scenarios and robot arm types. Our formulation supports experimentation with different theories of how viewers interpret robot motion. Through a series of human-subject experiments on real and simulated robots, we demonstrate that our method leads to improved collaborative performance against other methods, including the current state of the art. These experiments also show how our perception heuristic can affect collaborative outcomes.",authors:"Bodden, Christopher; Rakita, Daniel; Mutlu, Bilge; Gleicher, Michael",venue:"SAGE Publications",year:2018,link:"https://doi.org/10.1177/0278364918792295",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:10,citations:[],selected:!0,slug:"paper_10",title:"Transparent Robot Behavior by Adding Intuitive Visual and Acoustic Feedback to Motion Replanning",abstract:"Nowadays robots are able to work safely close to humans. They are light-weight, intrinsically safe and capable of avoiding obstacles as well as understand and predict human motions. In this collaborative scenario, the communication between humans and robots is a fundamental aspect to achieve good efficiency and ergonomics in the task execution. A lot of research has been made related to robot understanding and prediction of the human behavior, allowing the robot to replan its motion trajectories. This work is focused on the communication of the robot's intentions to the human to make its goals and planned trajectories easily understandable. Visual and acoustic information has been added to give the human an intuitive feedback to immediately understand the robot's plan. This allows a better interaction and makes the humans feel more comfortable, without any feeling of anxiety related to the unpredictability of the robot motion. Experiments have been conducted in a collaborative assembly scenario. The results of these tests were collected in questionnaires, in which the humans reported the differences and improvements they experienced using the feedback communication system.",authors:"Bolano, Gabriele; Roennau, Arne; Dillmann, Ruediger",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ROMAN.2018.8525671",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"Attention",parent:"Intent Type"},{name:"Robot-Focused",parent:"Attention"}]},{UID:11,citations:[],selected:!0,slug:"paper_11",title:"Deploying Multi-Modal Communication Using Augmented Reality in a Shared Workspace",abstract:"Robots are no longer working isolated in safety fences and Human-Robot Collaboration (HRC) is becoming one of the most promising topic of research to improve the efficiency in many application scenarios. Sharing the same workspace, both human and robot should clearly understand the intentions and motions of each other, in order to enable an efficient and effective interaction. In this work we propose an AR-based system to show the robot planned motion and target to the worker. We focused on representing this information in an intuitive way for inexperienced users. We introduced a multi-modal communication feedback in order to enable the user to agree with or change the robot plan using gestures and speech. The effectiveness of the system has been evaluated with test cases performed by a group of testers with no robotic experience. The results showed that the system helped the user to better understand the robot intentions and planned motion, improving the ergonomics and trust in the interaction. Furthermore, the evaluation included the rating of the different input modalities provided, in order to compare the different ways of communication proposed.",authors:"Bolano, Gabriele; Fu, Yuchao; Roennau, Arne; Dillmann, Ruediger",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/UR52253.2021.9494689",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:12,citations:[],selected:!0,slug:"paper_12",title:"Using Spatial and Temporal Contrast for Fluent Robot-Human Hand-Overs",abstract:"For robots to get integrated in daily tasks assisting humans, robot-human interactions will need to reach a level of fluency close to that of human-human interactions. In this paper we address the fluency of robot-human hand-overs. From an observational study with our robot HERB, we identify the key problems with a baseline hand-over action. We find that the failure to convey the intention of handing over causes delays in the transfer, while the lack of an intuitive signal to indicate timing of the hand-over causes early, unsuccessful attempts to take the object. We propose to address these problems with the use of spatial contrast, in the form of distinct hand-over poses, and temporal contrast, in the form of unambiguous transitions to the hand-over pose. We conduct a survey to identify distinct hand-over poses, and determine variables of the pose that have most communicative potential for the intent of handing over. We present an experiment that analyzes the effect of the two types of contrast on the fluency of hand-overs. We find that temporal contrast is particularly useful in improving fluency by eliminating early attempts of the human.",authors:"Cakmak, Maya; Srinivasa, Siddhartha S.; Lee, Min Kyung; Kiesler, Sara; Forlizzi, Jodi",venue:"ACM",year:2011,link:"https://doi.org/10.1145/1957656.1957823",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:13,citations:[],selected:!0,slug:"paper_13",title:"Communication Through Motion: Legibility of Multi-Robot Systems",abstract:"The interaction between a user and a multi-robot system in a shared environment is a relatively uncharted topic. But, as these types of systems will increase in the future years, an efficient way of communication is necessary. To this aim, it is interesting to discover if a multi-robot system can communicate its intentions exploiting only some motion-variables, which are characteristics of the motion of the robots. This study is about the legibility of a multi-robot system: in particular, we focus on the influence of these motion-variables on the legibility of more than one group of robots that move in a shared environment with the user. These motion-variables are: trajectory, dispersion and stiffness. They are generally used to define the motion of a group of mobile robots. Trajectory and dispersion were found relevant for the correctness of the communication between the user and the multi-robot system, while stiffness was found relevant for the rapidity of communication. The analysis of the influence of the motion-variables was carried out with an ANOVA (analysis of variance) based on a series of data coming from an experimental campaign conducted in a virtual reality set-up.",authors:"Capelli, Beatrice; Secchi, Cristian; Sabattini, Lorenzo",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/MRS.2019.8901100",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:14,citations:[],selected:!0,slug:"paper_14",title:"Emotion encoding in human-drone interaction",abstract:"Drones are becoming more popular and may soon be ubiquitous. As they enter our everyday environments, it becomes critical to ensure their usability through natural Human-Drone Interaction (HDI). Previous work in Human-Robot Interaction (HRI) shows that adding an emotional component is part of the key to success in robots' acceptability. We believe the adoption of personal drones would also benefit from adding an emotional component. This work defines a range of personality traits and emotional attributes that can be encoded in drones through their flight paths. We present a user study (N=20) and show how well three defined emotional states can be recognized. We draw conclusions on interaction techniques with drones and feedback strategies that use the drone's flight path and speed.",authors:"Cauchard, Jessica R.; Zhai, Kevin Y.; Spadafora, Marco; Landay, James A.",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/HRI.2016.7451761",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:15,citations:[],selected:!0,slug:"paper_15",title:"Using nonverbal signals to request help during human-robot collaboration",abstract:"Non-humanoid robots are becoming increasingly utilized for collaborative tasks that rely on each collaborator's ability to effectively convey their mental state while accurately estimating and interpreting their partner's knowledge, intent, and actions. During these tasks, it may be beneficial or even necessary for the human collaborator to assist the robot. Consequently, we explore the use of nonverbal signals to request help during a collaborative task. We focus on light and sound as they are commonly used communication channels across many domains. This paper analyzes the effectiveness of three nonverbal help signals that vary in urgency. Our results show that these signals significantly influence the human collaborator's and their perception of the collaboration.",authors:"Cha, Elizabeth; Mataric, Maja",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/IROS.2016.7759744",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"Robot-Focused",parent:"Attention"},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:16,citations:[],selected:!0,slug:"paper_16",title:"Bi-directional navigation intent communication using spatial augmented reality and eye-tracking glasses for improved safety in human\u2013robot interaction",abstract:"Safety, legibility and efficiency are essential for autonomous mobile robots that interact with humans. A key factor in this respect is bi-directional communication of navigation intent, which we focus on in this article with a particular view on industrial logistic applications. In the direction robot-to-human, we study how a robot can communicate its navigation intent using Spatial Augmented Reality (SAR) such that humans can intuitively understand the robot\u2019s intention and feel safe in the vicinity of robots. We conducted experiments with an autonomous forklift that projects various patterns on the shared floor space to convey its navigation intentions. We analyzed trajectories and eye gaze patterns of humans while interacting with an autonomous forklift and carried out stimulated recall interviews (SRI) in order to identify desirable features for projection of robot intentions. In the direction human-to-robot, we argue that robots in human co-habited environments need human-aware task and motion planning to support safety and efficiency, ideally responding to people\u2019s motion intentions as soon as they can be inferred from human cues. Eye gaze can convey information about intentions beyond what can be inferred from the trajectory and head pose of a person. Hence, we propose eye-tracking glasses as safety equipment in industrial environments shared by humans and robots. In this work, we investigate the possibility of human-to-robot implicit intention transference solely from eye gaze data and evaluate how the observed eye gaze patterns of the participants relate to their navigation decisions. We again analyzed trajectories and eye gaze patterns of humans while interacting with an autonomous forklift for clues that could reveal direction intent. Our analysis shows that people primarily gazed on that side of the robot they ultimately decided to pass by. We discuss implications of these results and relate to a control approach that uses human gaze for early obstacle avoidance.",authors:"Chadalavada, Ravi Teja; Andreasson, Henrik; Schindler, Maike; Palm, Rainer; Lilienthal, Achim J.",venue:"ScienceDirect",year:2020,link:"https://doi.org/10.1016/j.rcim.2019.101830",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:17,citations:[],selected:!0,slug:"paper_17",title:"Projection-Aware Task Planning and Execution for Human-in-the-Loop Operation of Robots in a Mixed-Reality Workspace",abstract:"Recent advances in mixed-reality technologies have renewed interest in alternative modes of communication for human-robot interaction. However, most of the work in this direction has been confined to tasks such as teleoperation, simulation or explication of individual actions of a robot. In this paper, we will discuss how the capability to project intentions affect the task planning capabilities of a robot. Specifically, we will start with a discussion on how projection actions can be used to reveal information regarding the future intentions of the robot at the time of task execution. We will then pose a new planning paradigm - projection-aware planning - whereby a robot can trade off its plan cost with its ability to reveal its intentions using its projection actions. We will demonstrate each of these scenarios with the help of a joint human-robot activity using the HoloLens.",authors:"Chakraborti, Tathagata; Sreedharan, Sarath; Kulkarni, Anagha; Kambhampati, Subbarao",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/IROS.2018.8593830",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"World Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot World Perception",parent:"State"},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:18,citations:[],selected:!0,slug:"paper_18",title:"Negotiation based Human-Robot Collaboration via Augmented Reality",abstract:"Effective human-robot collaboration (HRC) requires extensive communication among the human and robot teammates, because their actions can potentially produce conflicts, synergies, or both. We develop a novel augmented reality (AR) interface to bridge the communication gap between human and robot teammates. Building on our AR interface, we develop an AR-mediated, negotiation-based (ARN) framework for HRC. We have conducted experiments both in simulation and on real robots in an office environment, where multiple mobile robots work on delivery tasks. The robots could not complete the tasks on their own, but sometimes need help from their human teammate, rendering human-robot collaboration necessary. Results suggest that ARN significantly reduced the human-robot team's task completion time compared to a non-AR baseline approach.",authors:"Chandan, Kishan; Kudalkar, Vidisha; Li, Xiang; Zhang, Shiqi",venue:"arXiv",year:2019,link:"https://doi.org/10.48550/arXiv.1909.11227",tags:[{name:"Intent Type",parent:null},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:19,citations:[],selected:!0,slug:"paper_19",title:"Avoiding Human-Robot Collisions Using Haptic Communication",abstract:"Fully autonomous navigation in populated environments is still a challenging problem for mobile robots. This paper explores the idea of using active human-robot communication to facilitate navigation tasks. We propose to convey a robot's intent to human users via a wearable haptic interface. The interface can display distinct haptic cues by modulating vibration amplitudes and patterns. We applied the concept to a single human/single robot orthogonal encounter scenario, where one of the two parties has to yield the right of way to avoid collision. Under certain conditions, the robot's intent (to yield to the human or not) is revealed to the human via the haptic interface prior to the interaction. We conducted an experiment with 10 users, in which the robot was teleoperated as a substitute for autonomy. Results show that, when given priority, users become more risk-accepting and use different strategies to navigate the collision scenario than when the robot takes priority or there is no haptic communication channel. In addition, we propose a social-force based model to predict human movement during navigation. The effect of communication can be explained as a shift in the user's safety buffer and expectation of the robot's future velocity.",authors:"Che, Yuhang; Sun, Cuthbert T.; Okamura, Allison M.",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ICRA.2018.8460946",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"Robot-Focused",parent:"Attention"}]},{UID:20,citations:[],selected:!0,slug:"paper_20",title:"Efficient and Trustworthy Social Navigation via Explicit and Implicit Robot\u2013Human Communication",abstract:"In this article, we present a planning framework that uses a combination of implicit (robot motion) and explicit (visual/audio/haptic feedback) communication during mobile robot navigation. First, we developed a model that approximates both continuous movements and discrete behavior modes in human navigation, considering the effects of implicit and explicit communication on human decision-making. The model approximates the human as an optimal agent, with a reward function obtained through inverse reinforcement learning. Second, a planner uses this model to generate communicative actions that maximize the robot's transparency and efficiency. We implemented the planner on a mobile robot, using a wearable haptic device for explicit communication. In a user study of an indoor human-robot pair orthogonal crossing situation, the robot is able to actively communicate its intent to users in order to avoid collisions and facilitate efficient trajectories. Results show that the planner generated plans that are easier to understand, reduce users` effort, and increase users' trust of the robot, compared to simply performing collision avoidance. The key contribution of this article is the integration and analysis of explicit communication (together with implicit communication) for social navigation.",authors:"Che, Yuhang; Okamura, Allison M.; Sadigh, Dorsa",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/TRO.2020.2964824",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:21,citations:[],selected:!0,slug:"paper_21",title:"Touched by a Robot: An Investigation of Subjective Responses to Robot-Initiated Touch",abstract:"By initiating physical contact with people, robots can be more useful. For example, a robotic caregiver might make contact to provide physical assistance or facilitate communication. So as to better understand how people respond to robot-initiated touch, we conducted a 2x2 between-subjects experiment with 56 people in which a robotic nurse autonomously touched and wiped the subject\u2019s forearm. Our independent variables were whether or not the robot verbally warned the person before contact, and whether the robot verbally indicated that the touch was intended to clean the person\u2019s skin (instrumental touch) or to provide comfort (affective touch). On average, regardless of the treatment, participants had a generally positive subjective response. However, with instrumental touch people responded significantly more favorably. Since the physical behavior of the robot was the same for all trials, our results demonstrate that the perceived intent of the robot can significantly influence a person\u2019s subjective response to robot-initiated touch. Our results suggest that roboticists should consider this factor in addition to the mechanics of physical interaction. Unexpectedly, we found that participants tended to respond more favorably without a verbal warning. Although inconclusive, our results suggest that verbal warnings prior to contact should be carefully designed, if used at all.",authors:"Chen, Tiffany L.; King, Chih-Hung; Thomaz, Andrea L.; Kemp, Charles C.",venue:"ACM",year:2011,link:"https://doi.org/10.1145/1957656.1957818",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:22,citations:[],selected:!0,slug:"paper_22",title:"Dynamic Path Visualization for Human-Robot Collaboration",abstract:"Augmented reality technology can enable robots to visualize their future actions giving users crucial information to avoid collisions and other conflicting actions. Although a robot\u2019s entire action plan could be visualized (such as the output of a navigational planner), how far into the future it is appropriate to display the robot\u2019s plan is unknown. We developed a dynamic path visualizer that projects the robot\u2019s motion intent at varying lengths depending on the complexity of the upcoming path. We tested our approach in a virtual game where participants were tasked to collect and deliver gems to a robot that moves randomly towards a grid of markers in a confined area. Preliminary results on a small sample size indicate no significant effect on task performance; however, open-ended responses reveal participants preference towards visuals that show longer path projections.",authors:"Cleaver, Andre; Tang, Darren Vincent; Chen, Victoria; Short, Elaine Schaertl; Sinapov, Jivko",venue:"ACM",year:2021,link:"https://doi.org/10.1145/3434074.3447188",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:23,citations:[],selected:!0,slug:"paper_23",title:"MIRO: A Versatile Biomimetic Edutainment Robot",abstract:"Here we present MIRO, a companion robot designed to engage users in science and robotics via edutainment. MIRO is a robot that is biomimetic in aesthetics, morphology, behaviour, and control architecture. In this paper, we review how these design choices affect its suitability for a companionship role. In particular, we consider how MIRO\u2019s emulation of familiar mammalian body language as one component of a broader biomimetic expressive system provides effective communication of emotional state and intent. We go on to discuss how these features contribute to MIRO\u2019s potential in other domains such as healthcare, education, and research.",authors:"Collins, Emily C.; Prescott, Tony J.; Mitchinson, Ben; Conran, Sebastian",venue:"ACM",year:2015,link:"https://doi.org/10.1145/2832932.2832978",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:24,citations:[],selected:!0,slug:"paper_24",title:"Spatial augmented reality as a method for a mobile robot to communicate intended movement",abstract:"Our work evaluates a mobile robot\u2019s ability to communicate intended movements to humans via projection of visual arrows and a simplified map. Humans utilize a variety of techniques to signal intended movement in a co-occupied space. We evaluated an augmented reality projection provided by the robot. The projection is on the floor and consists of arrows and a simplified map. Two pilots and one quasi-experiment were conducted to examine the effectiveness of visual projection of arrows by a robot for signaling intended movement. The pilot work demonstrates the effectiveness of utilizing arrows as a communication medium. The experiment examined the effectiveness of a simplified map and arrows for signaling the short-, mid-range, and long-term intended movement. Two pilot experiments confirm that arrows are an effective symbol for a robot to use to signal intent. A field experiment demonstrates that a robot can use a projected arrow and simplified map to signal its intended movement and people understand the projection for upcoming short-, medium-, and long-term movement. Augmented reality, such as projected arrows and simplified map, are an effective tool for robots to use when signaling their upcoming movement to humans. Telepresence robots in organizations, museum docents, information kiosks, hospital assistants, factories, and as members of search and rescue teams are typical applications where mobile robots reside and interact with people.",authors:"Coovert, Michael D.; Lee, Tiffany; Shindev, Ivan; Sun, Yu",venue:"ScienceDirect",year:2014,link:"https://doi.org/10.1016/j.chb.2014.02.001",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot World Perception",parent:"State"}]},{UID:25,citations:[],selected:!0,slug:"paper_25",title:"Multimodal Interaction with an Autonomous Forklift",abstract:'We describe a multimodal framework for interacting with an autonomous robotic forklift. A key element enabling effective interaction is a wireless, handheld tablet with which a human supervisor can command the forklift using speech and sketch. Most current sketch interfaces treat the canvas as a blank slate. In contrast, our interface uses live and synthesized camera images from the forklift as a canvas, and augments them with object and obstacle information from the world. This connection enables users to "draw on the world," enabling a simpler set of sketched gestures. Our interface supports commands that include summoning the forklift and directing it to lift, transport, and place loads of palletized cargo. We describe an exploratory evaluation of the system designed to identify areas for detailed study.Our framework incorporates external signaling to interact with humans near the vehicle. The robot uses audible and visual annunciation to convey its current state and intended actions. The system also provides seamless autonomy handoff: any human can take control of the robot by entering its cabin, at which point the forklift can be operated manually until the human exits.',authors:"Correa, Andrew; Walter, Matthew R.; Fletcher, Luke; Glass, Jim; Teller, Seth; Davis, Randall",venue:"ACM",year:2010,link:"https://doi.org/10.1109/HRI.2010.5453188",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"},{name:"Robot World Perception",parent:"State"}]},{UID:26,citations:[],selected:!0,slug:"paper_26",title:"LED Strip Based Robot Movement Intention Signs for Human-Robot Interactions",abstract:"As a new kind of robots, called cooperative robots, are more commonly used in industry, a new way of communication is becoming more important due to the increasing number of closer cooperation between human and robots. This paper proposes the idea behind a novel method of using visual communication between cobots and humans focusing mainly on the field of industrial robotics. This device can decrease the mental stress experienced by the coworker and can increase the trust resulting in a closer to ergonomic workspace from the coworker viewpoint. Other possible usage is also discussed.",authors:"Domonkos, Mark; Dombi, Zoltan; Botzheim, Janos",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/CINTI51262.2020.9305854",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:27,citations:[],selected:!0,slug:"paper_27",title:"Generating legible motion",abstract:"Legible motion \u2014 motion that communicates its intent to a human observer \u2014 is crucial for enabling seamless human-robot collaboration. In this paper, we propose a functional gradient optimization technique for autonomously generating legible motion. Our algorithm optimizes a legibility metric inspired by the psychology of action interpretation in humans, resulting in motion trajectories that purposefully deviate from what an observer would expect in order to better convey intent. A trust region constraint on the optimization ensures that the motion does not become too surprising or unpredictable to the observer. Our studies with novice users that evaluate the resulting trajectories support the applicability of our method and of such a trust region. They show that within the region, legibility as measured in practice does significantly increase. Outside of it, however, the trajectory becomes confusing and the users\u2019 confidence in knowing the robot\u2019s intent significantly decreases.",authors:"Dragan, Anca and Srinivasa, Siddhartha",venue:"roboticsproceedings.org",year:2013,link:"http://www.roboticsproceedings.org/rss09/p24.pdf",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:28,citations:[],selected:!0,slug:"paper_28",title:"Effects of Robot Motion on Human-Robot Collaboration",abstract:"Most motion in robotics is purely functional, planned to achieve the goal and avoid collisions. Such motion is great in isolation, but collaboration affords a human who is watching the motion and making inferences about it, trying to coordinate with the robot to achieve the task. This paper analyzes the benefit of planning motion that explicitly enables the collaborator\u2019s inferences on the success of physical collaboration, as measured by both objective and subjective metrics. Results suggest that legible motion, planned to clearly express the robot\u2019s intent, leads to more fluent collaborations than predictable motion, planned to match the collaborator\u2019s expectations. Furthermore, purely functional motion can harm coordination, which negatively affects both task efficiency, as well as the participants\u2019 perception of the collaboration.",authors:"Dragan, Anca D.; Bauman, Shira; Forlizzi, Jodi; Srinivasa, Siddhartha S.",venue:"ACM",year:2015,link:"https://doi.org/10.1145/2696454.2696473",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:29,citations:[],selected:!0,slug:"paper_29",title:"Investigation of Communicative Flight Paths for Small Unmanned Aerial Systems * This work was supported by NSF NRI 1638099",abstract:"This project seeks to generate small Unmanned Aerial System (sUAS) flight paths that are broadly understood by the general population and can communicate states about both the sUAS and its understanding of the world. Previous work in sUAS flight paths has sought to communicate intent, destination, or emotion of the system without focusing on concrete states (e.g., low battery, landing, etc.). This work leverages biologically-based flight paths and experimental methodologies from human-human and human-humanoid robot interactions to assess the understanding of avian flight paths to communicate sUAS states to novice users. If successful, this work should inform: the human-robot interaction community about the perception of flight paths, sUAS manufacturers on how their systems could communicate with both operators and bystanders, and end users on ways to communicate with others when flying systems in public spaces. General design implications and future directions of work are suggested to build on the results here, which suggest that novice users gravitate towards labels they understand (draw attention and landing) while avoiding more technical labels (lost sensor).",authors:"Duncan, Brittany A.; Beachly, Evan; Bevins, Alisha; Elbaum, Sebasitan; Detweiler, Carrick",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ICRA.2018.8462871",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:30,citations:[],selected:!0,slug:"paper_30",title:"Follow me: Communicating intentions with a spherical robot",abstract:"In recent years, robots have gradually become incorporated in our society and therefore play more relevant role in social environments. These robots vary in form, some being more anthropomorphic than others. This, creates a need to study their interaction with the world. In this paper we used Sphero and BB-8, two robots with a simple spherical body devoid of verbal and other complex communication methods, to investigate how they can communicate intention to people. A set of behaviors based on pet behaviors was designed and tested in a controlled experiment, where the robot's aim was to convince a participant to follow it. We concluded that the use of these behaviors allows a robot to effectively communicate intention as well as create a bond with the participant, who would treat it as an equal, thereby engaging it in social interactions such as playing with it or talking to it.",authors:"Faria, Miguel; Costigliola, Andrea; Alves-Oliveira, Patricia; Paiva, Ana",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745189",tags:[{name:"Intent Type",parent:null},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:31,citations:[],selected:!0,slug:"paper_31",title:"\u201cMe and you together\u201d movement impact in multi-user collaboration tasks",abstract:"This paper presents a study on collaborative manipulation between an autonomous robot and multiple users. We investigate how different motion types impact people's ability to understand the robot's goals in a multi-user scenario. We propose an approach based on Collaborative Probabilistic Movement Primitives to generate the robot's movements, exploiting predictability and legibility of movement to express intentions through motion. We compare the impact on the interaction of using only either predictable or legible movements, and propose a third approach - hybrid motion - that selects, in each situation, whether to execute a predictable motion or a legible motion, depending on what the robot perceives as more efficient for the multi-user collaboration effort. To test the impact of the three motion types in the context of a collaborative task, we run a user study using a Baxter robot that autonomously serves cups of water to three users upon request. Our results show that, in the particular case where all users simultaneously request water, the hybrid motion performs better than the other two.",authors:"Faria, Miguel; Silva, Rui; Alves-Oliveira, Patricia; Melo, Francisco S.; Paiva, Ana",venue:"IEEE",year:2017,link:"https://doi.org/10.1109/IROS.2017.8206109",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:32,citations:[],selected:!0,slug:"paper_32",title:"Understanding Robots: Making Robots More Legible in Multi-Party Interactions",abstract:"In this work we explore implicit communication between humans and robots\u2014through movement\u2014in multi-party (or multi-user) interactions. In particular, we investigate how a robot can move to better convey its intentions using legible movements in multi-party interactions. Current research on the application of legible movements has focused on single-user interactions, causing a vacuum of knowledge regarding the impact of such movements in multi-party interactions. We propose a novel approach that extends the notion of legible motion to multi-party settings, by considering that legibility depends on all human users involved in the interaction, and should take into consideration how each of them perceives the robot\u2019s movements from their respective points-of-view. We show, through simulation and a user study, that our proposed model of multi-user legibility leads to movements that, on average, optimize the legibility of the motion as perceived by the group of users. Our model creates movements that allow each human to more quickly and confidently understand what are the robot\u2019s intentions, thus creating safer, clearer and more efficient interactions and collaborations.",authors:"Faria, Miguel; Melo, Francisco S.; Paiva, Ana",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/RO-MAN50785.2021.9515485",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:33,citations:[],selected:!0,slug:"paper_33",title:"Between legibility and contact: The role of gaze in robot approach",abstract:"In this paper, we explore experimentally the possible tradeoff between gaze to the user and gaze to the path in robot approach. While some previous work indicates that gaze towards the user increases perceived safety because the user feels recognized, other work indicates that it is legibility of the robot's actions that put users at ease. If the robot does not drive up to the person in a straight line directly, the robot can either continuously look at the person and thus maintain eye contact, or indicate its path through its gaze behavior, increasing legibility. In an experiment with N=36 participants, we tested the tradeoff between legibility and eye contact. The behavioral results show that users are significantly more at ease with the robot that gazes at them than with the robot that looks where it is going, measured by the number of instances of glances away from the robot. Likewise, the participants rate the robot that looks at them continuously as more intelligent and more cooperative. Thus, participants value mutual gaze higher than legibility.",authors:"Fischer, Kerstin; Jensen, Lars C.; Suvei, Stefan-Daniel; Bodenhagen, Leon",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745186",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:34,citations:[],selected:!0,slug:"paper_34",title:"Investigation of Unmanned Aerial Vehicle Gesture Perceptibility and Impact of Viewpoint Variance<sup>*</sup>",abstract:"Unmanned Aerial Vehicle (UAV) flight paths have been shown to communicate meaning to human observers, similar to human gestural communication. This paper presents the results of a UAV gesture perception study designed to assess how observer viewpoint perspective may impact how humans perceive the shape of UAV gestural motion. Robot gesture designers have demonstrated that robots can indeed communicate meaning through gesture; however, many of these results are limited to an idealized range of viewer perspectives and do not consider how the perception of a robot gesture may suffer from obfuscation or self-occlusion from some viewpoints. This paper presents the results of three online user-studies that examine participants' ability to accurately perceive the intended shape of two-dimensional UAV gestures from varying viewer perspectives. We used a logistic regression model to characterize participant gesture classification accuracy, demonstrating that viewer perspective does impact how participants perceive the shape of UAV gestures. Our results yielded a viewpoint angle threshold from beyond which participants were able to assess the intended shape of a gesture's motion with 90% accuracy. We also introduce a perceptibility score to capture user confidence, time to decision, and accuracy in labeling and to understand how differences in flight paths impact perception across viewpoints. These findings will enable UAV gesture systems that, with a high degree of confidence, ensure gesture motions can be accurately perceived by human observers.",authors:"Fletcher, Paul; Luther, Angeline; Duncan, Brittany; Detweiler, Carrick",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/ICRA48506.2021.9561094",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:35,citations:[],selected:!0,slug:"paper_35",title:"Augmenting the audio-based expression modality of a non-affective robot",abstract:"This paper investigates the potential benefits of augmenting audio-based affective means of expression to strengthen the perceived intentions of a robot. Robots are often viewed as being simple machines with limied capabilities of communication. Changing how a robot is perceived, towards a more affective interpretation of its intentions, requires careful consideration of the means of expression available to the robot. It also requires alignment between these means to ensure they work in coordination with each other to make the robot easier to understand. As an effort to strengthen the affective interpretation of a soft robotic arm robot, we altered its overall expression by changing the available audio-based expression modalities. The system mitigatedthe naturally occurring noise from actuators and pneumatic systems and used a custom sound that supported the movement of the robot. The robot was tested by interacting with human observers (n=78) and was perceived as being significantly more curious, happy and less angry when augmented by audio that aligned with the naturally occurred robot sounds. The results show that the audio-based expression modality of robots is a valuable communication tool to consider augmenting when designing robots that convey affective information.",authors:"Frederiksen, Morten Roed; Stoey, Kasper",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/ACII.2019.8925510",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:36,citations:[],selected:!0,slug:"paper_36",title:"Touch-based information transfer from a robot modeled on the hearing dog",abstract:"Research on physical human-robot interaction has been attracting attention recently, focusing on robot embodiment. The work reported here proposes Active Touch Communication Robot (AcToR), a robot that is modeled on the hearing dog. A hearing dog is a type of dog assist people who are deaf or hard of hearing by alerting their handler to important sounds. AcToR uses the sense of touch to notify a human of the intention to transfer information. For example, when AcToR detects that a cell phone that is in another location has received a call, AcToR moves to the user's location and makes contact with the user's body to notify the user of the incoming call. The AcToR robot is based on the Roomba<sup>\xae</sup> and uses the Roomba's bumper and contact sensors to detect contact. This paper reports the results of psychological experiments using the AcToR robot that indicate the feasibility of using touch to transfer information from a robot to a person.",authors:"Furuhashi, Michihiko; Nakamura, Tsuyoshi; Kanoh, Masayoshi; Yamada, Koji",venue:"IEEE",year:2015,link:"https://doi.org/10.1109/FUZZ-IEEE.2015.7337981",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"Robot-Focused",parent:"Attention"},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:37,citations:[],selected:!0,slug:"paper_37",title:"Generating anticipation in robot motion",abstract:"Robots that display anticipatory motion provide their human partners with greater time to respond in interactive tasks because human partners are aware of robot intent earlier. We create anticipatory motion autonomously from a single motion exemplar by extracting hand and body symbols that communicate motion intent and moving them earlier in the motion. We validate that our algorithm extracts the most salient frame (i.e. the correct symbol) which is the most informative about motion intent to human observers. Furthermore, we show that anticipatory variants allow humans to discern motion intent sooner than motions without anticipation, and that humans are able to reliably predict motion intent prior to the symbol frame when motion is anticipatory. Finally, we quantified the time range for robot motion when humans can perceive intent more accurately and the collaborative social benefits of anticipatory motion are greatest.",authors:"Gielniak, Michael J.; Thomaz, Andrea L.",venue:"IEEE",year:2011,link:"https://doi.org/10.1109/ROMAN.2011.6005255",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:38,citations:[],selected:!0,slug:"paper_38",title:"Robopal: Modeling Role Transitions in Human-Robot Interaction",abstract:"We have developed a new communication robot, Robopal, which is an indoor/outdoor robot for use in human-robot interaction research in the context of daily life. Robopal's intended applications involve leading and/or following a human to a destination. Preliminary experiments have been conducted to study nonverbal cues associated with leading and following behavior, and it has been observed that some behaviors, such as glancing towards the leader or follower, appear to be role-dependent. A system for representing these behaviors with a state transition model is described, based on four kinds of interaction roles: directive, responsive, collaborative, and independent. It is proposed that behavior modeling can be simplified by using this system to represent changes in the roles the robot and human play in an interaction, and by associating appropriate behaviors to each role",authors:"Glas, Dylan F.; Miyashita, Takahiro; Ishiguro, Hiroshi; Hagita, Norihiro",venue:"IEEE",year:2007,link:"https://doi.org/10.1109/ROBOT.2007.363636",tags:[{name:"Intent Type",parent:null},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:39,citations:[],selected:!0,slug:"paper_39",title:"Mind the ARm: Realtime Visualization of Robot Motion Intent in Head-Mounted Augmented Reality",abstract:"Established safety sensor technology shuts down industrial robots when a collision is detected, causing preventable loss of productivity. To minimize downtime, we implemented three Augmented Reality (AR) visualizations (Path, Preview, and Volume) which allow users to understand robot motion intent and give way to the robot. We compare the different visualizations in a user study in which a small cognitive task is performed in a shared workspace. We found that Preview and Path required significantly longer head rotations to perceive robot motion intent. Volume, however, required the shortest head rotation and was perceived as most safe, enabling closer proximity of the robot arm before one left the shared workspace without causing shutdowns.",authors:"Gruenefeld, Uwe; Pr\xe4del, Lars; Illing, Jannike; Stratmann, Tim; Drolshagen, Sandra; Pfingsthorn, Max",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3404983.3405509",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:40,citations:[],selected:!0,slug:"paper_40",title:"Seeing Thru Walls: Visualizing Mobile Robots in Augmented Reality",abstract:"We present an approach for visualizing mobile robots through an Augmented Reality headset when there is no line-of-sight visibility between the robot and the human. Three elements are visualized in Augmented Reality: 1) Robot\u2019s 3D model to indicate its position, 2) An arrow emanating from the robot to indicate its planned movement direction, and 3) A 2D grid to represent the ground plane. We conduct a user study with 18 participants, in which each participant are asked to retrieve objects, one at a time, from stations at the two sides of a T-junction at the end of a hallway where a mobile robot is roaming. The results show that visualizations improved the perceived safety and efficiency of the task and led to participants being more comfortable with the robot within their personal spaces. Furthermore, visualizing the motion intent in addition to the robot model was found to be more effective than visualizing the robot model alone. The proposed system can improve the safety of automated warehouses by increasing the visibility and predictability of robots.",authors:"Gu, Morris; Cosgun, Akansel; Chan, Wesley P.; Drummond, Tom; Croft, Elizabeth",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/RO-MAN50785.2021.9515322",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:41,citations:[],selected:!0,slug:"paper_41",title:"Projection mapping implementation: Enabling direct externalization of perception results and action intent to improve robot explainability",abstract:"Existing research on non-verbal cues, e.g., eye gaze or arm movement, may not accurately present a robot's internal states such as perception results and action intent. Projecting the states directly onto a robot's operating environment has the advantages of being direct, accurate, and more salient, eliminating mental inference about the robot's intention. However, there is a lack of tools for projection mapping in robotics, compared to established motion planning libraries (e.g., MoveIt). In this paper, we detail the implementation of projection mapping to enable researchers and practitioners to push the boundaries for better interaction between robots and humans. We also provide practical documentation and code for a sample manipulation projection mapping on GitHub: github.com/uml-robotics/projection_mapping.",authors:"Han, Zhao; Wilkinson, Alexander; Parrillo, Jenna; Allspaw, Jordan; Yanco, Holly A.",venue:"arXiv",year:2020,link:"https://doi.org/10.48550/arXiv.2010.02263\n",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot World Perception",parent:"State"}]},{UID:42,citations:[],selected:!0,slug:"paper_42",title:"Investigating the Effectiveness of Different Interaction Modalities for Spatial Human-Robot Interaction",abstract:"With the increasing use of social robots in real environments, one of the areas of research requiring more attention is the study of human-robot interaction (HRI) when a person and robot are moving close to each other. Understanding effective ways to design how a robot should communicate its intention during dynamic movement is based on what people\u2019s expectations are and how they interpret different cues from the robot. Building on the existing literature, we tested a range of non-verbal cues such as eye contact, gaze and head nodding as part of the robot\u2019s behaviour during close proximate passing. The research aimed to investigate the effects of these cues, as well as their combination with body posture, on the efficiency of passing and the quality of HRI. Our results show that the combination of eye contact and the robot turning sideways is the most effective and appropriate compared to other modalities.",authors:"He, Jinying; van Maris, Anouk; Caleb-Solly, Praminda",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3371382.3378273",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:43,citations:[],selected:!0,slug:"paper_43",title:"Hey Robot, Which Way Are You Going? Nonverbal Motion Legibility Cues for Human-Robot Spatial Interaction",abstract:"Mobile robots have recently been deployed in public spaces such as shopping malls, airports, and urban sidewalks. Most of these robots are designed with human-aware motion planning capabilities but are not designed to communicate with pedestrians. Pedestrians that encounter these robots without prior understanding of the robots' behaviour can experience discomfort, confusion, and delayed social acceptance. In this work we designed and evaluated nonverbal robot motion legibility cues, which communicate a mobile robot's motion intention to pedestrians. We compared a motion legibility cue using Projected Arrows to one using Flashing Lights. We designed the cues to communicate path information, goal information, or both, and explored different Robot Movement Scenarios. We conducted an online user study with 229 participants using videos of the motion legibility cues. Our results show that the absence of cues was not socially acceptable, and that Projected Arrows were the more socially acceptable cue in most experimental conditions. We conclude that the presence and choice of motion legibility cues can positively influence robots' acceptance and successful deployment in public spaces.",authors:"Hetherington, Nicholas J.; Croft, Elizabeth A.; van der Loos, H. MachielF.",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/LRA.2021.3068708",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:44,citations:[],selected:!0,slug:"paper_44",title:"Legible robot pointing",abstract:"Good communication is critical to seamless human-robot interaction. Among numerous communication channels, here we focus on gestures, and in particular on spacial deixis: pointing at objects in the environment in order to reference them. We propose a mathematical model that enables robots to generate pointing configurations that make the goal object as clear as possible - pointing configurations that are legible. We study the implications of legibility on pointing, e.g. that the robot will sometimes need to trade off efficiency for the sake of clarity. Finally, we test how well our model works in practice in a series of user studies, showing that the resulting pointing configurations make the goal object easier to infer for novice users.",authors:"Holladay, Rachel M.; Dragan, Anca D.; Srinivasa, Siddhartha S.",venue:"IEEE",year:2014,link:"https://doi.org/10.1109/ROMAN.2014.6926256",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:45,citations:[],selected:!0,slug:"paper_45",title:"Auditory display of directions and states for mobile systems",abstract:"Auditory displays for mobile systems, such as service robots, have been developed. The design of directional sounds and of additional sounds for robot states (e.g., Heavy Load), as well as the design of more complicated robot sound tracks are explained. Basic musical elements and robot movement sounds have been combined. Two experimental studies, on the understandability of the directional sounds and the robot state sounds as well as on the auditory perception of intended robot trajectories in a simulated supermarket scenario, are described. Subjective evaluations of sound characteristics such as urgency, expressiveness, and annoyance have been performed by non-musicians and musicians. These experimental results are compared with the diagrams which have been computed with two wavelet techniques for time-frequency analyses.",authors:"Johannsen, Gunnar",venue:"Georgia Institute of Technology",year:2002,link:"https://smartech.gatech.edu/handle/1853/51337",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:46,citations:[],selected:!0,slug:"paper_46",title:"Communicative Cues for Reach-to-Grasp Motions: From Humans to Robots",abstract:"Intent communication is an important challenge in the context of human-robot interaction. The aim of this work is to identify subtle non-verbal cues that make communication among humans fluent and using them to generate intent expressive robot motion. A human-human reach-to-grasp experiment ( n =14) identified two temporal and two spatial cues: (1) relative time to reach maximum hand aperture ( MA ), (2) overall motion duration ( OT ), (3) exaggeration in motion ( Exg ), and (4) change in grasp modality ( GM ). Results showed there was statistically significant difference in the temporal cues between no-intention and intention conditions. A follow-up experiment ( n =30) was conducted based on these results. Reach-to-grasp motions of a simulated robot containing different cue combinations were shown to the participants. They were asked to guess the target object during robot\u2019s motion, based on the assumption that intent expressive motion would result in earlier and more accurate guesses. Results showed that, OT, GM and several cue combinations led to faster and more accurate guesses which imply they can be used to generate communicative motion. However, MA had no effect, and surprisingly Exg had a negative effect on expressiveness.",authors:"Keb\xfcde, Dogancan; Eteke, Cem; Sezgin, Tevfik Metin; Akg\xfcn, Bars",venue:"ACM",year:2018,link:"https://dl.acm.org/doi/10.5555/3237383.3237830",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:47,citations:[],selected:!0,slug:"paper_47",title:"Nonverbal Robot-Group Interaction Using an Imitated Gaze Cue",abstract:"Ensuring that a particular and unsuspecting member of a group is the recipient of a salient-item hand-over is a complicated interaction. The robot must effectively, expediently and reliably communicate its intentions to advert any tendency within the group towards antinormative behaviour. In this paper, we study how a robot can establish the participant roles of such an interaction using imitated social and contextual cues. We designed two gaze cues, the first was designed to discourage antinormative behaviour through individualising a particular member of the group and the other to the contrary. We designed and conducted a field experiment (456 participants in 64 trials) in which small groups of people (between 3 and 20 people) assembled in front of the robot, which then attempted to pass a salient object to a particular group member by presenting a physical cue, followed by one of two variations of a gaze cue. Our results showed that presenting the individualising cue had a significant (z=3.733, p=0.0002) effect on the robot\u2019s ability to ensure that an arbitrary group member did not take the salient object and that the selected participant did.",authors:"Kirchner, Nathan; Alempijevic, Alen; Dissanayake, Gamini",venue:"ACM",year:2011,link:"https://doi.org/10.1145/1957656.1957824",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:48,citations:[],selected:!0,slug:"paper_48",title:"Hey! There is someone at your door. A hearing robot using visual communication signals of hearing dogs to communicate intent",abstract:"This paper presents a study of the readability of dog-inspired visual communication signals in a human-robot interaction scenario. This study was motivated by specially trained hearing dogs which provide assistance to their deaf owners by using visual communication signals to lead them to the sound source. For our human-robot interaction scenario, a robot was used in place of a hearing dog to lead participants to two different sound sources. The robot was preprogrammed with dog-inspired behaviors, controlled by a wizard who directly implemented the dog behavioral strategy on the robot during the trial. By using dog-inspired visual communication signals as a means of communication, the robot was able to lead participants to the sound sources (the microwave door, the front door). Findings indicate that untrained participants could correctly interpret the robot's intentions. Head movements and gaze directions were important for communicating the robot's intention using visual communication signals.",authors:"Koay, K. L.; Lakatos, G.; Syrdal, D. S.; Gacsi, M.; Bereczky, B.; Dautenhahn, K.; Miklosi, A.; Walters, M. L.",venue:"IEEE",year:2013,link:"https://doi.org/10.1109/ALIFE.2013.6602436",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"Robot-Focused",parent:"Attention"},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:49,citations:[],selected:!0,slug:"paper_49",title:"Effects of Integrated Intent Recognition and Communication on Human-Robot Collaboration",abstract:"Human-robot interaction research to date has investigated intent recognition and communication separately. In this paper, we explore the effects of integrating both the robot's ability to generate intentional motion and predict the human's motion in a collaborative physical task. We implemented an intent recognition system to recognize the human partner's hand motion intent and a motion planner system to enable the robot to communicate its intent by using legible and predictable motion. We tested this bi-directional intent system in a 2-way within-subjects user study. Results suggest that an integrated intent recognition and communication system may facilitate more collaborative behavior among team members.",authors:"Lee Chang, Mai; Gutierrez, Reymundo A.; Khante, Priyanka; Schaertl Short, Elaine; Lockerd Thomaz, Andrea",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/IROS.2018.8593359",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:50,citations:[],selected:!0,slug:"paper_50",title:"Methods for Expressing Robot Intent for Human\u2013Robot Collaboration in Shared Workspaces",abstract:"Human\u2013robot collaboration is becoming increasingly common in factories around the world; accordingly, we need to improve the interaction experiences between humans and robots working in these spaces. In this article, we report on a user study that investigated methods for providing information to a person about a robot\u2019s intent to move when working together in a shared workspace through signals provided by the robot. In this case, the workspace was the surface of a tabletop. Our study tested the effectiveness of three motion-based and three light-based intent signals as well as the overall level of comfort participants felt while working with the robot to sort colored blocks on the tabletop. Although not significant, our findings suggest that the light signal located closest to the workspace\u2014an LED bracelet located closest to the robot\u2019s end effector\u2014was the most noticeable and least confusing to participants. These findings can be leveraged to support human\u2013robot collaborations in shared spaces.",authors:"LeMasurier, Gregory; Bejerano, Gal; Albanese, Victoria; Parrillo, Jenna; Yanco, Holly A.; Amerson, Nicholas; Hetrick, Rebecca; Phillips, Elizabeth",venue:"ACM",year:2021,link:"https://doi.org/10.1145/3472223",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:51,citations:[],selected:!0,slug:"paper_51",title:"Towards situational awareness from robotic group motion",abstract:"The control of multiple robots in the context of tele-exploration tasks is often attentionally taxing, resulting in a loss of situational awareness for operators. Unmanned aerial vehicle swarms require significantly more multitasking than controlling a plane, thus making it necessary to devise intuitive feedback sources and control methods for these robots. The purpose of this article is to examine a swarm's nonverbal behaviour as a possible way to increase situational awareness and reduce the operators cognitive load by soliciting intuitions about the swarm's behaviour. To progress on the definition of a database of nonverbal expressions for robot swarms, we first define categories of communicative intents based on spontaneous descriptions of common swarm behaviours. The obtained typology confirms that the first two levels (as defined by Endsley: elements of environment and comprehension of the situation) can be shared through swarms motion-based communication. We then investigate group motion parameters potentially connected to these communicative intents. Results are that synchronized movement and tendency to form figures help convey meaningful information to the operator. We then discuss how this can be applied to realistic scenarios for the intuitive command of remote robotic teams.",authors:"Levillain, Florent; St-Onge, David; Beltrame, Giovanni; Zibetti, Elisabetta",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/RO-MAN46459.2019.8956381",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"World-Focused",parent:"Attention"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:52,citations:[],selected:!0,slug:"paper_52",title:"Mobile robot with eyeball expression as the preliminary-announcement and display of the robots following motion",abstract:"This paper explains the PMR-2R (prototype mobile robot \u20132 revised), the mobile robot with the eyeball expression as the preliminary-announcement and display of the robot\u2019s following motion. Firstly, we indicate the importance of the preliminary-announcement and display function of the mobile robot\u2019s following motion for the informational affinity between human being and a robot, with explaining the conventional methods and the related works. We show the proposed four methods which are categorized into two types: one type which indicates a state just after the moment and the other type which displays from the present to some future time continuously. Then we introduce the PMR-2R, which has the omni-directional display, the magicball, on which the eyeball expresses the robot\u2019s following direction of motion and the speed of motion at the same time. From the evaluation experiment, we confirmed the efficiency of the eyeball expression to transfer the information. We also obtained the announcement at around one or two second before the actual motion may be appropriate. And finally we compare the four types of eyeball expression: the one-eyeball type, the two-eyeball type, the will-o\u2019-the-wisp type, and the armor-helmet type. From the evaluation experiment, we have declared the importance to make the robot\u2019s front more intelligible especially to announce the robot\u2019s direction of motion.",authors:"Matsumaru, Takafumi; Iwase, Kazuya; Akiyama, Kyouhei; Kusada, Takashi; Ito, Tomotaka",venue:"Springer",year:2005,link:"https://doi.org/10.1007/s10514-005-0728-8",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:53,citations:[],selected:!0,slug:"paper_53",title:"Mobile robot with preliminary-announcement function of forthcoming motion using light-ray",abstract:"This paper discusses the design and the basic characteristic of the mobile robot PMR-1 with the preliminary-announcement and display function of the forthcoming operation (the direction of motion and the speed of motion) to the people around the robot by drawing a scheduled course on a running surface using light-ray. The laser pointer is used as a light source and the light from the laser pointer is reflected in a mirror. The light-ray is projected on a running surface and a scheduled course is drawn by rotating the reflector around the pan and the tilt axes. The preliminary-announcement and display unit of the developed mobile robot can indicate the operation until 3-second-later preliminarily, so the robot moves drawing the scheduled course from the present to 3-second-later. The experiment on coordination between the preliminary-announcement and the movement has been carried out, and we confirmed the correspondence of the announced course with the robot trajectory both in the case that the movement path is given beforehand and in the case that the robot is operated with manual input from a joystick in real-time. So we have validated the coordination algorithm between the preliminary-announcement and the real movement",authors:"Matsumaru, Takafumi",venue:"IEEE",year:2006,link:"https://doi.org/10.1109/IROS.2006.281981",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:54,citations:[],selected:!0,slug:"paper_54",title:"Mobile robot with preliminary-announcement and display function of forthcoming motion using projection equipment",abstract:"This paper discusses the mobile robot PMR-5 with the preliminary-announcement and display function which indicates the forthcoming operations to the people near the robot by using a projector. The projector is set on a mobile robot and a 2D frame is projected on a running surface. In the frame, not only the scheduled course but also the states of operation can be clearly announced as the information about movement. We examine the presentation of the states of operation such as stop or going back including the time information of the scheduled course on the developed robot. Scheduled course is expressed as the arrows considering the intelligibility at sight. Arrow expresses the direction of motion directly and the length of arrow can announce the speed of motion. Operation until 3-second-later is indicated and three arrows classified by color for each second are connected and displayed so these might show the changing of speed during 3-second period. The sign for spot revolution and the characters for stop and going back are also displayed. We exhibited the robot and about 200 visitors did the questionnaire evaluation. The average of 5-stage evaluation is 3.9 points and 4.5 points for the direction of motion and the speed of motion respectively. So we obtained the evaluation that it is intelligible in general",authors:"Matsumaru, Takafumi; Kusada, Takashi; Iwase, Kazuya",venue:"IEEE",year:2006,link:"https://doi.org/10.1109/ROMAN.2006.314368",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:55,citations:[],selected:!0,slug:"paper_55",title:"Mobile robot with preliminary-announcement and indication function of forthcoming operation using flat-panel display",abstract:"This research aims to propose the method and equipment to preliminary-announce and indicate the surrounding people both the speed of motion and the direction of motion of the mobile robot that moves on a two-dimensional plane. This paper discusses the mobile robot PMR-6, in which the liquid crystal display (LCD) is set up on the mobile unit, and the state of operation at 1.5 s before the actual motion is indicated. The basis of the content to display is 'arrow' considering the intelligibility for people even at first sight. The speed of motion is expressed as the size (length and width) of the arrow and its color based on traffic signal. The direction of motion is described with the curved condition of the arrow. The characters of STOP are displayed in red in case of stop. The robot was exhibited to the 2005 International Robot Exhibition held in Tokyo. About 200 visitors answered to the questionnaires. The average of five-stage evaluation is 3.56 and 3.97 points on the speed and on the direction respectively, so the method and expression were evaluated comparatively intelligible. As for the gender, the females appreciated about the speed of motion than the males on the whole. Concerning the age, some of the younger age and the upper age admired highly about the direction of motion than the middle age.",authors:"Matsumaru, Takafumi",venue:"IEEE",year:2007,link:"https://doi.org/10.1109/ROBOT.2007.363579",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:56,citations:[],selected:!0,slug:"paper_56",title:"Expression of intention by rotational head movements for teleoperated mobile robot",abstract:"We are studying a teleoperated mobile robot that provides useful information to a pedestrian. However, it is difficult for people to understand meanings of actions, motions or movements of many conventional robots. The purpose of this study is to improve pedestrian's impressions of a robot. Especially this paper describes people's understandability of robot behaviors when a robot turns around a corner or when a person and a robot pass each other in a corridor. Our robot shows its intention to make turn by rotating its head, as though a pedestrian shows a traveling direction by his/her gaze or face direction. The robot is teleoperated by an operator for safety in public spaces, and the direction of the robot head and the moving direction of the robot body are determined by an artificial potential field (APF) generated by a target position given by the operator, positions of obstacles and pedestrians. The APF for a pedestrian is generated based on her/his personal space of a person. Thus, the robot can express the intention of its action by rotating the head to look where it is going, when the robot changes its direction around pedestrians. The intention expression can be natural and understandable for them by the rotational movement of the head before the robot turns its body actually. Impression evaluation experiments with questionnaires were conducted under the two kinds of situations to reveal the validity and effectiveness of the intention expression by the robot's head rotation. Significant differences related to understandability and some impression words were observed between with and without rotating the head.",authors:"Mikawa, Masahiko; Yoshikawa, Yuriko; Fujisawa, Makoto",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/AMC.2019.8371097",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:57,citations:[],selected:!0,slug:"paper_57",title:"Meet Me Where i\u2019m Gazing: How Shared Attention Gaze Affects Human-Robot Handover Timing",abstract:'In this paper we provide empirical evidence that using humanlike gaze cues during human-robot handovers can improve the timing and perceived quality of the handover event. Handovers serve as the foundation of many human-robot tasks. Fluent, legible handover interactions require appropriate nonverbal cues to signal handover intent, location and timing. Inspired by observations of human-human handovers, we implemented gaze behaviors on a PR2 humanoid robot. The robot handed over water bottles to a total of 102 na\\"ive subjects while varying its gaze behaviour: no gaze, gaze designed to elicit shared attention at the handover location, and the shared attention gaze complemented with a turn-taking cue. We compared subject perception of and reaction time to the robot-initiated handovers across the three gaze conditions. Results indicate that subjects reach for the offered object significantly earlier when a robot provides a shared attention gaze cue during a handover. We also observed a statistical trend of subjects preferring handovers with turn-taking gaze cues over the other conditions. Our work demonstrates that gaze can play a key role in improving user experience of human-robot handovers, and help make handovers fast and fluent.',authors:"Moon, AJung; Troniak, Daniel M.; Gleeson, Brian; Pan, Matthew K.X.J.; Zheng, Minhua; Blumer, Benjamin A.; MacLean, Karon; Croft, Elizabeth A.",venue:"ACM",year:2014,link:"https://doi.org/10.1145/2559636.2559656",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:58,citations:[],selected:!0,slug:"paper_58",title:"Communicating Inferred Goals With Passive Augmented Reality and Active Haptic Feedback",abstract:"Robots learn as they interact with humans. Consider a human teleoperating an assistive robot arm: as the human guides and corrects the arm's motion, the robot gathers information about the human's desired task. But how does the human know what their robot has inferred? Today's approaches often focus on conveying intent: for instance, using legible motions or gestures to indicate what the robot is planning. However, closing the loop on robot inference requires more than just revealing the robot's current policy: the robot should also display the alternatives it thinks are likely, and prompt the human teacher when additional guidance is necessary. In this letter we propose a multimodal approach for communicating robot inference that combines both passive and active feedback. Specifically, we leverage information-rich augmented reality to passively visualize what the robot has inferred, and attention-grabbing haptic wristbands to actively prompt and direct the human's teaching. We apply our system to shared autonomy tasks where the robot must infer the human's goal in real-time. Within this context, we integrate passive and active modalities into a single algorithmic framework that determines when and which type of feedback to provide. Combining both passive and active feedback experimentally outperforms single modality baselines; during an in-person user study, we demonstrate that our integrated approach increases how efficiently humans teach the robot while simultaneously decreasing the amount of time humans spend interacting with the robot. Videos here: https://youtu.be/swq_u4iIP-g",authors:"Mullen, James; Mosier, Josh; Chakrabarti, Sounak; Chen, Anqi; White, Tyler; Losey, Dylan",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/LRA.2021.3111055",tags:[{name:"Intent Type",parent:null},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:59,citations:[],selected:!0,slug:"paper_59",title:"Nonverbal Leakage in Robots: Communication of Intentions through Seemingly Unintentional Behavior",abstract:'Human communication involves a number of nonverbal cues that are seemingly unintentional, unconscious, and automatic-both in their production and perception-and convey rich information on the emotional state and intentions of an individual. One family of such cues is called "nonverbal leakage." In this paper, we explore whether people can read nonverbal leakage cues-particularly gaze cues-in humanlike robots and make inferences on robots\u2019 intentions, and whether the physical design of the robot affects these inferences. We designed a gaze cue for Geminoid-a highly humanlike android-and Robovie-a robot with stylized, abstract humanlike features-that allowed the robots to "leak" information on what they might have in mind. In a controlled laboratory experiment, we asked participants to play a game of guessing with either of the robots and evaluated how the gaze cue affected participants\u2019 task performance. We found that the gaze cue did, in fact, lead to better performance, from which we infer that the cue led to attributions of mental states and intentionality. Our results have implications for robot design, particularly for designing expression of intentionality, and for our understanding of how people respond to human social cues when they are enacted by robots.',authors:"Mutlu, Bilge; Yamaoka, Fumitaka; Kanda, Takayuki; Ishiguro, Hiroshi; Hagita, Norihiro",venue:"ACM",year:2009,link:"https://doi.org/10.1145/1514095.1514110",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"World-Focused",parent:"Attention"}]},{UID:60,citations:[],selected:!0,slug:"paper_60",title:"Visualizing Robot Intent for Object Handovers with Augmented Reality",abstract:"Humans are highly skilled in communicating their intent for when and where a handover would occur. However, even the state-of-the-art robotic implementations for handovers display a general lack of communication skills. This study aims to visualize the internal state and intent of robots for Human-to-Robot Handovers using Augmented Reality. Specifically, we aim to visualize 3D models of the object and the robotic gripper to communicate the robot's estimation of where the object is and the pose in which the robot intends to grasp the object. We tested this design via a user study with 16 participants, in which each participant handed over a cube-shaped object to the robot 12 times. Results show that visualizing robot intent using augmented reality substantially improves the subjective experience of the users for handovers. Results also indicate that the effectiveness of augmented reality is even more pronounced for the perceived safety and fluency of the interaction when the robot makes errors in localizing the object.",authors:"Newbury, Rhys; Cosgun, Akansel; Crowley-Davis, Tysha; Chan, Wesley P.; Drummond, Tom; Croft, Elizabeth",venue:"arXiv",year:2021,link:"https://doi.org/10.48550/arXiv.2103.04055",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:61,citations:[],selected:!0,slug:"paper_61",title:"Bio-inspired multi-robot communication through behavior recognition",abstract:"This paper focuses on enabling multi-robot teams to cooperatively perform tasks without the use of radio or acoustic communication. One key to more effective cooperative interaction in a multi-robot team is the ability to understand the behavior and intent of other robots. This is similar to the honey bee \u201cwaggle dance\u201d in which a bee can communicate the orientation and distance of a food source. In this similar manner, our heterogenous multi-robot team uses a specific behavior to indicate the location of mine-like objects (MLOs). Observed teammate action sequences can be learned to perform behavior recognition and task-assignment in the absence of communication. We apply Conditional Random Fields (CRFs) to perform behavior recognition as an approach to task monitoring in the absence of communication in a challenging underwater environment. In order to demonstrate the use of behavior recognition of an Autonomous Underwater Vehicle (AUV) in a cooperative task, we use trajectory based techniques for model generation and behavior discrimination in experiments using simulated scenario data. Results are presented demonstrating heterogenous teammate cooperation between an AUV and an Autonomous Surface Vehicle (ASV) using behavior recognition rather than radio or acoustic communication in a mine clearing task.",authors:"Novitzky, Michael; Pippin, Charles; Collins, Thomas R.; Balch, Tucker R.; West, Michael E.",venue:"IEEE",year:2012,link:"https://doi.org/10.1109/ROBIO.2012.6491061",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:62,citations:[],selected:!0,slug:"paper_62",title:"A Drink-Serving Mobile Social Robot Selects Who to Interact with Using Gaze",abstract:'Robots will soon deliver food and beverages in various environments. These robots will need to communicate their intention efficiently; for example, they should indicate who they are addressing. We conducted a real-world study of a water serving robot at a university cafeteria. The robot was operated in a Wizard-of-Oz manner. It approached and offered water to students having their lunch. Our analyses of the relationship between robot gaze direction and the likelihood that someone takes a drink show that if people do not already have a drink and the interaction is not dominated by an overly enthusiastic user, the robot\u2019s gaze behavior is effective in selecting an interaction partner even "in the wild".',authors:"Palinko, Oskar; Fischer, Kerstin; Ruiz Ramirez, Eduardo; Damsgaard Nissen, Lotte; Langedijk, Rosalyn M.",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3371382.3378339",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:63,citations:[],selected:!0,slug:"paper_63",title:"Fostering short-term human anticipatory behavior in human-robot collaboration",abstract:"The present study reports on a human-robot collaboration experiment involving an industrial task with the specific aim of exploring the effects of (i) fostering human anticipatory behavior towards the robot, through visual cues of the robot\u2019s next move and (ii) robot adaptiveness to the human actions through reducing its motion speed with respect to human movement\u2019s proximity. For investigating these effects a generic collaborative picking and sorting task was designed, implemented and tested by volunteer participants, in a Virtual Reality simulation environment. Results demonstrated that, showing robot\u2019s intent through anticipatory cues significantly increased team efficiency, human safety and collaborative fluency in conjunction with a positive subjective inclination towards the robot. Robot adaptiveness significantly increased human safety without decreasing task efficiency and fluency, compared to a control condition.",authors:"Psarakis, Loizos; Nathanael, Dimitris; Marmaras, Nicolas",venue:"ScienceDirect",year:2022,link:"https://doi.org/10.1016/j.ergon.2021.103241",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:64,citations:[],selected:!0,slug:"paper_64",title:"Communicating and controlling robot arm motion intent through mixed-reality head-mounted displays",abstract:"Efficient motion intent communication is necessary for safe and collaborative work environments with co-located humans and robots. Humans efficiently communicate their motion intent to other humans through gestures, gaze, and other non-verbal cues, and can replan their motions in response. However, robots often have difficulty using these methods. Many existing methods for robot motion intent communication rely on 2D displays, which require the human to continually pause their work to check a visualization. We propose a mixed-reality head-mounted display (HMD) visualization of the intended robot motion over the wearer\u2019s real-world view of the robot and its environment. In addition, our interface allows users to adjust the intended goal pose of the end effector using hand gestures. We describe its implementation, which connects a ROS-enabled robot to the HoloLens using ROS Reality, using MoveIt for motion planning, and using Unity to render the visualization. To evaluate the effectiveness of this system against a 2D display visualization and against no visualization, we asked 32 participants to label various arm trajectories as either colliding or non-colliding with blocks arranged on a table. We found a 15% increase in accuracy with a 38% decrease in the time it took to complete the task compared with the next best system. These results demonstrate that a mixed-reality HMD allows a human to determine where the robot is going to move more quickly and accurately than existing baselines.",authors:"Rosen, Eric; Whitney, David; Phillips, Elizabeth; Chien, Gary; Tompkin, James; Konidaris, George; Tellex, Stefanie",venue:"SAGE Publications",year:2019,link:"https://doi.org/10.1177/0278364919842925",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:65,citations:[],selected:!0,slug:"paper_65",title:"Third point of view augmented reality for robot intentions visualization",abstract:"Lightweight, head-up displays integrated in industrial helmets allow to provide contextual information for industrial scenarios such as in maintenance. Moving from single display and single camera solutions to stereo perception and display opens new interaction possibilities. In particular this paper addresses the case of information sharing by a Baxter robot displayed to the user overlooking at the real scene. System design and interaction ideas are being presented.",authors:"Ruffaldi, Emanuele; Brizzi, Filippo; Tecchia, Franco; Bacinelli, Sandro",venue:"Springer",year:2016,link:"https://doi.org/10.1007/978-3-319-40621-3_35",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot World Perception",parent:"State"}]},{UID:66,citations:[],selected:!0,slug:"paper_66",title:"Communicating affect via flight path: exploring use of the laban effort system for designing affective locomotion paths",abstract:"People and animals use various kinds of motion in a multitude of ways to communicate their ideas and affective state, such as their moods or emotions. Further, people attribute affect and personalities to movements of even non-life like entities based solely on the style of their motions, e.g., the locomotion style of a geometric shape (how it moves about) can be interpreted as being shy, aggressive, etc. We investigate how robots can leverage this locomotion-style communication channel for communication with people. Specifically, our work deals with designing stylistic flying-robot locomotion paths for communicating affective state. To author and unpack the parameters of affect-oriented flying-robot locomotion styles we employ the Laban Effort System, a standard method for interpreting human motion commonly used in the performing arts. This paper describes our adaption of the Laban Effort System to author motions for flying robots, and the results of a formal experiment that investigated how various Laban Effort System parameters influence people's perception of the resulting robotic motions. We summarize with a set of guidelines for aiding designers in using the Laban Effort System to author flying robot motions to elicit desired affective responses.",authors:"Sharma, Megha; Hildebrandt, Dale; Newman, Gem; Young, James E.; Eskicioglu, Rasit",venue:"IEEE",year:2013,link:"https://doi.org/10.1109/HRI.2013.6483602",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:67,citations:[],selected:!0,slug:"paper_67",title:"Effect of Expressive Lights on Human Perception and Interpretation of Functional Robot",abstract:"Because appearance-constrained robots lack expressiveness, human users often find it hard to understand their behavior and intentions. To address this, expressive lights are considered to be an effective means for such robots to communicate with people. However, existing studies mainly focus on specific tasks or goals, leaving the knowledge of how expressive lights affect people\u2019s perception still unknown. In this pilot study, we investigate such a question by using a Roomba robot. We designed two light expressions, namely, green and low-intensity (GL) and red and high-intensity (RH). We used open-ended questions to evaluate people\u2019s perception and interpretation of the robot, which showed different light expressions as a way to communicate. Our findings reveal that simple light expressions can allow people to construct rich and complex interpretations of a robot\u2019s behavior, and such interpretations are heavily biased by the design of expressive lights.",authors:"Song, Sichao; Yamada, Seiji",venue:"ACM",year:2018,link:"https://doi.org/10.1145/3170427.3188547",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:68,citations:[],selected:!0,slug:"paper_68",title:"Designing LED Lights for Communicating Gaze with Appearance-Constrained Robots",abstract:"Functional robots are generally restricted in appearance, thus lacking ways to express their intent. In human-human interaction, gaze is an important cue for providing information and regulating interaction. In this pilot study, we investigate how we can implement gaze behavior in functional robots since gaze communication can allow humans to read a robot's intent and adjust their behavior accordingly. We explore design principles based on LED lights as we consider LEDs to be easily installed in most robots while not introducing features that are too human-like (to prevent users from having high expectations). In the paper, we present a design interface that allows designers to explore the parameter space of an LED strip attached to a Roomba robot. We then summarize a set of design principles for optimally simulating light-based gazes. Finally, our suggested design is evaluated by a large group of participants, and their comments are discussed.",authors:"Song, Sichao; Yamada, Seiji",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ROMAN.2018.8525661",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"World-Focused",parent:"Attention"}]},{UID:69,citations:[],selected:!0,slug:"paper_69",title:"Bioluminescence-Inspired Human-Robot Interaction: Designing Expressive Lights That Affect Human\u2019s Willingness to Interact with a Robot",abstract:"Bioluminescence is the production and emission of light by a living organism. It, as a means of communication, is of importance for the survival of various creatures. Inspired by bioluminescent light behaviors, we explore the design of expressive lights and evaluate the effect of such expressions on a human\xbbs perception of and attitude toward an appearance-constrained robot. Such robots are in urgent need of finding effective ways to present themselves and communicate their intentions due to a lack of social expressivity. We particularly focus on the expression of attractiveness and hostility because a robot would need to be able to attract or keep away human users in practical human-robot interaction (HRI) scenarios. In this work, we installed an LED lighting system on a Roomba robot and conducted a series of two experiments. We first worked through a structured approach to determine the best light expression designs for the robot to show attractiveness and hostility. This resulted in four recommended light expressions. Further, we performed a verification study to examine the effectiveness of such light expressions in a typical HRI context. On the basis of the findings, we offer design guidelines for expressive lights that HRI researchers and practitioners could readily employ.",authors:"Song, Sichao; Yamada, Seiji",venue:"ACM",year:2018,link:"https://doi.org/10.1145/3171221.3171249",tags:[{name:"Intent Type",parent:null},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:70,citations:[],selected:!0,slug:"paper_70",title:"Visual Attention in Spoken Human-Robot Interaction",abstract:'Psycholinguistic studies of situated language processing have revealed that gaze in the visual environment is tightly coupled with both spoken language comprehension and production. It has also been established that interlocutors monitor the gaze of their partners, a phenomenon called "joint attention", as a further means for facilitating mutual understanding. We hypothesise that human-robot interaction will benefit when the robot\u2019s language-related gaze behaviour is similar to that of people, potentially providing the user with valuable non-verbal information concerning the robot\u2019s intended message or the robot\u2019s successful understanding. We report findings from two eye-tracking experiments demonstrating (1) that human gaze is modulated by both the robot speech and gaze, and (2) that human comprehension of robot speech is improved when the robot\u2019s real-time gaze behaviour is similar to that of humans.',authors:"Staudte, Maria; Crocker, Matthew W.",venue:"ACM",year:2009,link:"https://doi.org/10.1145/1514095.1514111",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"World-Focused",parent:"Attention"}]},{UID:71,citations:[],selected:!0,slug:"paper_71",title:"Communication of Intent in Assistive Free Flyers",abstract:"Assistive free-flyers (AFFs) are an emerging robotic platform with unparalleled flight capabilities that appear uniquely suited to exploration, surveillance, inspection, and telepresence tasks. However, unconstrained aerial movements may make it difficult for colocated operators, collaborators, and observers to understand AFF intentions, potentially leading to difficulties understanding whether operator instructions are being executed properly or to safety concerns if future AFF motions are unknown or difficult to predict. To increase AFF usability when working in close proximity to users, we explore the design of natural and intuitive flight motions that may improve AFF abilities to communicate intent while simultaneously accomplishing task goals. We propose a formalism for representing AFF flight paths as a series of motion primitives and present two studies examining the effects of modifying the trajectories and velocities of these flight primitives based on natural motion principles. Our first study found that modified flight motions might allow AFFs to more effectively communicate intent and, in our second study, participants preferred interacting with an AFF that used a manipulated flight path, rated modified flight motions as more natural, and felt safer around an AFF with modified motion. Our proposed formalism and findings highlight the importance of robot motion in achieving effective human-robot interactions.",authors:"Szafir, Daniel; Mutlu, Bilge; Fong, Terrence",venue:"ACM",year:2014,link:"https://doi.org/10.1145/2559636.2559672",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:72,citations:[],selected:!0,slug:"paper_72",title:"Communicating Directionality in Flying Robots",abstract:"Small flying robots represent a rapidly emerging family of robotic technologies with aerial capabilities that enable unique forms of assistance in a variety of collaborative tasks. Such tasks will necessitate interaction with humans in close proximity, requiring that designers consider human perceptions regarding robots flying and acting within human environments. We explore the design space regarding explicit robot communication of flight intentions to nearby viewers. We apply design constraints to robot flight behaviors, using biological and airplane flight as inspiration, and develop a set of signaling mechanisms for visually communicating directionality while operating under such constraints. We implement our designs on two commercial flyers, requiring little modification to the base platforms, and evaluate each signaling mechanism, as well as a no-signaling baseline, in a user study in which participants were asked to predict robot intent. We found that three of our designs significantly improved viewer response time and accuracy over the baseline and that the form of the signal offered tradeoffs in precision, generalizability, and perceived robot usability.",authors:"Szafir, Daniel; Mutlu, Bilge; Fong, Terry",venue:"ACM",year:2015,link:"https://doi.org/10.1145/2696454.2696475",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:73,citations:[],selected:!0,slug:"paper_73",title:"Expressing thought: Improving robot readability with animation principles",abstract:"The animation techniques of anticipation and reaction can help create robot behaviors that are human readable such that people can figure out what the robot is doing, reasonably predict what the robot will do next, and ultimately interact with the robot in an effective way. By showing forethought before action and expressing a reaction to the task outcome (success or failure), we prototyped a set of human-robot interaction behaviors. In a 2 (forethought vs. none: between) \xd7 2 (reaction to outcome vs. none: between) \xd7 2 (success vs. failure task outcome: within) experiment, we tested the influences of forethought and reaction upon people's perceptions of the robot and the robot's readability. In this online video prototype experiment (N=273), we have found support for the hypothesis that perceptions of robots are influenced by robots showing forethought, the task outcome (success or failure), and showing goal-oriented reactions to those task outcomes. Implications for theory and design are discussed.",authors:"Takayama, Leila; Dooley, Doug; Ju, Wendy",venue:"IEEE",year:2011,link:"https://doi.org/10.1145/1957656.1957674",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:74,citations:[],selected:!0,slug:"paper_74",title:"The development and evaluation of Robot Light Skin: a novel robot signalling system to improve communication in industrial human\u2013robot collaboration",abstract:"In a human\u2013robot collaborative production system, the robot could make request for interaction or notify the human operator if an uncertainty arises. Conventional industrial tower lights were designed for generic machine signalling purposes which may not be the ultimate solution for robot signalling in a collaborative setting. In this type of system, human operators could be monitoring multiple robots while carrying out a manual task so it is important to minimise the diversion of their attention. This paper presents a novel robot signalling solution, the Robot Light Skin (RLS),which is an integrated signalling system that could be used on most articulated robots. Our experiment was conducted to validate this concept in terms of its effect on improving operator's reaction time, hit-rate, awareness and task performance. The results showed that participants reacted faster to the RLS as well as achieved higher hit-rate. An eye tracker was used in the experiment which shows a reduction in diversion away from the manual task when using the RLS. Future study should explore the effect of the RLS concept on large-scale systems and multi-robot systems.",authors:"Tang, Gilbert; Webb, Phil; Thrower, John",venue:"ScienceDirect",year:2019,link:"https://doi.org/10.1016/j.rcim.2018.08.005",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:75,citations:[],selected:!0,slug:"paper_75",title:"Intuitive and Safe Interaction in Multi-User Human Robot Collaboration Environments through Augmented Reality Displays",abstract:"As autonomous collaborative robots are more widely used in work environments alongside humans it is of great importance to facilitate the communication between people and robotic systems, in a way that promotes safety and productivity. To this end, we propose an Augmented Reality (AR) based system that allows workers in a human-robot collaborative environment to interact with a robot while also receiving information regarding the robot state and plans that relate to the human\u2019s safety and trust, such as the intended movement of the robotic arm or the navigation plan of the mobile platform. To evaluate the effectiveness of the proposed system we conducted experiments with 13 participants, where two users had to work in the same workspace while being assisted by a mobile manipulator. We measured the task completion time as well as the robot idle time using our AR-based human-robot interaction system and compared them to a conventional setup without the use of augmented reality. Additional, subjective evaluations related to user satisfaction, system usability, perceived safety and trust showed that users assessed the system in a positive way and preferred AR visualization over more traditional interfaces.",authors:"Tsamis, Georgios; Chantziaras, Georgios; Giakoumis, Dimitrios; Kostavelis, Ioannis; Kargakos, Andreas; Tsakiris, Athanasios; Tzovaras, Dimitrios",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/RO-MAN50785.2021.9515474",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:76,citations:[],selected:!0,slug:"paper_76",title:"Communicating Robot Motion Intent with Augmented Reality",abstract:"Humans coordinate teamwork by conveying intent through social cues, such as gestures and gaze behaviors. However, these methods may not be possible for appearance-constrained robots that lack anthropomorphic or zoomorphic features, such as aerial robots. We explore a new design space for communicating robot motion intent by investigating how augmented reality (AR) might mediate human-robot interactions. We develop a series of explicit and implicit designs for visually signaling robot motion intent using AR, which we evaluate in a user study. We found that several of our AR designs significantly improved objective task efficiency over a baseline in which users only received physically-embodied orientation cues. In addition, our designs offer several trade-offs in terms of intent clarity and user perceptions of the robot as a teammate.",authors:"Walker, Michael; Hedayati, Hooman; Lee, Jennifer; Szafir, Daniel",venue:"ACM",year:2018,link:"https://doi.org/10.1145/3171221.3171253",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:77,citations:[],selected:!0,slug:"paper_77",title:"Communicating robotic navigational intentions",abstract:"This paper presents a study on intention communication in a navigational context using a robotic wheelchair. The robotic wheelchair uses light projection to communicate its motion intentions. The novelty of the work is threefold: the communication of robot intentions to the passenger, the consideration of passenger and robot as a group (\u201cin-group\u201d) [1] who share motion intentions and the communication of the in-group intentions to other pedestrians (the \u201cout-group\u201d). A comparison in an autonomous navigation task where the robotic wheelchair autonomously navigates the environment with and without intention communication was performed showing that passengers and walking people found intention communication intuitive and helpful for passing by actions. Evaluation results significantly show human participant preference for having navigational intention communication for the wheelchair passenger and the person passing by it. Quantitative results show the motion of the person passing by the wheelchair with intention communication was significantly smoother compared to without intention communication.",authors:"Watanabe, Atsushi; Ikeda, Tetsushi; Morales, Yoichi; Shinozawa, Kazuhiko; Miyashita, Takahiro; Hagita, Norihiro",venue:"IEEE",year:2015,link:"https://doi.org/10.1109/IROS.2015.7354195",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:78,citations:[],selected:!0,slug:"paper_78",title:"A Laser Projection System for Robot Intention Communication and Human Robot Interaction",abstract:"In order to deploy service robots in environments where they encounter and/or cooperate with persons, one important key factor is human acceptance. Hence, information on which upcoming actions of the robot are based has to be made transparent and understandable to the human. However, considering the restricted power resources of mobile robot platforms, systems for visualization not only have to be expressive but also energy efficient. In this paper, we applied the well-known technique of laser scanning on a mobile robot to create a novel system for intention visualization and human-robot-interaction. We conducted user tests to compare our system to a low-power consuming LED video projector solution in order to evaluate the suitability for mobile platforms and to get human impressions of both systems. We can show that the presented system is preferred by most users in a dynamic test setup on a mobile platform.",authors:"Wengefeld, Tim; Hochemer, Dominik; Lewandowski, Benjamin; Kohler, Mona; Beer, Manuel; Gross, Horst-Michael",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/RO-MAN47096.2020.9223517",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"},{name:"Robot World Perception",parent:"State"},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:79,citations:[],selected:!0,slug:"paper_79",title:"Robot Gesture Sonification to Enhance Awareness of Robot Status and Enjoyment of Interaction",abstract:"We present a divergent approach to robotic sonification with the goal of improving the quality and safety of human-robot interactions. Sonification (turning data into sound) has been underutilized in robotics, and has broad potential to convey robotic movement and intentions to users without requiring visual engagement. We design and evaluate six different sonifications of movements for a robot with four degrees of freedom. Our sonification techniques include a direct mapping from each degree of freedom to pitch and timbre changes, emotion-based sound mappings, and velocity-based mappings using different types of sounds such as motors and music. We evaluate these sonifications using metrics for ease of use, enjoyment/appeal, and conveyance of movement information. Based on our results, we make recommendations to inform decisions for future robot sonification design. We suggest that when using sonification to improve safety of human-robot collaboration, it is necessary not only to convey sufficient information about movements, but also to convey that information in a pleasing and even social way to to enhance the human-robot relationship.",authors:"Zahray, Lisa; Savery, Richard; Syrkett, Liana; Weinberg, Gil",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/RO-MAN47096.2020.9223452",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:80,citations:[],selected:!0,slug:"paper_80",title:"Expressive robot motion timing",abstract:"Our goal is to enable robots to time their motion in a way that is purposefully expressive of their internal states, making them more transparent to people. We start by investigating what types of states motion timing is capable of expressing, focusing on robot manipulation and keeping the path constant while systematically varying the timing. We find that users naturally pick up on certain properties of the robot (like confidence), of the motion (like naturalness), or of the task (like the weight of the object that the robot is carrying). We then conduct a hypothesis-driven experiment to tease out the directions and magnitudes of these effects, and use our findings to develop candidate mathematical models for how users make these inferences from the timing. We find a strong correlation between the models and real user data, suggesting that robots can leverage these models to autonomously optimize the timing of their motion to be expressive.",authors:"Zhou, Allan; Hadfield-Menell, Dylan; Nagabandi, Anusha; Dragan, Anca D.",venue:"ACM",year:2017,link:"https://doi.org/10.1145/2909824.3020221",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:81,citations:[],selected:!0,slug:"paper_81",title:"Towards Explainable Shared Control using Augmented Reality",abstract:"Shared control plays a pivotal role in establishing effective human-robot interactions. Traditional control-sharing methods strive to complement a human's capabilities at safely completing a task, and thereby rely on users forming a mental model of the expected robot behaviour. However, these methods can often bewilder or frustrate users whenever their actions do not elicit the intended system response, forming a misalignment between the respective internal models of the robot and human. To resolve this model misalignment, we introduce Explainable Shared Control as a paradigm in which assistance and information feedback are jointly considered. Augmented reality is presented as an integral component of this paradigm, by visually unveiling the robot's inner workings to human operators. Explainable Shared Control is instantiated and tested for assistive navigation in a setup involving a robotic wheelchair and a Microsoft HoloLens with add-on eye tracking. Experimental results indicate that the introduced paradigm facilitates transparent assistance by improving recovery times from adverse events associated with model misalignment.",authors:"Zolotas, Mark; Demiris, Yiannis",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/IROS40897.2019.8968117",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"},{name:"Robot World Perception",parent:"State"}]}],taxonomy:[[{name:"Intent Type",expanded:!0,parent:null,level:1,start:9,stop:16}],[{name:"Motion",expanded:!0,parent:"Intent Type",level:2,start:9,stop:10},{name:"Attention",expanded:!0,parent:"Intent Type",level:2,start:11,stop:12},{name:"State",expanded:!0,parent:"Intent Type",level:2,start:13,stop:14},{name:"Instruction",expanded:!0,parent:"Intent Type",level:2,start:15,stop:16}],[{name:"Robot Self-Actions",expanded:!0,parent:"Motion",level:3,start:9,stop:9},{name:"World Actions",expanded:!0,parent:"Motion",level:3,start:10,stop:10},{name:"Robot-Focused",expanded:!0,parent:"Attention",level:3,start:11,stop:11},{name:"World-Focused",expanded:!0,parent:"Attention",level:3,start:12,stop:12},{name:"Robot Self-Perception",expanded:!0,parent:"State",level:3,start:13,stop:13},{name:"Robot World Perception",expanded:!0,parent:"State",level:3,start:14,stop:14},{name:"Robot-Centered",expanded:!0,parent:"Instruction",level:3,start:15,stop:15},{name:"World-Centered",expanded:!0,parent:"Instruction",level:3,start:16,stop:16}]]},{name:"Intent Information",data:[{UID:5,citations:[],selected:!0,slug:"paper_5",title:"Projecting robot intentions into human environments",abstract:"Trained human co-workers can often easily predict each other's intentions based on prior experience. When collaborating with a robot coworker, however, intentions are hard or impossible to infer. This difficulty of mental introspection makes human-robot collaboration challenging and can lead to dangerous misunderstandings. In this paper, we present a novel, object-aware projection technique that allows robots to visualize task information and intentions on physical objects in the environment. The approach uses modern object tracking methods in order to display information at specific spatial locations taking into account the pose and shape of surrounding objects. As a result, a human co-worker can be informed in a timely manner about the safety of the workspace, the site of next robot manipulation tasks, and next subtasks to perform. A preliminary usability study compares the approach to collaboration approaches based on monitors and printed text. The study indicates that, on average, the user effectiveness and satisfaction is higher with the projection based approach.",authors:"Andersen, Rasmus S.; Madsen, Ole; Moeslund, Thomas B.; Amor, Heni Ben",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745145",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Description",parent:"Unregistered in Space"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:6,citations:[],selected:!0,slug:"paper_6",title:"Designing Multimodal Intent Communication Strategies for Conflict Avoidance in Industrial Human-Robot Teams",abstract:"Robot-to-human intent communication has been proposed as a method of enabling fluent coordination in human-robot teams. Prior research has focused on identifying modalities by which intent information can be accurately communicated, but has not yet studied whether intent communication enables fluent or safer coordination in human-robot teams in which intent communication is only supportive to the team's primary task. To address this question, we conduct a study (N = 29) in a mock collaborative manufacturing scenario in which motion-based and display-based intent communication approaches are evaluated under varying penalties for failing to coordinate safely. Subjective and objective measures of team fluency suggest that although intent communication supports fluent coordination, using a purely motion-based or a purely display-based approach may not be the most effective strategy. Although multimodal intent communication did not significantly improve upon unimodal approaches, merging both motion-based and display-based intent communication seems to combine the strengths of both approaches. Interestingly, results also suggest that contrary to theoretical predictions, the positive effect of intent communication is generally robust to teaming scenarios that require members to operate concurrently.",authors:"Aubert, Miles C.; Bader, Hayden; Hauser, Kris",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ROMAN.2018.8525557",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:7,citations:[],selected:!0,slug:"paper_7",title:"Legible light communications for factory robots",abstract:"This work focuses on methods to improve mobile robot legibility in factories using lights. Implementation and evaluation were done at a robotics company that manufactures factory robots that work in human spaces. Three new sets of communicative lights were created and tested on the robots, integrated into the company's software stack and compared to the industry default lights that currently exist on the robots. All three newly designed light sets outperformed the industry default. Insights from this work have been integrated into software releases across North America.",authors:"Bacula, Alexandra; Mercer, Jason; Knight, Heather",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3371382.3378305",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:8,citations:[],selected:!0,slug:"paper_8",title:"Enhancing human understanding of a mobile robot\u2019s state and actions using expressive lights",abstract:"In order to be successfully integrated into human-populated environments, mobile robots need to express relevant information about their state to the outside world. In particular, animated lights are a promising way to express hidden robot state information such that it is visible at a distance. In this work, we present an online study to evaluate the effect of robot communication through expressive lights on people's understanding of the robot's state and actions. In our study, we use the CoBot mobile service robot with our light interface, designed to express relevant robot information to humans. We evaluate three designed light animations on three corresponding scenarios for each, for a total of nine scenarios. Our results suggest that expressive lights can play a significant role in helping people accurately hypothesize about a mobile robot's state and actions from afar when minimal contextual clues are present. We conclude that lights could be generally used as an effective non-verbal communication modality for mobile robots in the absence of, or as a complement to, other modalities.",authors:"Baraka, Kim; Rosenthal, Stephanie; Veloso, Manuela",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745187",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:9,citations:[],selected:!0,slug:"paper_9",title:"A flexible optimization-based method for synthesizing intent-expressive robot arm motion",abstract:"We present an approach to synthesize robot arm trajectories that effectively communicate the robot\u2019s intent to a human collaborator while achieving task goals. Our approach uses nonlinear constrained optimization to encode task requirements and desired motion properties. Our implementation allows for a wide range of constraints and objectives. We introduce a novel objective function to optimize robot arm motions for intent-expressiveness that works in a range of scenarios and robot arm types. Our formulation supports experimentation with different theories of how viewers interpret robot motion. Through a series of human-subject experiments on real and simulated robots, we demonstrate that our method leads to improved collaborative performance against other methods, including the current state of the art. These experiments also show how our perception heuristic can affect collaborative outcomes.",authors:"Bodden, Christopher; Rakita, Daniel; Mutlu, Bilge; Gleicher, Michael",venue:"SAGE Publications",year:2018,link:"https://doi.org/10.1177/0278364918792295",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:10,citations:[],selected:!0,slug:"paper_10",title:"Transparent Robot Behavior by Adding Intuitive Visual and Acoustic Feedback to Motion Replanning",abstract:"Nowadays robots are able to work safely close to humans. They are light-weight, intrinsically safe and capable of avoiding obstacles as well as understand and predict human motions. In this collaborative scenario, the communication between humans and robots is a fundamental aspect to achieve good efficiency and ergonomics in the task execution. A lot of research has been made related to robot understanding and prediction of the human behavior, allowing the robot to replan its motion trajectories. This work is focused on the communication of the robot's intentions to the human to make its goals and planned trajectories easily understandable. Visual and acoustic information has been added to give the human an intuitive feedback to immediately understand the robot's plan. This allows a better interaction and makes the humans feel more comfortable, without any feeling of anxiety related to the unpredictability of the robot motion. Experiments have been conducted in a collaborative assembly scenario. The results of these tests were collected in questionnaires, in which the humans reported the differences and improvements they experienced using the feedback communication system.",authors:"Bolano, Gabriele; Roennau, Arne; Dillmann, Ruediger",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ROMAN.2018.8525671",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Description",parent:"Unregistered in Space"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:11,citations:[],selected:!0,slug:"paper_11",title:"Deploying Multi-Modal Communication Using Augmented Reality in a Shared Workspace",abstract:"Robots are no longer working isolated in safety fences and Human-Robot Collaboration (HRC) is becoming one of the most promising topic of research to improve the efficiency in many application scenarios. Sharing the same workspace, both human and robot should clearly understand the intentions and motions of each other, in order to enable an efficient and effective interaction. In this work we propose an AR-based system to show the robot planned motion and target to the worker. We focused on representing this information in an intuitive way for inexperienced users. We introduced a multi-modal communication feedback in order to enable the user to agree with or change the robot plan using gestures and speech. The effectiveness of the system has been evaluated with test cases performed by a group of testers with no robotic experience. The results showed that the system helped the user to better understand the robot intentions and planned motion, improving the ergonomics and trust in the interaction. Furthermore, the evaluation included the rating of the different input modalities provided, in order to compare the different ways of communication proposed.",authors:"Bolano, Gabriele; Fu, Yuchao; Roennau, Arne; Dillmann, Ruediger",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/UR52253.2021.9494689",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:12,citations:[],selected:!0,slug:"paper_12",title:"Using Spatial and Temporal Contrast for Fluent Robot-Human Hand-Overs",abstract:"For robots to get integrated in daily tasks assisting humans, robot-human interactions will need to reach a level of fluency close to that of human-human interactions. In this paper we address the fluency of robot-human hand-overs. From an observational study with our robot HERB, we identify the key problems with a baseline hand-over action. We find that the failure to convey the intention of handing over causes delays in the transfer, while the lack of an intuitive signal to indicate timing of the hand-over causes early, unsuccessful attempts to take the object. We propose to address these problems with the use of spatial contrast, in the form of distinct hand-over poses, and temporal contrast, in the form of unambiguous transitions to the hand-over pose. We conduct a survey to identify distinct hand-over poses, and determine variables of the pose that have most communicative potential for the intent of handing over. We present an experiment that analyzes the effect of the two types of contrast on the fluency of hand-overs. We find that temporal contrast is particularly useful in improving fluency by eliminating early attempts of the human.",authors:"Cakmak, Maya; Srinivasa, Siddhartha S.; Lee, Min Kyung; Kiesler, Sara; Forlizzi, Jodi",venue:"ACM",year:2011,link:"https://doi.org/10.1145/1957656.1957823",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:13,citations:[],selected:!0,slug:"paper_13",title:"Communication Through Motion: Legibility of Multi-Robot Systems",abstract:"The interaction between a user and a multi-robot system in a shared environment is a relatively uncharted topic. But, as these types of systems will increase in the future years, an efficient way of communication is necessary. To this aim, it is interesting to discover if a multi-robot system can communicate its intentions exploiting only some motion-variables, which are characteristics of the motion of the robots. This study is about the legibility of a multi-robot system: in particular, we focus on the influence of these motion-variables on the legibility of more than one group of robots that move in a shared environment with the user. These motion-variables are: trajectory, dispersion and stiffness. They are generally used to define the motion of a group of mobile robots. Trajectory and dispersion were found relevant for the correctness of the communication between the user and the multi-robot system, while stiffness was found relevant for the rapidity of communication. The analysis of the influence of the motion-variables was carried out with an ANOVA (analysis of variance) based on a series of data coming from an experimental campaign conducted in a virtual reality set-up.",authors:"Capelli, Beatrice; Secchi, Cristian; Sabattini, Lorenzo",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/MRS.2019.8901100",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:14,citations:[],selected:!0,slug:"paper_14",title:"Emotion encoding in human-drone interaction",abstract:"Drones are becoming more popular and may soon be ubiquitous. As they enter our everyday environments, it becomes critical to ensure their usability through natural Human-Drone Interaction (HDI). Previous work in Human-Robot Interaction (HRI) shows that adding an emotional component is part of the key to success in robots' acceptability. We believe the adoption of personal drones would also benefit from adding an emotional component. This work defines a range of personality traits and emotional attributes that can be encoded in drones through their flight paths. We present a user study (N=20) and show how well three defined emotional states can be recognized. We draw conclusions on interaction techniques with drones and feedback strategies that use the drone's flight path and speed.",authors:"Cauchard, Jessica R.; Zhai, Kevin Y.; Spadafora, Marco; Landay, James A.",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/HRI.2016.7451761",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:15,citations:[],selected:!0,slug:"paper_15",title:"Using nonverbal signals to request help during human-robot collaboration",abstract:"Non-humanoid robots are becoming increasingly utilized for collaborative tasks that rely on each collaborator's ability to effectively convey their mental state while accurately estimating and interpreting their partner's knowledge, intent, and actions. During these tasks, it may be beneficial or even necessary for the human collaborator to assist the robot. Consequently, we explore the use of nonverbal signals to request help during a collaborative task. We focus on light and sound as they are commonly used communication channels across many domains. This paper analyzes the effectiveness of three nonverbal help signals that vary in urgency. Our results show that these signals significantly influence the human collaborator's and their perception of the collaboration.",authors:"Cha, Elizabeth; Mataric, Maja",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/IROS.2016.7759744",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"}]},{UID:16,citations:[],selected:!0,slug:"paper_16",title:"Bi-directional navigation intent communication using spatial augmented reality and eye-tracking glasses for improved safety in human\u2013robot interaction",abstract:"Safety, legibility and efficiency are essential for autonomous mobile robots that interact with humans. A key factor in this respect is bi-directional communication of navigation intent, which we focus on in this article with a particular view on industrial logistic applications. In the direction robot-to-human, we study how a robot can communicate its navigation intent using Spatial Augmented Reality (SAR) such that humans can intuitively understand the robot\u2019s intention and feel safe in the vicinity of robots. We conducted experiments with an autonomous forklift that projects various patterns on the shared floor space to convey its navigation intentions. We analyzed trajectories and eye gaze patterns of humans while interacting with an autonomous forklift and carried out stimulated recall interviews (SRI) in order to identify desirable features for projection of robot intentions. In the direction human-to-robot, we argue that robots in human co-habited environments need human-aware task and motion planning to support safety and efficiency, ideally responding to people\u2019s motion intentions as soon as they can be inferred from human cues. Eye gaze can convey information about intentions beyond what can be inferred from the trajectory and head pose of a person. Hence, we propose eye-tracking glasses as safety equipment in industrial environments shared by humans and robots. In this work, we investigate the possibility of human-to-robot implicit intention transference solely from eye gaze data and evaluate how the observed eye gaze patterns of the participants relate to their navigation decisions. We again analyzed trajectories and eye gaze patterns of humans while interacting with an autonomous forklift for clues that could reveal direction intent. Our analysis shows that people primarily gazed on that side of the robot they ultimately decided to pass by. We discuss implications of these results and relate to a control approach that uses human gaze for early obstacle avoidance.",authors:"Chadalavada, Ravi Teja; Andreasson, Henrik; Schindler, Maike; Palm, Rainer; Lilienthal, Achim J.",venue:"ScienceDirect",year:2020,link:"https://doi.org/10.1016/j.rcim.2019.101830",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:17,citations:[],selected:!0,slug:"paper_17",title:"Projection-Aware Task Planning and Execution for Human-in-the-Loop Operation of Robots in a Mixed-Reality Workspace",abstract:"Recent advances in mixed-reality technologies have renewed interest in alternative modes of communication for human-robot interaction. However, most of the work in this direction has been confined to tasks such as teleoperation, simulation or explication of individual actions of a robot. In this paper, we will discuss how the capability to project intentions affect the task planning capabilities of a robot. Specifically, we will start with a discussion on how projection actions can be used to reveal information regarding the future intentions of the robot at the time of task execution. We will then pose a new planning paradigm - projection-aware planning - whereby a robot can trade off its plan cost with its ability to reveal its intentions using its projection actions. We will demonstrate each of these scenarios with the help of a joint human-robot activity using the HoloLens.",authors:"Chakraborti, Tathagata; Sreedharan, Sarath; Kulkarni, Anagha; Kambhampati, Subbarao",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/IROS.2018.8593830",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:18,citations:[],selected:!0,slug:"paper_18",title:"Negotiation based Human-Robot Collaboration via Augmented Reality",abstract:"Effective human-robot collaboration (HRC) requires extensive communication among the human and robot teammates, because their actions can potentially produce conflicts, synergies, or both. We develop a novel augmented reality (AR) interface to bridge the communication gap between human and robot teammates. Building on our AR interface, we develop an AR-mediated, negotiation-based (ARN) framework for HRC. We have conducted experiments both in simulation and on real robots in an office environment, where multiple mobile robots work on delivery tasks. The robots could not complete the tasks on their own, but sometimes need help from their human teammate, rendering human-robot collaboration necessary. Results suggest that ARN significantly reduced the human-robot team's task completion time compared to a non-AR baseline approach.",authors:"Chandan, Kishan; Kudalkar, Vidisha; Li, Xiang; Zhang, Shiqi",venue:"arXiv",year:2019,link:"https://doi.org/10.48550/arXiv.1909.11227",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:19,citations:[],selected:!0,slug:"paper_19",title:"Avoiding Human-Robot Collisions Using Haptic Communication",abstract:"Fully autonomous navigation in populated environments is still a challenging problem for mobile robots. This paper explores the idea of using active human-robot communication to facilitate navigation tasks. We propose to convey a robot's intent to human users via a wearable haptic interface. The interface can display distinct haptic cues by modulating vibration amplitudes and patterns. We applied the concept to a single human/single robot orthogonal encounter scenario, where one of the two parties has to yield the right of way to avoid collision. Under certain conditions, the robot's intent (to yield to the human or not) is revealed to the human via the haptic interface prior to the interaction. We conducted an experiment with 10 users, in which the robot was teleoperated as a substitute for autonomy. Results show that, when given priority, users become more risk-accepting and use different strategies to navigate the collision scenario than when the robot takes priority or there is no haptic communication channel. In addition, we propose a social-force based model to predict human movement during navigation. The effect of communication can be explained as a shift in the user's safety buffer and expectation of the robot's future velocity.",authors:"Che, Yuhang; Sun, Cuthbert T.; Okamura, Allison M.",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ICRA.2018.8460946",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"}]},{UID:20,citations:[],selected:!0,slug:"paper_20",title:"Efficient and Trustworthy Social Navigation via Explicit and Implicit Robot\u2013Human Communication",abstract:"In this article, we present a planning framework that uses a combination of implicit (robot motion) and explicit (visual/audio/haptic feedback) communication during mobile robot navigation. First, we developed a model that approximates both continuous movements and discrete behavior modes in human navigation, considering the effects of implicit and explicit communication on human decision-making. The model approximates the human as an optimal agent, with a reward function obtained through inverse reinforcement learning. Second, a planner uses this model to generate communicative actions that maximize the robot's transparency and efficiency. We implemented the planner on a mobile robot, using a wearable haptic device for explicit communication. In a user study of an indoor human-robot pair orthogonal crossing situation, the robot is able to actively communicate its intent to users in order to avoid collisions and facilitate efficient trajectories. Results show that the planner generated plans that are easier to understand, reduce users` effort, and increase users' trust of the robot, compared to simply performing collision avoidance. The key contribution of this article is the integration and analysis of explicit communication (together with implicit communication) for social navigation.",authors:"Che, Yuhang; Okamura, Allison M.; Sadigh, Dorsa",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/TRO.2020.2964824",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:21,citations:[],selected:!0,slug:"paper_21",title:"Touched by a Robot: An Investigation of Subjective Responses to Robot-Initiated Touch",abstract:"By initiating physical contact with people, robots can be more useful. For example, a robotic caregiver might make contact to provide physical assistance or facilitate communication. So as to better understand how people respond to robot-initiated touch, we conducted a 2x2 between-subjects experiment with 56 people in which a robotic nurse autonomously touched and wiped the subject\u2019s forearm. Our independent variables were whether or not the robot verbally warned the person before contact, and whether the robot verbally indicated that the touch was intended to clean the person\u2019s skin (instrumental touch) or to provide comfort (affective touch). On average, regardless of the treatment, participants had a generally positive subjective response. However, with instrumental touch people responded significantly more favorably. Since the physical behavior of the robot was the same for all trials, our results demonstrate that the perceived intent of the robot can significantly influence a person\u2019s subjective response to robot-initiated touch. Our results suggest that roboticists should consider this factor in addition to the mechanics of physical interaction. Unexpectedly, we found that participants tended to respond more favorably without a verbal warning. Although inconclusive, our results suggest that verbal warnings prior to contact should be carefully designed, if used at all.",authors:"Chen, Tiffany L.; King, Chih-Hung; Thomaz, Andrea L.; Kemp, Charles C.",venue:"ACM",year:2011,link:"https://doi.org/10.1145/1957656.1957818",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Description",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:22,citations:[],selected:!0,slug:"paper_22",title:"Dynamic Path Visualization for Human-Robot Collaboration",abstract:"Augmented reality technology can enable robots to visualize their future actions giving users crucial information to avoid collisions and other conflicting actions. Although a robot\u2019s entire action plan could be visualized (such as the output of a navigational planner), how far into the future it is appropriate to display the robot\u2019s plan is unknown. We developed a dynamic path visualizer that projects the robot\u2019s motion intent at varying lengths depending on the complexity of the upcoming path. We tested our approach in a virtual game where participants were tasked to collect and deliver gems to a robot that moves randomly towards a grid of markers in a confined area. Preliminary results on a small sample size indicate no significant effect on task performance; however, open-ended responses reveal participants preference towards visuals that show longer path projections.",authors:"Cleaver, Andre; Tang, Darren Vincent; Chen, Victoria; Short, Elaine Schaertl; Sinapov, Jivko",venue:"ACM",year:2021,link:"https://doi.org/10.1145/3434074.3447188",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:23,citations:[],selected:!0,slug:"paper_23",title:"MIRO: A Versatile Biomimetic Edutainment Robot",abstract:"Here we present MIRO, a companion robot designed to engage users in science and robotics via edutainment. MIRO is a robot that is biomimetic in aesthetics, morphology, behaviour, and control architecture. In this paper, we review how these design choices affect its suitability for a companionship role. In particular, we consider how MIRO\u2019s emulation of familiar mammalian body language as one component of a broader biomimetic expressive system provides effective communication of emotional state and intent. We go on to discuss how these features contribute to MIRO\u2019s potential in other domains such as healthcare, education, and research.",authors:"Collins, Emily C.; Prescott, Tony J.; Mitchinson, Ben; Conran, Sebastian",venue:"ACM",year:2015,link:"https://doi.org/10.1145/2832932.2832978",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:24,citations:[],selected:!0,slug:"paper_24",title:"Spatial augmented reality as a method for a mobile robot to communicate intended movement",abstract:"Our work evaluates a mobile robot\u2019s ability to communicate intended movements to humans via projection of visual arrows and a simplified map. Humans utilize a variety of techniques to signal intended movement in a co-occupied space. We evaluated an augmented reality projection provided by the robot. The projection is on the floor and consists of arrows and a simplified map. Two pilots and one quasi-experiment were conducted to examine the effectiveness of visual projection of arrows by a robot for signaling intended movement. The pilot work demonstrates the effectiveness of utilizing arrows as a communication medium. The experiment examined the effectiveness of a simplified map and arrows for signaling the short-, mid-range, and long-term intended movement. Two pilot experiments confirm that arrows are an effective symbol for a robot to use to signal intent. A field experiment demonstrates that a robot can use a projected arrow and simplified map to signal its intended movement and people understand the projection for upcoming short-, medium-, and long-term movement. Augmented reality, such as projected arrows and simplified map, are an effective tool for robots to use when signaling their upcoming movement to humans. Telepresence robots in organizations, museum docents, information kiosks, hospital assistants, factories, and as members of search and rescue teams are typical applications where mobile robots reside and interact with people.",authors:"Coovert, Michael D.; Lee, Tiffany; Shindev, Ivan; Sun, Yu",venue:"ScienceDirect",year:2014,link:"https://doi.org/10.1016/j.chb.2014.02.001",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Directional",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:25,citations:[],selected:!0,slug:"paper_25",title:"Multimodal Interaction with an Autonomous Forklift",abstract:'We describe a multimodal framework for interacting with an autonomous robotic forklift. A key element enabling effective interaction is a wireless, handheld tablet with which a human supervisor can command the forklift using speech and sketch. Most current sketch interfaces treat the canvas as a blank slate. In contrast, our interface uses live and synthesized camera images from the forklift as a canvas, and augments them with object and obstacle information from the world. This connection enables users to "draw on the world," enabling a simpler set of sketched gestures. Our interface supports commands that include summoning the forklift and directing it to lift, transport, and place loads of palletized cargo. We describe an exploratory evaluation of the system designed to identify areas for detailed study.Our framework incorporates external signaling to interact with humans near the vehicle. The robot uses audible and visual annunciation to convey its current state and intended actions. The system also provides seamless autonomy handoff: any human can take control of the robot by entering its cabin, at which point the forklift can be operated manually until the human exits.',authors:"Correa, Andrew; Walter, Matthew R.; Fletcher, Luke; Glass, Jim; Teller, Seth; Davis, Randall",venue:"ACM",year:2010,link:"https://doi.org/10.1109/HRI.2010.5453188",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Directional",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Description",parent:"Unregistered in Space"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:26,citations:[],selected:!0,slug:"paper_26",title:"LED Strip Based Robot Movement Intention Signs for Human-Robot Interactions",abstract:"As a new kind of robots, called cooperative robots, are more commonly used in industry, a new way of communication is becoming more important due to the increasing number of closer cooperation between human and robots. This paper proposes the idea behind a novel method of using visual communication between cobots and humans focusing mainly on the field of industrial robotics. This device can decrease the mental stress experienced by the coworker and can increase the trust resulting in a closer to ergonomic workspace from the coworker viewpoint. Other possible usage is also discussed.",authors:"Domonkos, Mark; Dombi, Zoltan; Botzheim, Janos",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/CINTI51262.2020.9305854",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"}]},{UID:27,citations:[],selected:!0,slug:"paper_27",title:"Generating legible motion",abstract:"Legible motion \u2014 motion that communicates its intent to a human observer \u2014 is crucial for enabling seamless human-robot collaboration. In this paper, we propose a functional gradient optimization technique for autonomously generating legible motion. Our algorithm optimizes a legibility metric inspired by the psychology of action interpretation in humans, resulting in motion trajectories that purposefully deviate from what an observer would expect in order to better convey intent. A trust region constraint on the optimization ensures that the motion does not become too surprising or unpredictable to the observer. Our studies with novice users that evaluate the resulting trajectories support the applicability of our method and of such a trust region. They show that within the region, legibility as measured in practice does significantly increase. Outside of it, however, the trajectory becomes confusing and the users\u2019 confidence in knowing the robot\u2019s intent significantly decreases.",authors:"Dragan, Anca and Srinivasa, Siddhartha",venue:"roboticsproceedings.org",year:2013,link:"http://www.roboticsproceedings.org/rss09/p24.pdf",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:28,citations:[],selected:!0,slug:"paper_28",title:"Effects of Robot Motion on Human-Robot Collaboration",abstract:"Most motion in robotics is purely functional, planned to achieve the goal and avoid collisions. Such motion is great in isolation, but collaboration affords a human who is watching the motion and making inferences about it, trying to coordinate with the robot to achieve the task. This paper analyzes the benefit of planning motion that explicitly enables the collaborator\u2019s inferences on the success of physical collaboration, as measured by both objective and subjective metrics. Results suggest that legible motion, planned to clearly express the robot\u2019s intent, leads to more fluent collaborations than predictable motion, planned to match the collaborator\u2019s expectations. Furthermore, purely functional motion can harm coordination, which negatively affects both task efficiency, as well as the participants\u2019 perception of the collaboration.",authors:"Dragan, Anca D.; Bauman, Shira; Forlizzi, Jodi; Srinivasa, Siddhartha S.",venue:"ACM",year:2015,link:"https://doi.org/10.1145/2696454.2696473",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:29,citations:[],selected:!0,slug:"paper_29",title:"Investigation of Communicative Flight Paths for Small Unmanned Aerial Systems * This work was supported by NSF NRI 1638099",abstract:"This project seeks to generate small Unmanned Aerial System (sUAS) flight paths that are broadly understood by the general population and can communicate states about both the sUAS and its understanding of the world. Previous work in sUAS flight paths has sought to communicate intent, destination, or emotion of the system without focusing on concrete states (e.g., low battery, landing, etc.). This work leverages biologically-based flight paths and experimental methodologies from human-human and human-humanoid robot interactions to assess the understanding of avian flight paths to communicate sUAS states to novice users. If successful, this work should inform: the human-robot interaction community about the perception of flight paths, sUAS manufacturers on how their systems could communicate with both operators and bystanders, and end users on ways to communicate with others when flying systems in public spaces. General design implications and future directions of work are suggested to build on the results here, which suggest that novice users gravitate towards labels they understand (draw attention and landing) while avoiding more technical labels (lost sensor).",authors:"Duncan, Brittany A.; Beachly, Evan; Bevins, Alisha; Elbaum, Sebasitan; Detweiler, Carrick",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ICRA.2018.8462871",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:30,citations:[],selected:!0,slug:"paper_30",title:"Follow me: Communicating intentions with a spherical robot",abstract:"In recent years, robots have gradually become incorporated in our society and therefore play more relevant role in social environments. These robots vary in form, some being more anthropomorphic than others. This, creates a need to study their interaction with the world. In this paper we used Sphero and BB-8, two robots with a simple spherical body devoid of verbal and other complex communication methods, to investigate how they can communicate intention to people. A set of behaviors based on pet behaviors was designed and tested in a controlled experiment, where the robot's aim was to convince a participant to follow it. We concluded that the use of these behaviors allows a robot to effectively communicate intention as well as create a bond with the participant, who would treat it as an equal, thereby engaging it in social interactions such as playing with it or talking to it.",authors:"Faria, Miguel; Costigliola, Andrea; Alves-Oliveira, Patricia; Paiva, Ana",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745189",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:31,citations:[],selected:!0,slug:"paper_31",title:"\u201cMe and you together\u201d movement impact in multi-user collaboration tasks",abstract:"This paper presents a study on collaborative manipulation between an autonomous robot and multiple users. We investigate how different motion types impact people's ability to understand the robot's goals in a multi-user scenario. We propose an approach based on Collaborative Probabilistic Movement Primitives to generate the robot's movements, exploiting predictability and legibility of movement to express intentions through motion. We compare the impact on the interaction of using only either predictable or legible movements, and propose a third approach - hybrid motion - that selects, in each situation, whether to execute a predictable motion or a legible motion, depending on what the robot perceives as more efficient for the multi-user collaboration effort. To test the impact of the three motion types in the context of a collaborative task, we run a user study using a Baxter robot that autonomously serves cups of water to three users upon request. Our results show that, in the particular case where all users simultaneously request water, the hybrid motion performs better than the other two.",authors:"Faria, Miguel; Silva, Rui; Alves-Oliveira, Patricia; Melo, Francisco S.; Paiva, Ana",venue:"IEEE",year:2017,link:"https://doi.org/10.1109/IROS.2017.8206109",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:32,citations:[],selected:!0,slug:"paper_32",title:"Understanding Robots: Making Robots More Legible in Multi-Party Interactions",abstract:"In this work we explore implicit communication between humans and robots\u2014through movement\u2014in multi-party (or multi-user) interactions. In particular, we investigate how a robot can move to better convey its intentions using legible movements in multi-party interactions. Current research on the application of legible movements has focused on single-user interactions, causing a vacuum of knowledge regarding the impact of such movements in multi-party interactions. We propose a novel approach that extends the notion of legible motion to multi-party settings, by considering that legibility depends on all human users involved in the interaction, and should take into consideration how each of them perceives the robot\u2019s movements from their respective points-of-view. We show, through simulation and a user study, that our proposed model of multi-user legibility leads to movements that, on average, optimize the legibility of the motion as perceived by the group of users. Our model creates movements that allow each human to more quickly and confidently understand what are the robot\u2019s intentions, thus creating safer, clearer and more efficient interactions and collaborations.",authors:"Faria, Miguel; Melo, Francisco S.; Paiva, Ana",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/RO-MAN50785.2021.9515485",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:33,citations:[],selected:!0,slug:"paper_33",title:"Between legibility and contact: The role of gaze in robot approach",abstract:"In this paper, we explore experimentally the possible tradeoff between gaze to the user and gaze to the path in robot approach. While some previous work indicates that gaze towards the user increases perceived safety because the user feels recognized, other work indicates that it is legibility of the robot's actions that put users at ease. If the robot does not drive up to the person in a straight line directly, the robot can either continuously look at the person and thus maintain eye contact, or indicate its path through its gaze behavior, increasing legibility. In an experiment with N=36 participants, we tested the tradeoff between legibility and eye contact. The behavioral results show that users are significantly more at ease with the robot that gazes at them than with the robot that looks where it is going, measured by the number of instances of glances away from the robot. Likewise, the participants rate the robot that looks at them continuously as more intelligent and more cooperative. Thus, participants value mutual gaze higher than legibility.",authors:"Fischer, Kerstin; Jensen, Lars C.; Suvei, Stefan-Daniel; Bodenhagen, Leon",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745186",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:34,citations:[],selected:!0,slug:"paper_34",title:"Investigation of Unmanned Aerial Vehicle Gesture Perceptibility and Impact of Viewpoint Variance<sup>*</sup>",abstract:"Unmanned Aerial Vehicle (UAV) flight paths have been shown to communicate meaning to human observers, similar to human gestural communication. This paper presents the results of a UAV gesture perception study designed to assess how observer viewpoint perspective may impact how humans perceive the shape of UAV gestural motion. Robot gesture designers have demonstrated that robots can indeed communicate meaning through gesture; however, many of these results are limited to an idealized range of viewer perspectives and do not consider how the perception of a robot gesture may suffer from obfuscation or self-occlusion from some viewpoints. This paper presents the results of three online user-studies that examine participants' ability to accurately perceive the intended shape of two-dimensional UAV gestures from varying viewer perspectives. We used a logistic regression model to characterize participant gesture classification accuracy, demonstrating that viewer perspective does impact how participants perceive the shape of UAV gestures. Our results yielded a viewpoint angle threshold from beyond which participants were able to assess the intended shape of a gesture's motion with 90% accuracy. We also introduce a perceptibility score to capture user confidence, time to decision, and accuracy in labeling and to understand how differences in flight paths impact perception across viewpoints. These findings will enable UAV gesture systems that, with a high degree of confidence, ensure gesture motions can be accurately perceived by human observers.",authors:"Fletcher, Paul; Luther, Angeline; Duncan, Brittany; Detweiler, Carrick",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/ICRA48506.2021.9561094",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:35,citations:[],selected:!0,slug:"paper_35",title:"Augmenting the audio-based expression modality of a non-affective robot",abstract:"This paper investigates the potential benefits of augmenting audio-based affective means of expression to strengthen the perceived intentions of a robot. Robots are often viewed as being simple machines with limied capabilities of communication. Changing how a robot is perceived, towards a more affective interpretation of its intentions, requires careful consideration of the means of expression available to the robot. It also requires alignment between these means to ensure they work in coordination with each other to make the robot easier to understand. As an effort to strengthen the affective interpretation of a soft robotic arm robot, we altered its overall expression by changing the available audio-based expression modalities. The system mitigatedthe naturally occurring noise from actuators and pneumatic systems and used a custom sound that supported the movement of the robot. The robot was tested by interacting with human observers (n=78) and was perceived as being significantly more curious, happy and less angry when augmented by audio that aligned with the naturally occurred robot sounds. The results show that the audio-based expression modality of robots is a valuable communication tool to consider augmenting when designing robots that convey affective information.",authors:"Frederiksen, Morten Roed; Stoey, Kasper",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/ACII.2019.8925510",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"}]},{UID:36,citations:[],selected:!0,slug:"paper_36",title:"Touch-based information transfer from a robot modeled on the hearing dog",abstract:"Research on physical human-robot interaction has been attracting attention recently, focusing on robot embodiment. The work reported here proposes Active Touch Communication Robot (AcToR), a robot that is modeled on the hearing dog. A hearing dog is a type of dog assist people who are deaf or hard of hearing by alerting their handler to important sounds. AcToR uses the sense of touch to notify a human of the intention to transfer information. For example, when AcToR detects that a cell phone that is in another location has received a call, AcToR moves to the user's location and makes contact with the user's body to notify the user of the incoming call. The AcToR robot is based on the Roomba<sup>\xae</sup> and uses the Roomba's bumper and contact sensors to detect contact. This paper reports the results of psychological experiments using the AcToR robot that indicate the feasibility of using touch to transfer information from a robot to a person.",authors:"Furuhashi, Michihiko; Nakamura, Tsuyoshi; Kanoh, Masayoshi; Yamada, Koji",venue:"IEEE",year:2015,link:"https://doi.org/10.1109/FUZZ-IEEE.2015.7337981",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:37,citations:[],selected:!0,slug:"paper_37",title:"Generating anticipation in robot motion",abstract:"Robots that display anticipatory motion provide their human partners with greater time to respond in interactive tasks because human partners are aware of robot intent earlier. We create anticipatory motion autonomously from a single motion exemplar by extracting hand and body symbols that communicate motion intent and moving them earlier in the motion. We validate that our algorithm extracts the most salient frame (i.e. the correct symbol) which is the most informative about motion intent to human observers. Furthermore, we show that anticipatory variants allow humans to discern motion intent sooner than motions without anticipation, and that humans are able to reliably predict motion intent prior to the symbol frame when motion is anticipatory. Finally, we quantified the time range for robot motion when humans can perceive intent more accurately and the collaborative social benefits of anticipatory motion are greatest.",authors:"Gielniak, Michael J.; Thomaz, Andrea L.",venue:"IEEE",year:2011,link:"https://doi.org/10.1109/ROMAN.2011.6005255",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:38,citations:[],selected:!0,slug:"paper_38",title:"Robopal: Modeling Role Transitions in Human-Robot Interaction",abstract:"We have developed a new communication robot, Robopal, which is an indoor/outdoor robot for use in human-robot interaction research in the context of daily life. Robopal's intended applications involve leading and/or following a human to a destination. Preliminary experiments have been conducted to study nonverbal cues associated with leading and following behavior, and it has been observed that some behaviors, such as glancing towards the leader or follower, appear to be role-dependent. A system for representing these behaviors with a state transition model is described, based on four kinds of interaction roles: directive, responsive, collaborative, and independent. It is proposed that behavior modeling can be simplified by using this system to represent changes in the roles the robot and human play in an interaction, and by associating appropriate behaviors to each role",authors:"Glas, Dylan F.; Miyashita, Takahiro; Ishiguro, Hiroshi; Hagita, Norihiro",venue:"IEEE",year:2007,link:"https://doi.org/10.1109/ROBOT.2007.363636",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:39,citations:[],selected:!0,slug:"paper_39",title:"Mind the ARm: Realtime Visualization of Robot Motion Intent in Head-Mounted Augmented Reality",abstract:"Established safety sensor technology shuts down industrial robots when a collision is detected, causing preventable loss of productivity. To minimize downtime, we implemented three Augmented Reality (AR) visualizations (Path, Preview, and Volume) which allow users to understand robot motion intent and give way to the robot. We compare the different visualizations in a user study in which a small cognitive task is performed in a shared workspace. We found that Preview and Path required significantly longer head rotations to perceive robot motion intent. Volume, however, required the shortest head rotation and was perceived as most safe, enabling closer proximity of the robot arm before one left the shared workspace without causing shutdowns.",authors:"Gruenefeld, Uwe; Pr\xe4del, Lars; Illing, Jannike; Stratmann, Tim; Drolshagen, Sandra; Pfingsthorn, Max",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3404983.3405509",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:40,citations:[],selected:!0,slug:"paper_40",title:"Seeing Thru Walls: Visualizing Mobile Robots in Augmented Reality",abstract:"We present an approach for visualizing mobile robots through an Augmented Reality headset when there is no line-of-sight visibility between the robot and the human. Three elements are visualized in Augmented Reality: 1) Robot\u2019s 3D model to indicate its position, 2) An arrow emanating from the robot to indicate its planned movement direction, and 3) A 2D grid to represent the ground plane. We conduct a user study with 18 participants, in which each participant are asked to retrieve objects, one at a time, from stations at the two sides of a T-junction at the end of a hallway where a mobile robot is roaming. The results show that visualizations improved the perceived safety and efficiency of the task and led to participants being more comfortable with the robot within their personal spaces. Furthermore, visualizing the motion intent in addition to the robot model was found to be more effective than visualizing the robot model alone. The proposed system can improve the safety of automated warehouses by increasing the visibility and predictability of robots.",authors:"Gu, Morris; Cosgun, Akansel; Chan, Wesley P.; Drummond, Tom; Croft, Elizabeth",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/RO-MAN50785.2021.9515322",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Directional",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:41,citations:[],selected:!0,slug:"paper_41",title:"Projection mapping implementation: Enabling direct externalization of perception results and action intent to improve robot explainability",abstract:"Existing research on non-verbal cues, e.g., eye gaze or arm movement, may not accurately present a robot's internal states such as perception results and action intent. Projecting the states directly onto a robot's operating environment has the advantages of being direct, accurate, and more salient, eliminating mental inference about the robot's intention. However, there is a lack of tools for projection mapping in robotics, compared to established motion planning libraries (e.g., MoveIt). In this paper, we detail the implementation of projection mapping to enable researchers and practitioners to push the boundaries for better interaction between robots and humans. We also provide practical documentation and code for a sample manipulation projection mapping on GitHub: github.com/uml-robotics/projection_mapping.",authors:"Han, Zhao; Wilkinson, Alexander; Parrillo, Jenna; Allspaw, Jordan; Yanco, Holly A.",venue:"arXiv",year:2020,link:"https://doi.org/10.48550/arXiv.2010.02263\n",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:42,citations:[],selected:!0,slug:"paper_42",title:"Investigating the Effectiveness of Different Interaction Modalities for Spatial Human-Robot Interaction",abstract:"With the increasing use of social robots in real environments, one of the areas of research requiring more attention is the study of human-robot interaction (HRI) when a person and robot are moving close to each other. Understanding effective ways to design how a robot should communicate its intention during dynamic movement is based on what people\u2019s expectations are and how they interpret different cues from the robot. Building on the existing literature, we tested a range of non-verbal cues such as eye contact, gaze and head nodding as part of the robot\u2019s behaviour during close proximate passing. The research aimed to investigate the effects of these cues, as well as their combination with body posture, on the efficiency of passing and the quality of HRI. Our results show that the combination of eye contact and the robot turning sideways is the most effective and appropriate compared to other modalities.",authors:"He, Jinying; van Maris, Anouk; Caleb-Solly, Praminda",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3371382.3378273",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:43,citations:[],selected:!0,slug:"paper_43",title:"Hey Robot, Which Way Are You Going? Nonverbal Motion Legibility Cues for Human-Robot Spatial Interaction",abstract:"Mobile robots have recently been deployed in public spaces such as shopping malls, airports, and urban sidewalks. Most of these robots are designed with human-aware motion planning capabilities but are not designed to communicate with pedestrians. Pedestrians that encounter these robots without prior understanding of the robots' behaviour can experience discomfort, confusion, and delayed social acceptance. In this work we designed and evaluated nonverbal robot motion legibility cues, which communicate a mobile robot's motion intention to pedestrians. We compared a motion legibility cue using Projected Arrows to one using Flashing Lights. We designed the cues to communicate path information, goal information, or both, and explored different Robot Movement Scenarios. We conducted an online user study with 229 participants using videos of the motion legibility cues. Our results show that the absence of cues was not socially acceptable, and that Projected Arrows were the more socially acceptable cue in most experimental conditions. We conclude that the presence and choice of motion legibility cues can positively influence robots' acceptance and successful deployment in public spaces.",authors:"Hetherington, Nicholas J.; Croft, Elizabeth A.; van der Loos, H. MachielF.",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/LRA.2021.3068708",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Directional",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"}]},{UID:44,citations:[],selected:!0,slug:"paper_44",title:"Legible robot pointing",abstract:"Good communication is critical to seamless human-robot interaction. Among numerous communication channels, here we focus on gestures, and in particular on spacial deixis: pointing at objects in the environment in order to reference them. We propose a mathematical model that enables robots to generate pointing configurations that make the goal object as clear as possible - pointing configurations that are legible. We study the implications of legibility on pointing, e.g. that the robot will sometimes need to trade off efficiency for the sake of clarity. Finally, we test how well our model works in practice in a series of user studies, showing that the resulting pointing configurations make the goal object easier to infer for novice users.",authors:"Holladay, Rachel M.; Dragan, Anca D.; Srinivasa, Siddhartha S.",venue:"IEEE",year:2014,link:"https://doi.org/10.1109/ROMAN.2014.6926256",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:45,citations:[],selected:!0,slug:"paper_45",title:"Auditory display of directions and states for mobile systems",abstract:"Auditory displays for mobile systems, such as service robots, have been developed. The design of directional sounds and of additional sounds for robot states (e.g., Heavy Load), as well as the design of more complicated robot sound tracks are explained. Basic musical elements and robot movement sounds have been combined. Two experimental studies, on the understandability of the directional sounds and the robot state sounds as well as on the auditory perception of intended robot trajectories in a simulated supermarket scenario, are described. Subjective evaluations of sound characteristics such as urgency, expressiveness, and annoyance have been performed by non-musicians and musicians. These experimental results are compared with the diagrams which have been computed with two wavelet techniques for time-frequency analyses.",authors:"Johannsen, Gunnar",venue:"Georgia Institute of Technology",year:2002,link:"https://smartech.gatech.edu/handle/1853/51337",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"}]},{UID:46,citations:[],selected:!0,slug:"paper_46",title:"Communicative Cues for Reach-to-Grasp Motions: From Humans to Robots",abstract:"Intent communication is an important challenge in the context of human-robot interaction. The aim of this work is to identify subtle non-verbal cues that make communication among humans fluent and using them to generate intent expressive robot motion. A human-human reach-to-grasp experiment ( n =14) identified two temporal and two spatial cues: (1) relative time to reach maximum hand aperture ( MA ), (2) overall motion duration ( OT ), (3) exaggeration in motion ( Exg ), and (4) change in grasp modality ( GM ). Results showed there was statistically significant difference in the temporal cues between no-intention and intention conditions. A follow-up experiment ( n =30) was conducted based on these results. Reach-to-grasp motions of a simulated robot containing different cue combinations were shown to the participants. They were asked to guess the target object during robot\u2019s motion, based on the assumption that intent expressive motion would result in earlier and more accurate guesses. Results showed that, OT, GM and several cue combinations led to faster and more accurate guesses which imply they can be used to generate communicative motion. However, MA had no effect, and surprisingly Exg had a negative effect on expressiveness.",authors:"Keb\xfcde, Dogancan; Eteke, Cem; Sezgin, Tevfik Metin; Akg\xfcn, Bars",venue:"ACM",year:2018,link:"https://dl.acm.org/doi/10.5555/3237383.3237830",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:47,citations:[],selected:!0,slug:"paper_47",title:"Nonverbal Robot-Group Interaction Using an Imitated Gaze Cue",abstract:"Ensuring that a particular and unsuspecting member of a group is the recipient of a salient-item hand-over is a complicated interaction. The robot must effectively, expediently and reliably communicate its intentions to advert any tendency within the group towards antinormative behaviour. In this paper, we study how a robot can establish the participant roles of such an interaction using imitated social and contextual cues. We designed two gaze cues, the first was designed to discourage antinormative behaviour through individualising a particular member of the group and the other to the contrary. We designed and conducted a field experiment (456 participants in 64 trials) in which small groups of people (between 3 and 20 people) assembled in front of the robot, which then attempted to pass a salient object to a particular group member by presenting a physical cue, followed by one of two variations of a gaze cue. Our results showed that presenting the individualising cue had a significant (z=3.733, p=0.0002) effect on the robot\u2019s ability to ensure that an arbitrary group member did not take the salient object and that the selected participant did.",authors:"Kirchner, Nathan; Alempijevic, Alen; Dissanayake, Gamini",venue:"ACM",year:2011,link:"https://doi.org/10.1145/1957656.1957824",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:48,citations:[],selected:!0,slug:"paper_48",title:"Hey! There is someone at your door. A hearing robot using visual communication signals of hearing dogs to communicate intent",abstract:"This paper presents a study of the readability of dog-inspired visual communication signals in a human-robot interaction scenario. This study was motivated by specially trained hearing dogs which provide assistance to their deaf owners by using visual communication signals to lead them to the sound source. For our human-robot interaction scenario, a robot was used in place of a hearing dog to lead participants to two different sound sources. The robot was preprogrammed with dog-inspired behaviors, controlled by a wizard who directly implemented the dog behavioral strategy on the robot during the trial. By using dog-inspired visual communication signals as a means of communication, the robot was able to lead participants to the sound sources (the microwave door, the front door). Findings indicate that untrained participants could correctly interpret the robot's intentions. Head movements and gaze directions were important for communicating the robot's intention using visual communication signals.",authors:"Koay, K. L.; Lakatos, G.; Syrdal, D. S.; Gacsi, M.; Bereczky, B.; Dautenhahn, K.; Miklosi, A.; Walters, M. L.",venue:"IEEE",year:2013,link:"https://doi.org/10.1109/ALIFE.2013.6602436",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:49,citations:[],selected:!0,slug:"paper_49",title:"Effects of Integrated Intent Recognition and Communication on Human-Robot Collaboration",abstract:"Human-robot interaction research to date has investigated intent recognition and communication separately. In this paper, we explore the effects of integrating both the robot's ability to generate intentional motion and predict the human's motion in a collaborative physical task. We implemented an intent recognition system to recognize the human partner's hand motion intent and a motion planner system to enable the robot to communicate its intent by using legible and predictable motion. We tested this bi-directional intent system in a 2-way within-subjects user study. Results suggest that an integrated intent recognition and communication system may facilitate more collaborative behavior among team members.",authors:"Lee Chang, Mai; Gutierrez, Reymundo A.; Khante, Priyanka; Schaertl Short, Elaine; Lockerd Thomaz, Andrea",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/IROS.2018.8593359",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:50,citations:[],selected:!0,slug:"paper_50",title:"Methods for Expressing Robot Intent for Human\u2013Robot Collaboration in Shared Workspaces",abstract:"Human\u2013robot collaboration is becoming increasingly common in factories around the world; accordingly, we need to improve the interaction experiences between humans and robots working in these spaces. In this article, we report on a user study that investigated methods for providing information to a person about a robot\u2019s intent to move when working together in a shared workspace through signals provided by the robot. In this case, the workspace was the surface of a tabletop. Our study tested the effectiveness of three motion-based and three light-based intent signals as well as the overall level of comfort participants felt while working with the robot to sort colored blocks on the tabletop. Although not significant, our findings suggest that the light signal located closest to the workspace\u2014an LED bracelet located closest to the robot\u2019s end effector\u2014was the most noticeable and least confusing to participants. These findings can be leveraged to support human\u2013robot collaborations in shared spaces.",authors:"LeMasurier, Gregory; Bejerano, Gal; Albanese, Victoria; Parrillo, Jenna; Yanco, Holly A.; Amerson, Nicholas; Hetrick, Rebecca; Phillips, Elizabeth",venue:"ACM",year:2021,link:"https://doi.org/10.1145/3472223",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"}]},{UID:51,citations:[],selected:!0,slug:"paper_51",title:"Towards situational awareness from robotic group motion",abstract:"The control of multiple robots in the context of tele-exploration tasks is often attentionally taxing, resulting in a loss of situational awareness for operators. Unmanned aerial vehicle swarms require significantly more multitasking than controlling a plane, thus making it necessary to devise intuitive feedback sources and control methods for these robots. The purpose of this article is to examine a swarm's nonverbal behaviour as a possible way to increase situational awareness and reduce the operators cognitive load by soliciting intuitions about the swarm's behaviour. To progress on the definition of a database of nonverbal expressions for robot swarms, we first define categories of communicative intents based on spontaneous descriptions of common swarm behaviours. The obtained typology confirms that the first two levels (as defined by Endsley: elements of environment and comprehension of the situation) can be shared through swarms motion-based communication. We then investigate group motion parameters potentially connected to these communicative intents. Results are that synchronized movement and tendency to form figures help convey meaningful information to the operator. We then discuss how this can be applied to realistic scenarios for the intuitive command of remote robotic teams.",authors:"Levillain, Florent; St-Onge, David; Beltrame, Giovanni; Zibetti, Elisabetta",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/RO-MAN46459.2019.8956381",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:52,citations:[],selected:!0,slug:"paper_52",title:"Mobile robot with eyeball expression as the preliminary-announcement and display of the robots following motion",abstract:"This paper explains the PMR-2R (prototype mobile robot \u20132 revised), the mobile robot with the eyeball expression as the preliminary-announcement and display of the robot\u2019s following motion. Firstly, we indicate the importance of the preliminary-announcement and display function of the mobile robot\u2019s following motion for the informational affinity between human being and a robot, with explaining the conventional methods and the related works. We show the proposed four methods which are categorized into two types: one type which indicates a state just after the moment and the other type which displays from the present to some future time continuously. Then we introduce the PMR-2R, which has the omni-directional display, the magicball, on which the eyeball expresses the robot\u2019s following direction of motion and the speed of motion at the same time. From the evaluation experiment, we confirmed the efficiency of the eyeball expression to transfer the information. We also obtained the announcement at around one or two second before the actual motion may be appropriate. And finally we compare the four types of eyeball expression: the one-eyeball type, the two-eyeball type, the will-o\u2019-the-wisp type, and the armor-helmet type. From the evaluation experiment, we have declared the importance to make the robot\u2019s front more intelligible especially to announce the robot\u2019s direction of motion.",authors:"Matsumaru, Takafumi; Iwase, Kazuya; Akiyama, Kyouhei; Kusada, Takashi; Ito, Tomotaka",venue:"Springer",year:2005,link:"https://doi.org/10.1007/s10514-005-0728-8",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:53,citations:[],selected:!0,slug:"paper_53",title:"Mobile robot with preliminary-announcement function of forthcoming motion using light-ray",abstract:"This paper discusses the design and the basic characteristic of the mobile robot PMR-1 with the preliminary-announcement and display function of the forthcoming operation (the direction of motion and the speed of motion) to the people around the robot by drawing a scheduled course on a running surface using light-ray. The laser pointer is used as a light source and the light from the laser pointer is reflected in a mirror. The light-ray is projected on a running surface and a scheduled course is drawn by rotating the reflector around the pan and the tilt axes. The preliminary-announcement and display unit of the developed mobile robot can indicate the operation until 3-second-later preliminarily, so the robot moves drawing the scheduled course from the present to 3-second-later. The experiment on coordination between the preliminary-announcement and the movement has been carried out, and we confirmed the correspondence of the announced course with the robot trajectory both in the case that the movement path is given beforehand and in the case that the robot is operated with manual input from a joystick in real-time. So we have validated the coordination algorithm between the preliminary-announcement and the real movement",authors:"Matsumaru, Takafumi",venue:"IEEE",year:2006,link:"https://doi.org/10.1109/IROS.2006.281981",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:54,citations:[],selected:!0,slug:"paper_54",title:"Mobile robot with preliminary-announcement and display function of forthcoming motion using projection equipment",abstract:"This paper discusses the mobile robot PMR-5 with the preliminary-announcement and display function which indicates the forthcoming operations to the people near the robot by using a projector. The projector is set on a mobile robot and a 2D frame is projected on a running surface. In the frame, not only the scheduled course but also the states of operation can be clearly announced as the information about movement. We examine the presentation of the states of operation such as stop or going back including the time information of the scheduled course on the developed robot. Scheduled course is expressed as the arrows considering the intelligibility at sight. Arrow expresses the direction of motion directly and the length of arrow can announce the speed of motion. Operation until 3-second-later is indicated and three arrows classified by color for each second are connected and displayed so these might show the changing of speed during 3-second period. The sign for spot revolution and the characters for stop and going back are also displayed. We exhibited the robot and about 200 visitors did the questionnaire evaluation. The average of 5-stage evaluation is 3.9 points and 4.5 points for the direction of motion and the speed of motion respectively. So we obtained the evaluation that it is intelligible in general",authors:"Matsumaru, Takafumi; Kusada, Takashi; Iwase, Kazuya",venue:"IEEE",year:2006,link:"https://doi.org/10.1109/ROMAN.2006.314368",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Description",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:55,citations:[],selected:!0,slug:"paper_55",title:"Mobile robot with preliminary-announcement and indication function of forthcoming operation using flat-panel display",abstract:"This research aims to propose the method and equipment to preliminary-announce and indicate the surrounding people both the speed of motion and the direction of motion of the mobile robot that moves on a two-dimensional plane. This paper discusses the mobile robot PMR-6, in which the liquid crystal display (LCD) is set up on the mobile unit, and the state of operation at 1.5 s before the actual motion is indicated. The basis of the content to display is 'arrow' considering the intelligibility for people even at first sight. The speed of motion is expressed as the size (length and width) of the arrow and its color based on traffic signal. The direction of motion is described with the curved condition of the arrow. The characters of STOP are displayed in red in case of stop. The robot was exhibited to the 2005 International Robot Exhibition held in Tokyo. About 200 visitors answered to the questionnaires. The average of five-stage evaluation is 3.56 and 3.97 points on the speed and on the direction respectively, so the method and expression were evaluated comparatively intelligible. As for the gender, the females appreciated about the speed of motion than the males on the whole. Concerning the age, some of the younger age and the upper age admired highly about the direction of motion than the middle age.",authors:"Matsumaru, Takafumi",venue:"IEEE",year:2007,link:"https://doi.org/10.1109/ROBOT.2007.363579",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Description",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:56,citations:[],selected:!0,slug:"paper_56",title:"Expression of intention by rotational head movements for teleoperated mobile robot",abstract:"We are studying a teleoperated mobile robot that provides useful information to a pedestrian. However, it is difficult for people to understand meanings of actions, motions or movements of many conventional robots. The purpose of this study is to improve pedestrian's impressions of a robot. Especially this paper describes people's understandability of robot behaviors when a robot turns around a corner or when a person and a robot pass each other in a corridor. Our robot shows its intention to make turn by rotating its head, as though a pedestrian shows a traveling direction by his/her gaze or face direction. The robot is teleoperated by an operator for safety in public spaces, and the direction of the robot head and the moving direction of the robot body are determined by an artificial potential field (APF) generated by a target position given by the operator, positions of obstacles and pedestrians. The APF for a pedestrian is generated based on her/his personal space of a person. Thus, the robot can express the intention of its action by rotating the head to look where it is going, when the robot changes its direction around pedestrians. The intention expression can be natural and understandable for them by the rotational movement of the head before the robot turns its body actually. Impression evaluation experiments with questionnaires were conducted under the two kinds of situations to reveal the validity and effectiveness of the intention expression by the robot's head rotation. Significant differences related to understandability and some impression words were observed between with and without rotating the head.",authors:"Mikawa, Masahiko; Yoshikawa, Yuriko; Fujisawa, Makoto",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/AMC.2019.8371097",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:57,citations:[],selected:!0,slug:"paper_57",title:"Meet Me Where i\u2019m Gazing: How Shared Attention Gaze Affects Human-Robot Handover Timing",abstract:'In this paper we provide empirical evidence that using humanlike gaze cues during human-robot handovers can improve the timing and perceived quality of the handover event. Handovers serve as the foundation of many human-robot tasks. Fluent, legible handover interactions require appropriate nonverbal cues to signal handover intent, location and timing. Inspired by observations of human-human handovers, we implemented gaze behaviors on a PR2 humanoid robot. The robot handed over water bottles to a total of 102 na\\"ive subjects while varying its gaze behaviour: no gaze, gaze designed to elicit shared attention at the handover location, and the shared attention gaze complemented with a turn-taking cue. We compared subject perception of and reaction time to the robot-initiated handovers across the three gaze conditions. Results indicate that subjects reach for the offered object significantly earlier when a robot provides a shared attention gaze cue during a handover. We also observed a statistical trend of subjects preferring handovers with turn-taking gaze cues over the other conditions. Our work demonstrates that gaze can play a key role in improving user experience of human-robot handovers, and help make handovers fast and fluent.',authors:"Moon, AJung; Troniak, Daniel M.; Gleeson, Brian; Pan, Matthew K.X.J.; Zheng, Minhua; Blumer, Benjamin A.; MacLean, Karon; Croft, Elizabeth A.",venue:"ACM",year:2014,link:"https://doi.org/10.1145/2559636.2559656",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Continuous",parent:"Temporal Dimension"}]},{UID:58,citations:[],selected:!0,slug:"paper_58",title:"Communicating Inferred Goals With Passive Augmented Reality and Active Haptic Feedback",abstract:"Robots learn as they interact with humans. Consider a human teleoperating an assistive robot arm: as the human guides and corrects the arm's motion, the robot gathers information about the human's desired task. But how does the human know what their robot has inferred? Today's approaches often focus on conveying intent: for instance, using legible motions or gestures to indicate what the robot is planning. However, closing the loop on robot inference requires more than just revealing the robot's current policy: the robot should also display the alternatives it thinks are likely, and prompt the human teacher when additional guidance is necessary. In this letter we propose a multimodal approach for communicating robot inference that combines both passive and active feedback. Specifically, we leverage information-rich augmented reality to passively visualize what the robot has inferred, and attention-grabbing haptic wristbands to actively prompt and direct the human's teaching. We apply our system to shared autonomy tasks where the robot must infer the human's goal in real-time. Within this context, we integrate passive and active modalities into a single algorithmic framework that determines when and which type of feedback to provide. Combining both passive and active feedback experimentally outperforms single modality baselines; during an in-person user study, we demonstrate that our integrated approach increases how efficiently humans teach the robot while simultaneously decreasing the amount of time humans spend interacting with the robot. Videos here: https://youtu.be/swq_u4iIP-g",authors:"Mullen, James; Mosier, Josh; Chakrabarti, Sounak; Chen, Anqi; White, Tyler; Losey, Dylan",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/LRA.2021.3111055",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"}]},{UID:59,citations:[],selected:!0,slug:"paper_59",title:"Nonverbal Leakage in Robots: Communication of Intentions through Seemingly Unintentional Behavior",abstract:'Human communication involves a number of nonverbal cues that are seemingly unintentional, unconscious, and automatic-both in their production and perception-and convey rich information on the emotional state and intentions of an individual. One family of such cues is called "nonverbal leakage." In this paper, we explore whether people can read nonverbal leakage cues-particularly gaze cues-in humanlike robots and make inferences on robots\u2019 intentions, and whether the physical design of the robot affects these inferences. We designed a gaze cue for Geminoid-a highly humanlike android-and Robovie-a robot with stylized, abstract humanlike features-that allowed the robots to "leak" information on what they might have in mind. In a controlled laboratory experiment, we asked participants to play a game of guessing with either of the robots and evaluated how the gaze cue affected participants\u2019 task performance. We found that the gaze cue did, in fact, lead to better performance, from which we infer that the cue led to attributions of mental states and intentionality. Our results have implications for robot design, particularly for designing expression of intentionality, and for our understanding of how people respond to human social cues when they are enacted by robots.',authors:"Mutlu, Bilge; Yamaoka, Fumitaka; Kanda, Takayuki; Ishiguro, Hiroshi; Hagita, Norihiro",venue:"ACM",year:2009,link:"https://doi.org/10.1145/1514095.1514110",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:60,citations:[],selected:!0,slug:"paper_60",title:"Visualizing Robot Intent for Object Handovers with Augmented Reality",abstract:"Humans are highly skilled in communicating their intent for when and where a handover would occur. However, even the state-of-the-art robotic implementations for handovers display a general lack of communication skills. This study aims to visualize the internal state and intent of robots for Human-to-Robot Handovers using Augmented Reality. Specifically, we aim to visualize 3D models of the object and the robotic gripper to communicate the robot's estimation of where the object is and the pose in which the robot intends to grasp the object. We tested this design via a user study with 16 participants, in which each participant handed over a cube-shaped object to the robot 12 times. Results show that visualizing robot intent using augmented reality substantially improves the subjective experience of the users for handovers. Results also indicate that the effectiveness of augmented reality is even more pronounced for the perceived safety and fluency of the interaction when the robot makes errors in localizing the object.",authors:"Newbury, Rhys; Cosgun, Akansel; Crowley-Davis, Tysha; Chan, Wesley P.; Drummond, Tom; Croft, Elizabeth",venue:"arXiv",year:2021,link:"https://doi.org/10.48550/arXiv.2103.04055",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:61,citations:[],selected:!0,slug:"paper_61",title:"Bio-inspired multi-robot communication through behavior recognition",abstract:"This paper focuses on enabling multi-robot teams to cooperatively perform tasks without the use of radio or acoustic communication. One key to more effective cooperative interaction in a multi-robot team is the ability to understand the behavior and intent of other robots. This is similar to the honey bee \u201cwaggle dance\u201d in which a bee can communicate the orientation and distance of a food source. In this similar manner, our heterogenous multi-robot team uses a specific behavior to indicate the location of mine-like objects (MLOs). Observed teammate action sequences can be learned to perform behavior recognition and task-assignment in the absence of communication. We apply Conditional Random Fields (CRFs) to perform behavior recognition as an approach to task monitoring in the absence of communication in a challenging underwater environment. In order to demonstrate the use of behavior recognition of an Autonomous Underwater Vehicle (AUV) in a cooperative task, we use trajectory based techniques for model generation and behavior discrimination in experiments using simulated scenario data. Results are presented demonstrating heterogenous teammate cooperation between an AUV and an Autonomous Surface Vehicle (ASV) using behavior recognition rather than radio or acoustic communication in a mine clearing task.",authors:"Novitzky, Michael; Pippin, Charles; Collins, Thomas R.; Balch, Tucker R.; West, Michael E.",venue:"IEEE",year:2012,link:"https://doi.org/10.1109/ROBIO.2012.6491061",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:62,citations:[],selected:!0,slug:"paper_62",title:"A Drink-Serving Mobile Social Robot Selects Who to Interact with Using Gaze",abstract:'Robots will soon deliver food and beverages in various environments. These robots will need to communicate their intention efficiently; for example, they should indicate who they are addressing. We conducted a real-world study of a water serving robot at a university cafeteria. The robot was operated in a Wizard-of-Oz manner. It approached and offered water to students having their lunch. Our analyses of the relationship between robot gaze direction and the likelihood that someone takes a drink show that if people do not already have a drink and the interaction is not dominated by an overly enthusiastic user, the robot\u2019s gaze behavior is effective in selecting an interaction partner even "in the wild".',authors:"Palinko, Oskar; Fischer, Kerstin; Ruiz Ramirez, Eduardo; Damsgaard Nissen, Lotte; Langedijk, Rosalyn M.",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3371382.3378339",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:63,citations:[],selected:!0,slug:"paper_63",title:"Fostering short-term human anticipatory behavior in human-robot collaboration",abstract:"The present study reports on a human-robot collaboration experiment involving an industrial task with the specific aim of exploring the effects of (i) fostering human anticipatory behavior towards the robot, through visual cues of the robot\u2019s next move and (ii) robot adaptiveness to the human actions through reducing its motion speed with respect to human movement\u2019s proximity. For investigating these effects a generic collaborative picking and sorting task was designed, implemented and tested by volunteer participants, in a Virtual Reality simulation environment. Results demonstrated that, showing robot\u2019s intent through anticipatory cues significantly increased team efficiency, human safety and collaborative fluency in conjunction with a positive subjective inclination towards the robot. Robot adaptiveness significantly increased human safety without decreasing task efficiency and fluency, compared to a control condition.",authors:"Psarakis, Loizos; Nathanael, Dimitris; Marmaras, Nicolas",venue:"ScienceDirect",year:2022,link:"https://doi.org/10.1016/j.ergon.2021.103241",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:64,citations:[],selected:!0,slug:"paper_64",title:"Communicating and controlling robot arm motion intent through mixed-reality head-mounted displays",abstract:"Efficient motion intent communication is necessary for safe and collaborative work environments with co-located humans and robots. Humans efficiently communicate their motion intent to other humans through gestures, gaze, and other non-verbal cues, and can replan their motions in response. However, robots often have difficulty using these methods. Many existing methods for robot motion intent communication rely on 2D displays, which require the human to continually pause their work to check a visualization. We propose a mixed-reality head-mounted display (HMD) visualization of the intended robot motion over the wearer\u2019s real-world view of the robot and its environment. In addition, our interface allows users to adjust the intended goal pose of the end effector using hand gestures. We describe its implementation, which connects a ROS-enabled robot to the HoloLens using ROS Reality, using MoveIt for motion planning, and using Unity to render the visualization. To evaluate the effectiveness of this system against a 2D display visualization and against no visualization, we asked 32 participants to label various arm trajectories as either colliding or non-colliding with blocks arranged on a table. We found a 15% increase in accuracy with a 38% decrease in the time it took to complete the task compared with the next best system. These results demonstrate that a mixed-reality HMD allows a human to determine where the robot is going to move more quickly and accurately than existing baselines.",authors:"Rosen, Eric; Whitney, David; Phillips, Elizabeth; Chien, Gary; Tompkin, James; Konidaris, George; Tellex, Stefanie",venue:"SAGE Publications",year:2019,link:"https://doi.org/10.1177/0278364919842925",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:65,citations:[],selected:!0,slug:"paper_65",title:"Third point of view augmented reality for robot intentions visualization",abstract:"Lightweight, head-up displays integrated in industrial helmets allow to provide contextual information for industrial scenarios such as in maintenance. Moving from single display and single camera solutions to stereo perception and display opens new interaction possibilities. In particular this paper addresses the case of information sharing by a Baxter robot displayed to the user overlooking at the real scene. System design and interaction ideas are being presented.",authors:"Ruffaldi, Emanuele; Brizzi, Filippo; Tecchia, Franco; Bacinelli, Sandro",venue:"Springer",year:2016,link:"https://doi.org/10.1007/978-3-319-40621-3_35",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Directional",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:66,citations:[],selected:!0,slug:"paper_66",title:"Communicating affect via flight path: exploring use of the laban effort system for designing affective locomotion paths",abstract:"People and animals use various kinds of motion in a multitude of ways to communicate their ideas and affective state, such as their moods or emotions. Further, people attribute affect and personalities to movements of even non-life like entities based solely on the style of their motions, e.g., the locomotion style of a geometric shape (how it moves about) can be interpreted as being shy, aggressive, etc. We investigate how robots can leverage this locomotion-style communication channel for communication with people. Specifically, our work deals with designing stylistic flying-robot locomotion paths for communicating affective state. To author and unpack the parameters of affect-oriented flying-robot locomotion styles we employ the Laban Effort System, a standard method for interpreting human motion commonly used in the performing arts. This paper describes our adaption of the Laban Effort System to author motions for flying robots, and the results of a formal experiment that investigated how various Laban Effort System parameters influence people's perception of the resulting robotic motions. We summarize with a set of guidelines for aiding designers in using the Laban Effort System to author flying robot motions to elicit desired affective responses.",authors:"Sharma, Megha; Hildebrandt, Dale; Newman, Gem; Young, James E.; Eskicioglu, Rasit",venue:"IEEE",year:2013,link:"https://doi.org/10.1109/HRI.2013.6483602",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:67,citations:[],selected:!0,slug:"paper_67",title:"Effect of Expressive Lights on Human Perception and Interpretation of Functional Robot",abstract:"Because appearance-constrained robots lack expressiveness, human users often find it hard to understand their behavior and intentions. To address this, expressive lights are considered to be an effective means for such robots to communicate with people. However, existing studies mainly focus on specific tasks or goals, leaving the knowledge of how expressive lights affect people\u2019s perception still unknown. In this pilot study, we investigate such a question by using a Roomba robot. We designed two light expressions, namely, green and low-intensity (GL) and red and high-intensity (RH). We used open-ended questions to evaluate people\u2019s perception and interpretation of the robot, which showed different light expressions as a way to communicate. Our findings reveal that simple light expressions can allow people to construct rich and complex interpretations of a robot\u2019s behavior, and such interpretations are heavily biased by the design of expressive lights.",authors:"Song, Sichao; Yamada, Seiji",venue:"ACM",year:2018,link:"https://doi.org/10.1145/3170427.3188547",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"}]},{UID:68,citations:[],selected:!0,slug:"paper_68",title:"Designing LED Lights for Communicating Gaze with Appearance-Constrained Robots",abstract:"Functional robots are generally restricted in appearance, thus lacking ways to express their intent. In human-human interaction, gaze is an important cue for providing information and regulating interaction. In this pilot study, we investigate how we can implement gaze behavior in functional robots since gaze communication can allow humans to read a robot's intent and adjust their behavior accordingly. We explore design principles based on LED lights as we consider LEDs to be easily installed in most robots while not introducing features that are too human-like (to prevent users from having high expectations). In the paper, we present a design interface that allows designers to explore the parameter space of an LED strip attached to a Roomba robot. We then summarize a set of design principles for optimally simulating light-based gazes. Finally, our suggested design is evaluated by a large group of participants, and their comments are discussed.",authors:"Song, Sichao; Yamada, Seiji",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ROMAN.2018.8525661",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:69,citations:[],selected:!0,slug:"paper_69",title:"Bioluminescence-Inspired Human-Robot Interaction: Designing Expressive Lights That Affect Human\u2019s Willingness to Interact with a Robot",abstract:"Bioluminescence is the production and emission of light by a living organism. It, as a means of communication, is of importance for the survival of various creatures. Inspired by bioluminescent light behaviors, we explore the design of expressive lights and evaluate the effect of such expressions on a human\xbbs perception of and attitude toward an appearance-constrained robot. Such robots are in urgent need of finding effective ways to present themselves and communicate their intentions due to a lack of social expressivity. We particularly focus on the expression of attractiveness and hostility because a robot would need to be able to attract or keep away human users in practical human-robot interaction (HRI) scenarios. In this work, we installed an LED lighting system on a Roomba robot and conducted a series of two experiments. We first worked through a structured approach to determine the best light expression designs for the robot to show attractiveness and hostility. This resulted in four recommended light expressions. Further, we performed a verification study to examine the effectiveness of such light expressions in a typical HRI context. On the basis of the findings, we offer design guidelines for expressive lights that HRI researchers and practitioners could readily employ.",authors:"Song, Sichao; Yamada, Seiji",venue:"ACM",year:2018,link:"https://doi.org/10.1145/3171221.3171249",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:70,citations:[],selected:!0,slug:"paper_70",title:"Visual Attention in Spoken Human-Robot Interaction",abstract:'Psycholinguistic studies of situated language processing have revealed that gaze in the visual environment is tightly coupled with both spoken language comprehension and production. It has also been established that interlocutors monitor the gaze of their partners, a phenomenon called "joint attention", as a further means for facilitating mutual understanding. We hypothesise that human-robot interaction will benefit when the robot\u2019s language-related gaze behaviour is similar to that of people, potentially providing the user with valuable non-verbal information concerning the robot\u2019s intended message or the robot\u2019s successful understanding. We report findings from two eye-tracking experiments demonstrating (1) that human gaze is modulated by both the robot speech and gaze, and (2) that human comprehension of robot speech is improved when the robot\u2019s real-time gaze behaviour is similar to that of humans.',authors:"Staudte, Maria; Crocker, Matthew W.",venue:"ACM",year:2009,link:"https://doi.org/10.1145/1514095.1514111",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Directional",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Description",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:71,citations:[],selected:!0,slug:"paper_71",title:"Communication of Intent in Assistive Free Flyers",abstract:"Assistive free-flyers (AFFs) are an emerging robotic platform with unparalleled flight capabilities that appear uniquely suited to exploration, surveillance, inspection, and telepresence tasks. However, unconstrained aerial movements may make it difficult for colocated operators, collaborators, and observers to understand AFF intentions, potentially leading to difficulties understanding whether operator instructions are being executed properly or to safety concerns if future AFF motions are unknown or difficult to predict. To increase AFF usability when working in close proximity to users, we explore the design of natural and intuitive flight motions that may improve AFF abilities to communicate intent while simultaneously accomplishing task goals. We propose a formalism for representing AFF flight paths as a series of motion primitives and present two studies examining the effects of modifying the trajectories and velocities of these flight primitives based on natural motion principles. Our first study found that modified flight motions might allow AFFs to more effectively communicate intent and, in our second study, participants preferred interacting with an AFF that used a manipulated flight path, rated modified flight motions as more natural, and felt safer around an AFF with modified motion. Our proposed formalism and findings highlight the importance of robot motion in achieving effective human-robot interactions.",authors:"Szafir, Daniel; Mutlu, Bilge; Fong, Terrence",venue:"ACM",year:2014,link:"https://doi.org/10.1145/2559636.2559672",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:72,citations:[],selected:!0,slug:"paper_72",title:"Communicating Directionality in Flying Robots",abstract:"Small flying robots represent a rapidly emerging family of robotic technologies with aerial capabilities that enable unique forms of assistance in a variety of collaborative tasks. Such tasks will necessitate interaction with humans in close proximity, requiring that designers consider human perceptions regarding robots flying and acting within human environments. We explore the design space regarding explicit robot communication of flight intentions to nearby viewers. We apply design constraints to robot flight behaviors, using biological and airplane flight as inspiration, and develop a set of signaling mechanisms for visually communicating directionality while operating under such constraints. We implement our designs on two commercial flyers, requiring little modification to the base platforms, and evaluate each signaling mechanism, as well as a no-signaling baseline, in a user study in which participants were asked to predict robot intent. We found that three of our designs significantly improved viewer response time and accuracy over the baseline and that the form of the signal offered tradeoffs in precision, generalizability, and perceived robot usability.",authors:"Szafir, Daniel; Mutlu, Bilge; Fong, Terry",venue:"ACM",year:2015,link:"https://doi.org/10.1145/2696454.2696475",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"}]},{UID:73,citations:[],selected:!0,slug:"paper_73",title:"Expressing thought: Improving robot readability with animation principles",abstract:"The animation techniques of anticipation and reaction can help create robot behaviors that are human readable such that people can figure out what the robot is doing, reasonably predict what the robot will do next, and ultimately interact with the robot in an effective way. By showing forethought before action and expressing a reaction to the task outcome (success or failure), we prototyped a set of human-robot interaction behaviors. In a 2 (forethought vs. none: between) \xd7 2 (reaction to outcome vs. none: between) \xd7 2 (success vs. failure task outcome: within) experiment, we tested the influences of forethought and reaction upon people's perceptions of the robot and the robot's readability. In this online video prototype experiment (N=273), we have found support for the hypothesis that perceptions of robots are influenced by robots showing forethought, the task outcome (success or failure), and showing goal-oriented reactions to those task outcomes. Implications for theory and design are discussed.",authors:"Takayama, Leila; Dooley, Doug; Ju, Wendy",venue:"IEEE",year:2011,link:"https://doi.org/10.1145/1957656.1957674",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:74,citations:[],selected:!0,slug:"paper_74",title:"The development and evaluation of Robot Light Skin: a novel robot signalling system to improve communication in industrial human\u2013robot collaboration",abstract:"In a human\u2013robot collaborative production system, the robot could make request for interaction or notify the human operator if an uncertainty arises. Conventional industrial tower lights were designed for generic machine signalling purposes which may not be the ultimate solution for robot signalling in a collaborative setting. In this type of system, human operators could be monitoring multiple robots while carrying out a manual task so it is important to minimise the diversion of their attention. This paper presents a novel robot signalling solution, the Robot Light Skin (RLS),which is an integrated signalling system that could be used on most articulated robots. Our experiment was conducted to validate this concept in terms of its effect on improving operator's reaction time, hit-rate, awareness and task performance. The results showed that participants reacted faster to the RLS as well as achieved higher hit-rate. An eye tracker was used in the experiment which shows a reduction in diversion away from the manual task when using the RLS. Future study should explore the effect of the RLS concept on large-scale systems and multi-robot systems.",authors:"Tang, Gilbert; Webb, Phil; Thrower, John",venue:"ScienceDirect",year:2019,link:"https://doi.org/10.1016/j.rcim.2018.08.005",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"}]},{UID:75,citations:[],selected:!0,slug:"paper_75",title:"Intuitive and Safe Interaction in Multi-User Human Robot Collaboration Environments through Augmented Reality Displays",abstract:"As autonomous collaborative robots are more widely used in work environments alongside humans it is of great importance to facilitate the communication between people and robotic systems, in a way that promotes safety and productivity. To this end, we propose an Augmented Reality (AR) based system that allows workers in a human-robot collaborative environment to interact with a robot while also receiving information regarding the robot state and plans that relate to the human\u2019s safety and trust, such as the intended movement of the robotic arm or the navigation plan of the mobile platform. To evaluate the effectiveness of the proposed system we conducted experiments with 13 participants, where two users had to work in the same workspace while being assisted by a mobile manipulator. We measured the task completion time as well as the robot idle time using our AR-based human-robot interaction system and compared them to a conventional setup without the use of augmented reality. Additional, subjective evaluations related to user satisfaction, system usability, perceived safety and trust showed that users assessed the system in a positive way and preferred AR visualization over more traditional interfaces.",authors:"Tsamis, Georgios; Chantziaras, Georgios; Giakoumis, Dimitrios; Kostavelis, Ioannis; Kargakos, Andreas; Tsakiris, Athanasios; Tzovaras, Dimitrios",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/RO-MAN50785.2021.9515474",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:76,citations:[],selected:!0,slug:"paper_76",title:"Communicating Robot Motion Intent with Augmented Reality",abstract:"Humans coordinate teamwork by conveying intent through social cues, such as gestures and gaze behaviors. However, these methods may not be possible for appearance-constrained robots that lack anthropomorphic or zoomorphic features, such as aerial robots. We explore a new design space for communicating robot motion intent by investigating how augmented reality (AR) might mediate human-robot interactions. We develop a series of explicit and implicit designs for visually signaling robot motion intent using AR, which we evaluate in a user study. We found that several of our AR designs significantly improved objective task efficiency over a baseline in which users only received physically-embodied orientation cues. In addition, our designs offer several trade-offs in terms of intent clarity and user perceptions of the robot as a teammate.",authors:"Walker, Michael; Hedayati, Hooman; Lee, Jennifer; Szafir, Daniel",venue:"ACM",year:2018,link:"https://doi.org/10.1145/3171221.3171253",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Directional",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:77,citations:[],selected:!0,slug:"paper_77",title:"Communicating robotic navigational intentions",abstract:"This paper presents a study on intention communication in a navigational context using a robotic wheelchair. The robotic wheelchair uses light projection to communicate its motion intentions. The novelty of the work is threefold: the communication of robot intentions to the passenger, the consideration of passenger and robot as a group (\u201cin-group\u201d) [1] who share motion intentions and the communication of the in-group intentions to other pedestrians (the \u201cout-group\u201d). A comparison in an autonomous navigation task where the robotic wheelchair autonomously navigates the environment with and without intention communication was performed showing that passengers and walking people found intention communication intuitive and helpful for passing by actions. Evaluation results significantly show human participant preference for having navigational intention communication for the wheelchair passenger and the person passing by it. Quantitative results show the motion of the person passing by the wheelchair with intention communication was significantly smoother compared to without intention communication.",authors:"Watanabe, Atsushi; Ikeda, Tetsushi; Morales, Yoichi; Shinozawa, Kazuhiko; Miyashita, Takahiro; Hagita, Norihiro",venue:"IEEE",year:2015,link:"https://doi.org/10.1109/IROS.2015.7354195",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:78,citations:[],selected:!0,slug:"paper_78",title:"A Laser Projection System for Robot Intention Communication and Human Robot Interaction",abstract:"In order to deploy service robots in environments where they encounter and/or cooperate with persons, one important key factor is human acceptance. Hence, information on which upcoming actions of the robot are based has to be made transparent and understandable to the human. However, considering the restricted power resources of mobile robot platforms, systems for visualization not only have to be expressive but also energy efficient. In this paper, we applied the well-known technique of laser scanning on a mobile robot to create a novel system for intention visualization and human-robot-interaction. We conducted user tests to compare our system to a low-power consuming LED video projector solution in order to evaluate the suitability for mobile platforms and to get human impressions of both systems. We can show that the presented system is preferred by most users in a dynamic test setup on a mobile platform.",authors:"Wengefeld, Tim; Hochemer, Dominik; Lewandowski, Benjamin; Kohler, Mona; Beer, Manuel; Gross, Horst-Michael",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/RO-MAN47096.2020.9223517",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Directional",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Description",parent:"Unregistered in Space"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:79,citations:[],selected:!0,slug:"paper_79",title:"Robot Gesture Sonification to Enhance Awareness of Robot Status and Enjoyment of Interaction",abstract:"We present a divergent approach to robotic sonification with the goal of improving the quality and safety of human-robot interactions. Sonification (turning data into sound) has been underutilized in robotics, and has broad potential to convey robotic movement and intentions to users without requiring visual engagement. We design and evaluate six different sonifications of movements for a robot with four degrees of freedom. Our sonification techniques include a direct mapping from each degree of freedom to pitch and timbre changes, emotion-based sound mappings, and velocity-based mappings using different types of sounds such as motors and music. We evaluate these sonifications using metrics for ease of use, enjoyment/appeal, and conveyance of movement information. Based on our results, we make recommendations to inform decisions for future robot sonification design. We suggest that when using sonification to improve safety of human-robot collaboration, it is necessary not only to convey sufficient information about movements, but also to convey that information in a pleasing and even social way to to enhance the human-robot relationship.",authors:"Zahray, Lisa; Savery, Richard; Syrkett, Liana; Weinberg, Gil",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/RO-MAN47096.2020.9223452",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"}]},{UID:80,citations:[],selected:!0,slug:"paper_80",title:"Expressive robot motion timing",abstract:"Our goal is to enable robots to time their motion in a way that is purposefully expressive of their internal states, making them more transparent to people. We start by investigating what types of states motion timing is capable of expressing, focusing on robot manipulation and keeping the path constant while systematically varying the timing. We find that users naturally pick up on certain properties of the robot (like confidence), of the motion (like naturalness), or of the task (like the weight of the object that the robot is carrying). We then conduct a hypothesis-driven experiment to tease out the directions and magnitudes of these effects, and use our findings to develop candidate mathematical models for how users make these inferences from the timing. We find a strong correlation between the models and real user data, suggesting that robots can leverage these models to autonomously optimize the timing of their motion to be expressive.",authors:"Zhou, Allan; Hadfield-Menell, Dylan; Nagabandi, Anusha; Dragan, Anca D.",venue:"ACM",year:2017,link:"https://doi.org/10.1145/2909824.3020221",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Continuous",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"}]},{UID:81,citations:[],selected:!0,slug:"paper_81",title:"Towards Explainable Shared Control using Augmented Reality",abstract:"Shared control plays a pivotal role in establishing effective human-robot interactions. Traditional control-sharing methods strive to complement a human's capabilities at safely completing a task, and thereby rely on users forming a mental model of the expected robot behaviour. However, these methods can often bewilder or frustrate users whenever their actions do not elicit the intended system response, forming a misalignment between the respective internal models of the robot and human. To resolve this model misalignment, we introduce Explainable Shared Control as a paradigm in which assistance and information feedback are jointly considered. Augmented reality is presented as an integral component of this paradigm, by visually unveiling the robot's inner workings to human operators. Explainable Shared Control is instantiated and tested for assistive navigation in a setup involving a robotic wheelchair and a Microsoft HoloLens with add-on eye tracking. Experimental results indicate that the introduced paradigm facilitates transparent assistance by improving recovery times from adverse events associated with model misalignment.",authors:"Zolotas, Mark; Demiris, Yiannis",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/IROS40897.2019.8968117",tags:[{name:"Intent Information",parent:null},{name:"Spatial Dimension",parent:"Intent Information"},{name:"Registered in Space",parent:"Spatial Dimension"},{name:"Local",parent:"Registered in Space"},{name:"Unregistered in Space",parent:"Spatial Dimension"},{name:"Symbol",parent:"Unregistered in Space"},{name:"Temporal Dimension",parent:"Intent Information"},{name:"Discrete",parent:"Temporal Dimension"},{name:"Signal",parent:"Unregistered in Space"},{name:"Continuous",parent:"Temporal Dimension"}]}],taxonomy:[[{name:"Intent Information",expanded:!0,parent:null,level:1,start:17,stop:23}],[{name:"Spatial Dimension",expanded:!0,parent:"Intent Information",level:2,start:17,stop:21},{name:"Temporal Dimension",expanded:!0,parent:"Intent Information",level:2,start:22,stop:23}],[{name:"Registered in Space",expanded:!0,parent:"Spatial Dimension",level:3,start:17,stop:18},{name:"Unregistered in Space",expanded:!0,parent:"Spatial Dimension",level:3,start:19,stop:21},{name:"Discrete",expanded:!0,parent:"Temporal Dimension",level:3,start:22,stop:22},{name:"Continuous",expanded:!0,parent:"Temporal Dimension",level:3,start:23,stop:23}],[{name:"Local",expanded:!0,parent:"Registered in Space",level:4,start:17,stop:17},{name:"Directional",expanded:!0,parent:"Registered in Space",level:4,start:18,stop:18},{name:"Description",expanded:!0,parent:"Unregistered in Space",level:4,start:19,stop:19},{name:"Symbol",expanded:!0,parent:"Unregistered in Space",level:4,start:20,stop:20},{name:"Signal",expanded:!0,parent:"Unregistered in Space",level:4,start:21,stop:23}]]},{name:"Intent Location",data:[{UID:5,citations:[],selected:!0,slug:"paper_5",title:"Projecting robot intentions into human environments",abstract:"Trained human co-workers can often easily predict each other's intentions based on prior experience. When collaborating with a robot coworker, however, intentions are hard or impossible to infer. This difficulty of mental introspection makes human-robot collaboration challenging and can lead to dangerous misunderstandings. In this paper, we present a novel, object-aware projection technique that allows robots to visualize task information and intentions on physical objects in the environment. The approach uses modern object tracking methods in order to display information at specific spatial locations taking into account the pose and shape of surrounding objects. As a result, a human co-worker can be informed in a timely manner about the safety of the workspace, the site of next robot manipulation tasks, and next subtasks to perform. A preliminary usability study compares the approach to collaboration approaches based on monitors and printed text. The study indicates that, on average, the user effectiveness and satisfaction is higher with the projection based approach.",authors:"Andersen, Rasmus S.; Madsen, Ole; Moeslund, Thomas B.; Amor, Heni Ben",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745145",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:6,citations:[],selected:!0,slug:"paper_6",title:"Designing Multimodal Intent Communication Strategies for Conflict Avoidance in Industrial Human-Robot Teams",abstract:"Robot-to-human intent communication has been proposed as a method of enabling fluent coordination in human-robot teams. Prior research has focused on identifying modalities by which intent information can be accurately communicated, but has not yet studied whether intent communication enables fluent or safer coordination in human-robot teams in which intent communication is only supportive to the team's primary task. To address this question, we conduct a study (N = 29) in a mock collaborative manufacturing scenario in which motion-based and display-based intent communication approaches are evaluated under varying penalties for failing to coordinate safely. Subjective and objective measures of team fluency suggest that although intent communication supports fluent coordination, using a purely motion-based or a purely display-based approach may not be the most effective strategy. Although multimodal intent communication did not significantly improve upon unimodal approaches, merging both motion-based and display-based intent communication seems to combine the strengths of both approaches. Interestingly, results also suggest that contrary to theoretical predictions, the positive effect of intent communication is generally robust to teaming scenarios that require members to operate concurrently.",authors:"Aubert, Miles C.; Bader, Hayden; Hauser, Kris",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ROMAN.2018.8525557",tags:[{name:"Intent Location",parent:null},{name:"On-World",parent:"Intent Location"},{name:"Hand-Held",parent:"On-Human"},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:7,citations:[],selected:!0,slug:"paper_7",title:"Legible light communications for factory robots",abstract:"This work focuses on methods to improve mobile robot legibility in factories using lights. Implementation and evaluation were done at a robotics company that manufactures factory robots that work in human spaces. Three new sets of communicative lights were created and tested on the robots, integrated into the company's software stack and compared to the industry default lights that currently exist on the robots. All three newly designed light sets outperformed the industry default. Insights from this work have been integrated into software releases across North America.",authors:"Bacula, Alexandra; Mercer, Jason; Knight, Heather",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3371382.3378305",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:8,citations:[],selected:!0,slug:"paper_8",title:"Enhancing human understanding of a mobile robot\u2019s state and actions using expressive lights",abstract:"In order to be successfully integrated into human-populated environments, mobile robots need to express relevant information about their state to the outside world. In particular, animated lights are a promising way to express hidden robot state information such that it is visible at a distance. In this work, we present an online study to evaluate the effect of robot communication through expressive lights on people's understanding of the robot's state and actions. In our study, we use the CoBot mobile service robot with our light interface, designed to express relevant robot information to humans. We evaluate three designed light animations on three corresponding scenarios for each, for a total of nine scenarios. Our results suggest that expressive lights can play a significant role in helping people accurately hypothesize about a mobile robot's state and actions from afar when minimal contextual clues are present. We conclude that lights could be generally used as an effective non-verbal communication modality for mobile robots in the absence of, or as a complement to, other modalities.",authors:"Baraka, Kim; Rosenthal, Stephanie; Veloso, Manuela",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745187",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:9,citations:[],selected:!0,slug:"paper_9",title:"A flexible optimization-based method for synthesizing intent-expressive robot arm motion",abstract:"We present an approach to synthesize robot arm trajectories that effectively communicate the robot\u2019s intent to a human collaborator while achieving task goals. Our approach uses nonlinear constrained optimization to encode task requirements and desired motion properties. Our implementation allows for a wide range of constraints and objectives. We introduce a novel objective function to optimize robot arm motions for intent-expressiveness that works in a range of scenarios and robot arm types. Our formulation supports experimentation with different theories of how viewers interpret robot motion. Through a series of human-subject experiments on real and simulated robots, we demonstrate that our method leads to improved collaborative performance against other methods, including the current state of the art. These experiments also show how our perception heuristic can affect collaborative outcomes.",authors:"Bodden, Christopher; Rakita, Daniel; Mutlu, Bilge; Gleicher, Michael",venue:"SAGE Publications",year:2018,link:"https://doi.org/10.1177/0278364918792295",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:10,citations:[],selected:!0,slug:"paper_10",title:"Transparent Robot Behavior by Adding Intuitive Visual and Acoustic Feedback to Motion Replanning",abstract:"Nowadays robots are able to work safely close to humans. They are light-weight, intrinsically safe and capable of avoiding obstacles as well as understand and predict human motions. In this collaborative scenario, the communication between humans and robots is a fundamental aspect to achieve good efficiency and ergonomics in the task execution. A lot of research has been made related to robot understanding and prediction of the human behavior, allowing the robot to replan its motion trajectories. This work is focused on the communication of the robot's intentions to the human to make its goals and planned trajectories easily understandable. Visual and acoustic information has been added to give the human an intuitive feedback to immediately understand the robot's plan. This allows a better interaction and makes the humans feel more comfortable, without any feeling of anxiety related to the unpredictability of the robot motion. Experiments have been conducted in a collaborative assembly scenario. The results of these tests were collected in questionnaires, in which the humans reported the differences and improvements they experienced using the feedback communication system.",authors:"Bolano, Gabriele; Roennau, Arne; Dillmann, Ruediger",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ROMAN.2018.8525671",tags:[{name:"Intent Location",parent:null},{name:"On-World",parent:"Intent Location"},{name:"Hand-Held",parent:"On-Human"}]},{UID:11,citations:[],selected:!0,slug:"paper_11",title:"Deploying Multi-Modal Communication Using Augmented Reality in a Shared Workspace",abstract:"Robots are no longer working isolated in safety fences and Human-Robot Collaboration (HRC) is becoming one of the most promising topic of research to improve the efficiency in many application scenarios. Sharing the same workspace, both human and robot should clearly understand the intentions and motions of each other, in order to enable an efficient and effective interaction. In this work we propose an AR-based system to show the robot planned motion and target to the worker. We focused on representing this information in an intuitive way for inexperienced users. We introduced a multi-modal communication feedback in order to enable the user to agree with or change the robot plan using gestures and speech. The effectiveness of the system has been evaluated with test cases performed by a group of testers with no robotic experience. The results showed that the system helped the user to better understand the robot intentions and planned motion, improving the ergonomics and trust in the interaction. Furthermore, the evaluation included the rating of the different input modalities provided, in order to compare the different ways of communication proposed.",authors:"Bolano, Gabriele; Fu, Yuchao; Roennau, Arne; Dillmann, Ruediger",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/UR52253.2021.9494689",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Head-Attached",parent:"On-Human"}]},{UID:12,citations:[],selected:!0,slug:"paper_12",title:"Using Spatial and Temporal Contrast for Fluent Robot-Human Hand-Overs",abstract:"For robots to get integrated in daily tasks assisting humans, robot-human interactions will need to reach a level of fluency close to that of human-human interactions. In this paper we address the fluency of robot-human hand-overs. From an observational study with our robot HERB, we identify the key problems with a baseline hand-over action. We find that the failure to convey the intention of handing over causes delays in the transfer, while the lack of an intuitive signal to indicate timing of the hand-over causes early, unsuccessful attempts to take the object. We propose to address these problems with the use of spatial contrast, in the form of distinct hand-over poses, and temporal contrast, in the form of unambiguous transitions to the hand-over pose. We conduct a survey to identify distinct hand-over poses, and determine variables of the pose that have most communicative potential for the intent of handing over. We present an experiment that analyzes the effect of the two types of contrast on the fluency of hand-overs. We find that temporal contrast is particularly useful in improving fluency by eliminating early attempts of the human.",authors:"Cakmak, Maya; Srinivasa, Siddhartha S.; Lee, Min Kyung; Kiesler, Sara; Forlizzi, Jodi",venue:"ACM",year:2011,link:"https://doi.org/10.1145/1957656.1957823",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:13,citations:[],selected:!0,slug:"paper_13",title:"Communication Through Motion: Legibility of Multi-Robot Systems",abstract:"The interaction between a user and a multi-robot system in a shared environment is a relatively uncharted topic. But, as these types of systems will increase in the future years, an efficient way of communication is necessary. To this aim, it is interesting to discover if a multi-robot system can communicate its intentions exploiting only some motion-variables, which are characteristics of the motion of the robots. This study is about the legibility of a multi-robot system: in particular, we focus on the influence of these motion-variables on the legibility of more than one group of robots that move in a shared environment with the user. These motion-variables are: trajectory, dispersion and stiffness. They are generally used to define the motion of a group of mobile robots. Trajectory and dispersion were found relevant for the correctness of the communication between the user and the multi-robot system, while stiffness was found relevant for the rapidity of communication. The analysis of the influence of the motion-variables was carried out with an ANOVA (analysis of variance) based on a series of data coming from an experimental campaign conducted in a virtual reality set-up.",authors:"Capelli, Beatrice; Secchi, Cristian; Sabattini, Lorenzo",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/MRS.2019.8901100",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:14,citations:[],selected:!0,slug:"paper_14",title:"Emotion encoding in human-drone interaction",abstract:"Drones are becoming more popular and may soon be ubiquitous. As they enter our everyday environments, it becomes critical to ensure their usability through natural Human-Drone Interaction (HDI). Previous work in Human-Robot Interaction (HRI) shows that adding an emotional component is part of the key to success in robots' acceptability. We believe the adoption of personal drones would also benefit from adding an emotional component. This work defines a range of personality traits and emotional attributes that can be encoded in drones through their flight paths. We present a user study (N=20) and show how well three defined emotional states can be recognized. We draw conclusions on interaction techniques with drones and feedback strategies that use the drone's flight path and speed.",authors:"Cauchard, Jessica R.; Zhai, Kevin Y.; Spadafora, Marco; Landay, James A.",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/HRI.2016.7451761",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:15,citations:[],selected:!0,slug:"paper_15",title:"Using nonverbal signals to request help during human-robot collaboration",abstract:"Non-humanoid robots are becoming increasingly utilized for collaborative tasks that rely on each collaborator's ability to effectively convey their mental state while accurately estimating and interpreting their partner's knowledge, intent, and actions. During these tasks, it may be beneficial or even necessary for the human collaborator to assist the robot. Consequently, we explore the use of nonverbal signals to request help during a collaborative task. We focus on light and sound as they are commonly used communication channels across many domains. This paper analyzes the effectiveness of three nonverbal help signals that vary in urgency. Our results show that these signals significantly influence the human collaborator's and their perception of the collaboration.",authors:"Cha, Elizabeth; Mataric, Maja",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/IROS.2016.7759744",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:16,citations:[],selected:!0,slug:"paper_16",title:"Bi-directional navigation intent communication using spatial augmented reality and eye-tracking glasses for improved safety in human\u2013robot interaction",abstract:"Safety, legibility and efficiency are essential for autonomous mobile robots that interact with humans. A key factor in this respect is bi-directional communication of navigation intent, which we focus on in this article with a particular view on industrial logistic applications. In the direction robot-to-human, we study how a robot can communicate its navigation intent using Spatial Augmented Reality (SAR) such that humans can intuitively understand the robot\u2019s intention and feel safe in the vicinity of robots. We conducted experiments with an autonomous forklift that projects various patterns on the shared floor space to convey its navigation intentions. We analyzed trajectories and eye gaze patterns of humans while interacting with an autonomous forklift and carried out stimulated recall interviews (SRI) in order to identify desirable features for projection of robot intentions. In the direction human-to-robot, we argue that robots in human co-habited environments need human-aware task and motion planning to support safety and efficiency, ideally responding to people\u2019s motion intentions as soon as they can be inferred from human cues. Eye gaze can convey information about intentions beyond what can be inferred from the trajectory and head pose of a person. Hence, we propose eye-tracking glasses as safety equipment in industrial environments shared by humans and robots. In this work, we investigate the possibility of human-to-robot implicit intention transference solely from eye gaze data and evaluate how the observed eye gaze patterns of the participants relate to their navigation decisions. We again analyzed trajectories and eye gaze patterns of humans while interacting with an autonomous forklift for clues that could reveal direction intent. Our analysis shows that people primarily gazed on that side of the robot they ultimately decided to pass by. We discuss implications of these results and relate to a control approach that uses human gaze for early obstacle avoidance.",authors:"Chadalavada, Ravi Teja; Andreasson, Henrik; Schindler, Maike; Palm, Rainer; Lilienthal, Achim J.",venue:"ScienceDirect",year:2020,link:"https://doi.org/10.1016/j.rcim.2019.101830",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:17,citations:[],selected:!0,slug:"paper_17",title:"Projection-Aware Task Planning and Execution for Human-in-the-Loop Operation of Robots in a Mixed-Reality Workspace",abstract:"Recent advances in mixed-reality technologies have renewed interest in alternative modes of communication for human-robot interaction. However, most of the work in this direction has been confined to tasks such as teleoperation, simulation or explication of individual actions of a robot. In this paper, we will discuss how the capability to project intentions affect the task planning capabilities of a robot. Specifically, we will start with a discussion on how projection actions can be used to reveal information regarding the future intentions of the robot at the time of task execution. We will then pose a new planning paradigm - projection-aware planning - whereby a robot can trade off its plan cost with its ability to reveal its intentions using its projection actions. We will demonstrate each of these scenarios with the help of a joint human-robot activity using the HoloLens.",authors:"Chakraborti, Tathagata; Sreedharan, Sarath; Kulkarni, Anagha; Kambhampati, Subbarao",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/IROS.2018.8593830",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Head-Attached",parent:"On-Human"}]},{UID:18,citations:[],selected:!0,slug:"paper_18",title:"Negotiation based Human-Robot Collaboration via Augmented Reality",abstract:"Effective human-robot collaboration (HRC) requires extensive communication among the human and robot teammates, because their actions can potentially produce conflicts, synergies, or both. We develop a novel augmented reality (AR) interface to bridge the communication gap between human and robot teammates. Building on our AR interface, we develop an AR-mediated, negotiation-based (ARN) framework for HRC. We have conducted experiments both in simulation and on real robots in an office environment, where multiple mobile robots work on delivery tasks. The robots could not complete the tasks on their own, but sometimes need help from their human teammate, rendering human-robot collaboration necessary. Results suggest that ARN significantly reduced the human-robot team's task completion time compared to a non-AR baseline approach.",authors:"Chandan, Kishan; Kudalkar, Vidisha; Li, Xiang; Zhang, Shiqi",venue:"arXiv",year:2019,link:"https://doi.org/10.48550/arXiv.1909.11227",tags:[{name:"Intent Location",parent:null},{name:"On-World",parent:"Intent Location"},{name:"Hand-Held",parent:"On-Human"}]},{UID:19,citations:[],selected:!0,slug:"paper_19",title:"Avoiding Human-Robot Collisions Using Haptic Communication",abstract:"Fully autonomous navigation in populated environments is still a challenging problem for mobile robots. This paper explores the idea of using active human-robot communication to facilitate navigation tasks. We propose to convey a robot's intent to human users via a wearable haptic interface. The interface can display distinct haptic cues by modulating vibration amplitudes and patterns. We applied the concept to a single human/single robot orthogonal encounter scenario, where one of the two parties has to yield the right of way to avoid collision. Under certain conditions, the robot's intent (to yield to the human or not) is revealed to the human via the haptic interface prior to the interaction. We conducted an experiment with 10 users, in which the robot was teleoperated as a substitute for autonomy. Results show that, when given priority, users become more risk-accepting and use different strategies to navigate the collision scenario than when the robot takes priority or there is no haptic communication channel. In addition, we propose a social-force based model to predict human movement during navigation. The effect of communication can be explained as a shift in the user's safety buffer and expectation of the robot's future velocity.",authors:"Che, Yuhang; Sun, Cuthbert T.; Okamura, Allison M.",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ICRA.2018.8460946",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Hand-Held",parent:"On-Human"}]},{UID:20,citations:[],selected:!0,slug:"paper_20",title:"Efficient and Trustworthy Social Navigation via Explicit and Implicit Robot\u2013Human Communication",abstract:"In this article, we present a planning framework that uses a combination of implicit (robot motion) and explicit (visual/audio/haptic feedback) communication during mobile robot navigation. First, we developed a model that approximates both continuous movements and discrete behavior modes in human navigation, considering the effects of implicit and explicit communication on human decision-making. The model approximates the human as an optimal agent, with a reward function obtained through inverse reinforcement learning. Second, a planner uses this model to generate communicative actions that maximize the robot's transparency and efficiency. We implemented the planner on a mobile robot, using a wearable haptic device for explicit communication. In a user study of an indoor human-robot pair orthogonal crossing situation, the robot is able to actively communicate its intent to users in order to avoid collisions and facilitate efficient trajectories. Results show that the planner generated plans that are easier to understand, reduce users` effort, and increase users' trust of the robot, compared to simply performing collision avoidance. The key contribution of this article is the integration and analysis of explicit communication (together with implicit communication) for social navigation.",authors:"Che, Yuhang; Okamura, Allison M.; Sadigh, Dorsa",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/TRO.2020.2964824",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Hand-Held",parent:"On-Human"},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:21,citations:[],selected:!0,slug:"paper_21",title:"Touched by a Robot: An Investigation of Subjective Responses to Robot-Initiated Touch",abstract:"By initiating physical contact with people, robots can be more useful. For example, a robotic caregiver might make contact to provide physical assistance or facilitate communication. So as to better understand how people respond to robot-initiated touch, we conducted a 2x2 between-subjects experiment with 56 people in which a robotic nurse autonomously touched and wiped the subject\u2019s forearm. Our independent variables were whether or not the robot verbally warned the person before contact, and whether the robot verbally indicated that the touch was intended to clean the person\u2019s skin (instrumental touch) or to provide comfort (affective touch). On average, regardless of the treatment, participants had a generally positive subjective response. However, with instrumental touch people responded significantly more favorably. Since the physical behavior of the robot was the same for all trials, our results demonstrate that the perceived intent of the robot can significantly influence a person\u2019s subjective response to robot-initiated touch. Our results suggest that roboticists should consider this factor in addition to the mechanics of physical interaction. Unexpectedly, we found that participants tended to respond more favorably without a verbal warning. Although inconclusive, our results suggest that verbal warnings prior to contact should be carefully designed, if used at all.",authors:"Chen, Tiffany L.; King, Chih-Hung; Thomaz, Andrea L.; Kemp, Charles C.",venue:"ACM",year:2011,link:"https://doi.org/10.1145/1957656.1957818",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:22,citations:[],selected:!0,slug:"paper_22",title:"Dynamic Path Visualization for Human-Robot Collaboration",abstract:"Augmented reality technology can enable robots to visualize their future actions giving users crucial information to avoid collisions and other conflicting actions. Although a robot\u2019s entire action plan could be visualized (such as the output of a navigational planner), how far into the future it is appropriate to display the robot\u2019s plan is unknown. We developed a dynamic path visualizer that projects the robot\u2019s motion intent at varying lengths depending on the complexity of the upcoming path. We tested our approach in a virtual game where participants were tasked to collect and deliver gems to a robot that moves randomly towards a grid of markers in a confined area. Preliminary results on a small sample size indicate no significant effect on task performance; however, open-ended responses reveal participants preference towards visuals that show longer path projections.",authors:"Cleaver, Andre; Tang, Darren Vincent; Chen, Victoria; Short, Elaine Schaertl; Sinapov, Jivko",venue:"ACM",year:2021,link:"https://doi.org/10.1145/3434074.3447188",tags:[{name:"Intent Location",parent:null},{name:"On-World",parent:"Intent Location"},{name:"Hand-Held",parent:"On-Human"}]},{UID:23,citations:[],selected:!0,slug:"paper_23",title:"MIRO: A Versatile Biomimetic Edutainment Robot",abstract:"Here we present MIRO, a companion robot designed to engage users in science and robotics via edutainment. MIRO is a robot that is biomimetic in aesthetics, morphology, behaviour, and control architecture. In this paper, we review how these design choices affect its suitability for a companionship role. In particular, we consider how MIRO\u2019s emulation of familiar mammalian body language as one component of a broader biomimetic expressive system provides effective communication of emotional state and intent. We go on to discuss how these features contribute to MIRO\u2019s potential in other domains such as healthcare, education, and research.",authors:"Collins, Emily C.; Prescott, Tony J.; Mitchinson, Ben; Conran, Sebastian",venue:"ACM",year:2015,link:"https://doi.org/10.1145/2832932.2832978",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:24,citations:[],selected:!0,slug:"paper_24",title:"Spatial augmented reality as a method for a mobile robot to communicate intended movement",abstract:"Our work evaluates a mobile robot\u2019s ability to communicate intended movements to humans via projection of visual arrows and a simplified map. Humans utilize a variety of techniques to signal intended movement in a co-occupied space. We evaluated an augmented reality projection provided by the robot. The projection is on the floor and consists of arrows and a simplified map. Two pilots and one quasi-experiment were conducted to examine the effectiveness of visual projection of arrows by a robot for signaling intended movement. The pilot work demonstrates the effectiveness of utilizing arrows as a communication medium. The experiment examined the effectiveness of a simplified map and arrows for signaling the short-, mid-range, and long-term intended movement. Two pilot experiments confirm that arrows are an effective symbol for a robot to use to signal intent. A field experiment demonstrates that a robot can use a projected arrow and simplified map to signal its intended movement and people understand the projection for upcoming short-, medium-, and long-term movement. Augmented reality, such as projected arrows and simplified map, are an effective tool for robots to use when signaling their upcoming movement to humans. Telepresence robots in organizations, museum docents, information kiosks, hospital assistants, factories, and as members of search and rescue teams are typical applications where mobile robots reside and interact with people.",authors:"Coovert, Michael D.; Lee, Tiffany; Shindev, Ivan; Sun, Yu",venue:"ScienceDirect",year:2014,link:"https://doi.org/10.1016/j.chb.2014.02.001",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:25,citations:[],selected:!0,slug:"paper_25",title:"Multimodal Interaction with an Autonomous Forklift",abstract:'We describe a multimodal framework for interacting with an autonomous robotic forklift. A key element enabling effective interaction is a wireless, handheld tablet with which a human supervisor can command the forklift using speech and sketch. Most current sketch interfaces treat the canvas as a blank slate. In contrast, our interface uses live and synthesized camera images from the forklift as a canvas, and augments them with object and obstacle information from the world. This connection enables users to "draw on the world," enabling a simpler set of sketched gestures. Our interface supports commands that include summoning the forklift and directing it to lift, transport, and place loads of palletized cargo. We describe an exploratory evaluation of the system designed to identify areas for detailed study.Our framework incorporates external signaling to interact with humans near the vehicle. The robot uses audible and visual annunciation to convey its current state and intended actions. The system also provides seamless autonomy handoff: any human can take control of the robot by entering its cabin, at which point the forklift can be operated manually until the human exits.',authors:"Correa, Andrew; Walter, Matthew R.; Fletcher, Luke; Glass, Jim; Teller, Seth; Davis, Randall",venue:"ACM",year:2010,link:"https://doi.org/10.1109/HRI.2010.5453188",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Hand-Held",parent:"On-Human"},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:26,citations:[],selected:!0,slug:"paper_26",title:"LED Strip Based Robot Movement Intention Signs for Human-Robot Interactions",abstract:"As a new kind of robots, called cooperative robots, are more commonly used in industry, a new way of communication is becoming more important due to the increasing number of closer cooperation between human and robots. This paper proposes the idea behind a novel method of using visual communication between cobots and humans focusing mainly on the field of industrial robotics. This device can decrease the mental stress experienced by the coworker and can increase the trust resulting in a closer to ergonomic workspace from the coworker viewpoint. Other possible usage is also discussed.",authors:"Domonkos, Mark; Dombi, Zoltan; Botzheim, Janos",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/CINTI51262.2020.9305854",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:27,citations:[],selected:!0,slug:"paper_27",title:"Generating legible motion",abstract:"Legible motion \u2014 motion that communicates its intent to a human observer \u2014 is crucial for enabling seamless human-robot collaboration. In this paper, we propose a functional gradient optimization technique for autonomously generating legible motion. Our algorithm optimizes a legibility metric inspired by the psychology of action interpretation in humans, resulting in motion trajectories that purposefully deviate from what an observer would expect in order to better convey intent. A trust region constraint on the optimization ensures that the motion does not become too surprising or unpredictable to the observer. Our studies with novice users that evaluate the resulting trajectories support the applicability of our method and of such a trust region. They show that within the region, legibility as measured in practice does significantly increase. Outside of it, however, the trajectory becomes confusing and the users\u2019 confidence in knowing the robot\u2019s intent significantly decreases.",authors:"Dragan, Anca and Srinivasa, Siddhartha",venue:"roboticsproceedings.org",year:2013,link:"http://www.roboticsproceedings.org/rss09/p24.pdf",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:28,citations:[],selected:!0,slug:"paper_28",title:"Effects of Robot Motion on Human-Robot Collaboration",abstract:"Most motion in robotics is purely functional, planned to achieve the goal and avoid collisions. Such motion is great in isolation, but collaboration affords a human who is watching the motion and making inferences about it, trying to coordinate with the robot to achieve the task. This paper analyzes the benefit of planning motion that explicitly enables the collaborator\u2019s inferences on the success of physical collaboration, as measured by both objective and subjective metrics. Results suggest that legible motion, planned to clearly express the robot\u2019s intent, leads to more fluent collaborations than predictable motion, planned to match the collaborator\u2019s expectations. Furthermore, purely functional motion can harm coordination, which negatively affects both task efficiency, as well as the participants\u2019 perception of the collaboration.",authors:"Dragan, Anca D.; Bauman, Shira; Forlizzi, Jodi; Srinivasa, Siddhartha S.",venue:"ACM",year:2015,link:"https://doi.org/10.1145/2696454.2696473",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:29,citations:[],selected:!0,slug:"paper_29",title:"Investigation of Communicative Flight Paths for Small Unmanned Aerial Systems * This work was supported by NSF NRI 1638099",abstract:"This project seeks to generate small Unmanned Aerial System (sUAS) flight paths that are broadly understood by the general population and can communicate states about both the sUAS and its understanding of the world. Previous work in sUAS flight paths has sought to communicate intent, destination, or emotion of the system without focusing on concrete states (e.g., low battery, landing, etc.). This work leverages biologically-based flight paths and experimental methodologies from human-human and human-humanoid robot interactions to assess the understanding of avian flight paths to communicate sUAS states to novice users. If successful, this work should inform: the human-robot interaction community about the perception of flight paths, sUAS manufacturers on how their systems could communicate with both operators and bystanders, and end users on ways to communicate with others when flying systems in public spaces. General design implications and future directions of work are suggested to build on the results here, which suggest that novice users gravitate towards labels they understand (draw attention and landing) while avoiding more technical labels (lost sensor).",authors:"Duncan, Brittany A.; Beachly, Evan; Bevins, Alisha; Elbaum, Sebasitan; Detweiler, Carrick",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ICRA.2018.8462871",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:30,citations:[],selected:!0,slug:"paper_30",title:"Follow me: Communicating intentions with a spherical robot",abstract:"In recent years, robots have gradually become incorporated in our society and therefore play more relevant role in social environments. These robots vary in form, some being more anthropomorphic than others. This, creates a need to study their interaction with the world. In this paper we used Sphero and BB-8, two robots with a simple spherical body devoid of verbal and other complex communication methods, to investigate how they can communicate intention to people. A set of behaviors based on pet behaviors was designed and tested in a controlled experiment, where the robot's aim was to convince a participant to follow it. We concluded that the use of these behaviors allows a robot to effectively communicate intention as well as create a bond with the participant, who would treat it as an equal, thereby engaging it in social interactions such as playing with it or talking to it.",authors:"Faria, Miguel; Costigliola, Andrea; Alves-Oliveira, Patricia; Paiva, Ana",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745189",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:31,citations:[],selected:!0,slug:"paper_31",title:"\u201cMe and you together\u201d movement impact in multi-user collaboration tasks",abstract:"This paper presents a study on collaborative manipulation between an autonomous robot and multiple users. We investigate how different motion types impact people's ability to understand the robot's goals in a multi-user scenario. We propose an approach based on Collaborative Probabilistic Movement Primitives to generate the robot's movements, exploiting predictability and legibility of movement to express intentions through motion. We compare the impact on the interaction of using only either predictable or legible movements, and propose a third approach - hybrid motion - that selects, in each situation, whether to execute a predictable motion or a legible motion, depending on what the robot perceives as more efficient for the multi-user collaboration effort. To test the impact of the three motion types in the context of a collaborative task, we run a user study using a Baxter robot that autonomously serves cups of water to three users upon request. Our results show that, in the particular case where all users simultaneously request water, the hybrid motion performs better than the other two.",authors:"Faria, Miguel; Silva, Rui; Alves-Oliveira, Patricia; Melo, Francisco S.; Paiva, Ana",venue:"IEEE",year:2017,link:"https://doi.org/10.1109/IROS.2017.8206109",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:32,citations:[],selected:!0,slug:"paper_32",title:"Understanding Robots: Making Robots More Legible in Multi-Party Interactions",abstract:"In this work we explore implicit communication between humans and robots\u2014through movement\u2014in multi-party (or multi-user) interactions. In particular, we investigate how a robot can move to better convey its intentions using legible movements in multi-party interactions. Current research on the application of legible movements has focused on single-user interactions, causing a vacuum of knowledge regarding the impact of such movements in multi-party interactions. We propose a novel approach that extends the notion of legible motion to multi-party settings, by considering that legibility depends on all human users involved in the interaction, and should take into consideration how each of them perceives the robot\u2019s movements from their respective points-of-view. We show, through simulation and a user study, that our proposed model of multi-user legibility leads to movements that, on average, optimize the legibility of the motion as perceived by the group of users. Our model creates movements that allow each human to more quickly and confidently understand what are the robot\u2019s intentions, thus creating safer, clearer and more efficient interactions and collaborations.",authors:"Faria, Miguel; Melo, Francisco S.; Paiva, Ana",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/RO-MAN50785.2021.9515485",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:33,citations:[],selected:!0,slug:"paper_33",title:"Between legibility and contact: The role of gaze in robot approach",abstract:"In this paper, we explore experimentally the possible tradeoff between gaze to the user and gaze to the path in robot approach. While some previous work indicates that gaze towards the user increases perceived safety because the user feels recognized, other work indicates that it is legibility of the robot's actions that put users at ease. If the robot does not drive up to the person in a straight line directly, the robot can either continuously look at the person and thus maintain eye contact, or indicate its path through its gaze behavior, increasing legibility. In an experiment with N=36 participants, we tested the tradeoff between legibility and eye contact. The behavioral results show that users are significantly more at ease with the robot that gazes at them than with the robot that looks where it is going, measured by the number of instances of glances away from the robot. Likewise, the participants rate the robot that looks at them continuously as more intelligent and more cooperative. Thus, participants value mutual gaze higher than legibility.",authors:"Fischer, Kerstin; Jensen, Lars C.; Suvei, Stefan-Daniel; Bodenhagen, Leon",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745186",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:34,citations:[],selected:!0,slug:"paper_34",title:"Investigation of Unmanned Aerial Vehicle Gesture Perceptibility and Impact of Viewpoint Variance<sup>*</sup>",abstract:"Unmanned Aerial Vehicle (UAV) flight paths have been shown to communicate meaning to human observers, similar to human gestural communication. This paper presents the results of a UAV gesture perception study designed to assess how observer viewpoint perspective may impact how humans perceive the shape of UAV gestural motion. Robot gesture designers have demonstrated that robots can indeed communicate meaning through gesture; however, many of these results are limited to an idealized range of viewer perspectives and do not consider how the perception of a robot gesture may suffer from obfuscation or self-occlusion from some viewpoints. This paper presents the results of three online user-studies that examine participants' ability to accurately perceive the intended shape of two-dimensional UAV gestures from varying viewer perspectives. We used a logistic regression model to characterize participant gesture classification accuracy, demonstrating that viewer perspective does impact how participants perceive the shape of UAV gestures. Our results yielded a viewpoint angle threshold from beyond which participants were able to assess the intended shape of a gesture's motion with 90% accuracy. We also introduce a perceptibility score to capture user confidence, time to decision, and accuracy in labeling and to understand how differences in flight paths impact perception across viewpoints. These findings will enable UAV gesture systems that, with a high degree of confidence, ensure gesture motions can be accurately perceived by human observers.",authors:"Fletcher, Paul; Luther, Angeline; Duncan, Brittany; Detweiler, Carrick",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/ICRA48506.2021.9561094",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:35,citations:[],selected:!0,slug:"paper_35",title:"Augmenting the audio-based expression modality of a non-affective robot",abstract:"This paper investigates the potential benefits of augmenting audio-based affective means of expression to strengthen the perceived intentions of a robot. Robots are often viewed as being simple machines with limied capabilities of communication. Changing how a robot is perceived, towards a more affective interpretation of its intentions, requires careful consideration of the means of expression available to the robot. It also requires alignment between these means to ensure they work in coordination with each other to make the robot easier to understand. As an effort to strengthen the affective interpretation of a soft robotic arm robot, we altered its overall expression by changing the available audio-based expression modalities. The system mitigatedthe naturally occurring noise from actuators and pneumatic systems and used a custom sound that supported the movement of the robot. The robot was tested by interacting with human observers (n=78) and was perceived as being significantly more curious, happy and less angry when augmented by audio that aligned with the naturally occurred robot sounds. The results show that the audio-based expression modality of robots is a valuable communication tool to consider augmenting when designing robots that convey affective information.",authors:"Frederiksen, Morten Roed; Stoey, Kasper",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/ACII.2019.8925510",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:36,citations:[],selected:!0,slug:"paper_36",title:"Touch-based information transfer from a robot modeled on the hearing dog",abstract:"Research on physical human-robot interaction has been attracting attention recently, focusing on robot embodiment. The work reported here proposes Active Touch Communication Robot (AcToR), a robot that is modeled on the hearing dog. A hearing dog is a type of dog assist people who are deaf or hard of hearing by alerting their handler to important sounds. AcToR uses the sense of touch to notify a human of the intention to transfer information. For example, when AcToR detects that a cell phone that is in another location has received a call, AcToR moves to the user's location and makes contact with the user's body to notify the user of the incoming call. The AcToR robot is based on the Roomba<sup>\xae</sup> and uses the Roomba's bumper and contact sensors to detect contact. This paper reports the results of psychological experiments using the AcToR robot that indicate the feasibility of using touch to transfer information from a robot to a person.",authors:"Furuhashi, Michihiko; Nakamura, Tsuyoshi; Kanoh, Masayoshi; Yamada, Koji",venue:"IEEE",year:2015,link:"https://doi.org/10.1109/FUZZ-IEEE.2015.7337981",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:37,citations:[],selected:!0,slug:"paper_37",title:"Generating anticipation in robot motion",abstract:"Robots that display anticipatory motion provide their human partners with greater time to respond in interactive tasks because human partners are aware of robot intent earlier. We create anticipatory motion autonomously from a single motion exemplar by extracting hand and body symbols that communicate motion intent and moving them earlier in the motion. We validate that our algorithm extracts the most salient frame (i.e. the correct symbol) which is the most informative about motion intent to human observers. Furthermore, we show that anticipatory variants allow humans to discern motion intent sooner than motions without anticipation, and that humans are able to reliably predict motion intent prior to the symbol frame when motion is anticipatory. Finally, we quantified the time range for robot motion when humans can perceive intent more accurately and the collaborative social benefits of anticipatory motion are greatest.",authors:"Gielniak, Michael J.; Thomaz, Andrea L.",venue:"IEEE",year:2011,link:"https://doi.org/10.1109/ROMAN.2011.6005255",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:38,citations:[],selected:!0,slug:"paper_38",title:"Robopal: Modeling Role Transitions in Human-Robot Interaction",abstract:"We have developed a new communication robot, Robopal, which is an indoor/outdoor robot for use in human-robot interaction research in the context of daily life. Robopal's intended applications involve leading and/or following a human to a destination. Preliminary experiments have been conducted to study nonverbal cues associated with leading and following behavior, and it has been observed that some behaviors, such as glancing towards the leader or follower, appear to be role-dependent. A system for representing these behaviors with a state transition model is described, based on four kinds of interaction roles: directive, responsive, collaborative, and independent. It is proposed that behavior modeling can be simplified by using this system to represent changes in the roles the robot and human play in an interaction, and by associating appropriate behaviors to each role",authors:"Glas, Dylan F.; Miyashita, Takahiro; Ishiguro, Hiroshi; Hagita, Norihiro",venue:"IEEE",year:2007,link:"https://doi.org/10.1109/ROBOT.2007.363636",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:39,citations:[],selected:!0,slug:"paper_39",title:"Mind the ARm: Realtime Visualization of Robot Motion Intent in Head-Mounted Augmented Reality",abstract:"Established safety sensor technology shuts down industrial robots when a collision is detected, causing preventable loss of productivity. To minimize downtime, we implemented three Augmented Reality (AR) visualizations (Path, Preview, and Volume) which allow users to understand robot motion intent and give way to the robot. We compare the different visualizations in a user study in which a small cognitive task is performed in a shared workspace. We found that Preview and Path required significantly longer head rotations to perceive robot motion intent. Volume, however, required the shortest head rotation and was perceived as most safe, enabling closer proximity of the robot arm before one left the shared workspace without causing shutdowns.",authors:"Gruenefeld, Uwe; Pr\xe4del, Lars; Illing, Jannike; Stratmann, Tim; Drolshagen, Sandra; Pfingsthorn, Max",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3404983.3405509",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Head-Attached",parent:"On-Human"}]},{UID:40,citations:[],selected:!0,slug:"paper_40",title:"Seeing Thru Walls: Visualizing Mobile Robots in Augmented Reality",abstract:"We present an approach for visualizing mobile robots through an Augmented Reality headset when there is no line-of-sight visibility between the robot and the human. Three elements are visualized in Augmented Reality: 1) Robot\u2019s 3D model to indicate its position, 2) An arrow emanating from the robot to indicate its planned movement direction, and 3) A 2D grid to represent the ground plane. We conduct a user study with 18 participants, in which each participant are asked to retrieve objects, one at a time, from stations at the two sides of a T-junction at the end of a hallway where a mobile robot is roaming. The results show that visualizations improved the perceived safety and efficiency of the task and led to participants being more comfortable with the robot within their personal spaces. Furthermore, visualizing the motion intent in addition to the robot model was found to be more effective than visualizing the robot model alone. The proposed system can improve the safety of automated warehouses by increasing the visibility and predictability of robots.",authors:"Gu, Morris; Cosgun, Akansel; Chan, Wesley P.; Drummond, Tom; Croft, Elizabeth",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/RO-MAN50785.2021.9515322",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Head-Attached",parent:"On-Human"}]},{UID:41,citations:[],selected:!0,slug:"paper_41",title:"Projection mapping implementation: Enabling direct externalization of perception results and action intent to improve robot explainability",abstract:"Existing research on non-verbal cues, e.g., eye gaze or arm movement, may not accurately present a robot's internal states such as perception results and action intent. Projecting the states directly onto a robot's operating environment has the advantages of being direct, accurate, and more salient, eliminating mental inference about the robot's intention. However, there is a lack of tools for projection mapping in robotics, compared to established motion planning libraries (e.g., MoveIt). In this paper, we detail the implementation of projection mapping to enable researchers and practitioners to push the boundaries for better interaction between robots and humans. We also provide practical documentation and code for a sample manipulation projection mapping on GitHub: github.com/uml-robotics/projection_mapping.",authors:"Han, Zhao; Wilkinson, Alexander; Parrillo, Jenna; Allspaw, Jordan; Yanco, Holly A.",venue:"arXiv",year:2020,link:"https://doi.org/10.48550/arXiv.2010.02263\n",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:42,citations:[],selected:!0,slug:"paper_42",title:"Investigating the Effectiveness of Different Interaction Modalities for Spatial Human-Robot Interaction",abstract:"With the increasing use of social robots in real environments, one of the areas of research requiring more attention is the study of human-robot interaction (HRI) when a person and robot are moving close to each other. Understanding effective ways to design how a robot should communicate its intention during dynamic movement is based on what people\u2019s expectations are and how they interpret different cues from the robot. Building on the existing literature, we tested a range of non-verbal cues such as eye contact, gaze and head nodding as part of the robot\u2019s behaviour during close proximate passing. The research aimed to investigate the effects of these cues, as well as their combination with body posture, on the efficiency of passing and the quality of HRI. Our results show that the combination of eye contact and the robot turning sideways is the most effective and appropriate compared to other modalities.",authors:"He, Jinying; van Maris, Anouk; Caleb-Solly, Praminda",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3371382.3378273",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:43,citations:[],selected:!0,slug:"paper_43",title:"Hey Robot, Which Way Are You Going? Nonverbal Motion Legibility Cues for Human-Robot Spatial Interaction",abstract:"Mobile robots have recently been deployed in public spaces such as shopping malls, airports, and urban sidewalks. Most of these robots are designed with human-aware motion planning capabilities but are not designed to communicate with pedestrians. Pedestrians that encounter these robots without prior understanding of the robots' behaviour can experience discomfort, confusion, and delayed social acceptance. In this work we designed and evaluated nonverbal robot motion legibility cues, which communicate a mobile robot's motion intention to pedestrians. We compared a motion legibility cue using Projected Arrows to one using Flashing Lights. We designed the cues to communicate path information, goal information, or both, and explored different Robot Movement Scenarios. We conducted an online user study with 229 participants using videos of the motion legibility cues. Our results show that the absence of cues was not socially acceptable, and that Projected Arrows were the more socially acceptable cue in most experimental conditions. We conclude that the presence and choice of motion legibility cues can positively influence robots' acceptance and successful deployment in public spaces.",authors:"Hetherington, Nicholas J.; Croft, Elizabeth A.; van der Loos, H. MachielF.",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/LRA.2021.3068708",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:44,citations:[],selected:!0,slug:"paper_44",title:"Legible robot pointing",abstract:"Good communication is critical to seamless human-robot interaction. Among numerous communication channels, here we focus on gestures, and in particular on spacial deixis: pointing at objects in the environment in order to reference them. We propose a mathematical model that enables robots to generate pointing configurations that make the goal object as clear as possible - pointing configurations that are legible. We study the implications of legibility on pointing, e.g. that the robot will sometimes need to trade off efficiency for the sake of clarity. Finally, we test how well our model works in practice in a series of user studies, showing that the resulting pointing configurations make the goal object easier to infer for novice users.",authors:"Holladay, Rachel M.; Dragan, Anca D.; Srinivasa, Siddhartha S.",venue:"IEEE",year:2014,link:"https://doi.org/10.1109/ROMAN.2014.6926256",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:45,citations:[],selected:!0,slug:"paper_45",title:"Auditory display of directions and states for mobile systems",abstract:"Auditory displays for mobile systems, such as service robots, have been developed. The design of directional sounds and of additional sounds for robot states (e.g., Heavy Load), as well as the design of more complicated robot sound tracks are explained. Basic musical elements and robot movement sounds have been combined. Two experimental studies, on the understandability of the directional sounds and the robot state sounds as well as on the auditory perception of intended robot trajectories in a simulated supermarket scenario, are described. Subjective evaluations of sound characteristics such as urgency, expressiveness, and annoyance have been performed by non-musicians and musicians. These experimental results are compared with the diagrams which have been computed with two wavelet techniques for time-frequency analyses.",authors:"Johannsen, Gunnar",venue:"Georgia Institute of Technology",year:2002,link:"https://smartech.gatech.edu/handle/1853/51337",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:46,citations:[],selected:!0,slug:"paper_46",title:"Communicative Cues for Reach-to-Grasp Motions: From Humans to Robots",abstract:"Intent communication is an important challenge in the context of human-robot interaction. The aim of this work is to identify subtle non-verbal cues that make communication among humans fluent and using them to generate intent expressive robot motion. A human-human reach-to-grasp experiment ( n =14) identified two temporal and two spatial cues: (1) relative time to reach maximum hand aperture ( MA ), (2) overall motion duration ( OT ), (3) exaggeration in motion ( Exg ), and (4) change in grasp modality ( GM ). Results showed there was statistically significant difference in the temporal cues between no-intention and intention conditions. A follow-up experiment ( n =30) was conducted based on these results. Reach-to-grasp motions of a simulated robot containing different cue combinations were shown to the participants. They were asked to guess the target object during robot\u2019s motion, based on the assumption that intent expressive motion would result in earlier and more accurate guesses. Results showed that, OT, GM and several cue combinations led to faster and more accurate guesses which imply they can be used to generate communicative motion. However, MA had no effect, and surprisingly Exg had a negative effect on expressiveness.",authors:"Keb\xfcde, Dogancan; Eteke, Cem; Sezgin, Tevfik Metin; Akg\xfcn, Bars",venue:"ACM",year:2018,link:"https://dl.acm.org/doi/10.5555/3237383.3237830",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:47,citations:[],selected:!0,slug:"paper_47",title:"Nonverbal Robot-Group Interaction Using an Imitated Gaze Cue",abstract:"Ensuring that a particular and unsuspecting member of a group is the recipient of a salient-item hand-over is a complicated interaction. The robot must effectively, expediently and reliably communicate its intentions to advert any tendency within the group towards antinormative behaviour. In this paper, we study how a robot can establish the participant roles of such an interaction using imitated social and contextual cues. We designed two gaze cues, the first was designed to discourage antinormative behaviour through individualising a particular member of the group and the other to the contrary. We designed and conducted a field experiment (456 participants in 64 trials) in which small groups of people (between 3 and 20 people) assembled in front of the robot, which then attempted to pass a salient object to a particular group member by presenting a physical cue, followed by one of two variations of a gaze cue. Our results showed that presenting the individualising cue had a significant (z=3.733, p=0.0002) effect on the robot\u2019s ability to ensure that an arbitrary group member did not take the salient object and that the selected participant did.",authors:"Kirchner, Nathan; Alempijevic, Alen; Dissanayake, Gamini",venue:"ACM",year:2011,link:"https://doi.org/10.1145/1957656.1957824",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:48,citations:[],selected:!0,slug:"paper_48",title:"Hey! There is someone at your door. A hearing robot using visual communication signals of hearing dogs to communicate intent",abstract:"This paper presents a study of the readability of dog-inspired visual communication signals in a human-robot interaction scenario. This study was motivated by specially trained hearing dogs which provide assistance to their deaf owners by using visual communication signals to lead them to the sound source. For our human-robot interaction scenario, a robot was used in place of a hearing dog to lead participants to two different sound sources. The robot was preprogrammed with dog-inspired behaviors, controlled by a wizard who directly implemented the dog behavioral strategy on the robot during the trial. By using dog-inspired visual communication signals as a means of communication, the robot was able to lead participants to the sound sources (the microwave door, the front door). Findings indicate that untrained participants could correctly interpret the robot's intentions. Head movements and gaze directions were important for communicating the robot's intention using visual communication signals.",authors:"Koay, K. L.; Lakatos, G.; Syrdal, D. S.; Gacsi, M.; Bereczky, B.; Dautenhahn, K.; Miklosi, A.; Walters, M. L.",venue:"IEEE",year:2013,link:"https://doi.org/10.1109/ALIFE.2013.6602436",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:49,citations:[],selected:!0,slug:"paper_49",title:"Effects of Integrated Intent Recognition and Communication on Human-Robot Collaboration",abstract:"Human-robot interaction research to date has investigated intent recognition and communication separately. In this paper, we explore the effects of integrating both the robot's ability to generate intentional motion and predict the human's motion in a collaborative physical task. We implemented an intent recognition system to recognize the human partner's hand motion intent and a motion planner system to enable the robot to communicate its intent by using legible and predictable motion. We tested this bi-directional intent system in a 2-way within-subjects user study. Results suggest that an integrated intent recognition and communication system may facilitate more collaborative behavior among team members.",authors:"Lee Chang, Mai; Gutierrez, Reymundo A.; Khante, Priyanka; Schaertl Short, Elaine; Lockerd Thomaz, Andrea",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/IROS.2018.8593359",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:50,citations:[],selected:!0,slug:"paper_50",title:"Methods for Expressing Robot Intent for Human\u2013Robot Collaboration in Shared Workspaces",abstract:"Human\u2013robot collaboration is becoming increasingly common in factories around the world; accordingly, we need to improve the interaction experiences between humans and robots working in these spaces. In this article, we report on a user study that investigated methods for providing information to a person about a robot\u2019s intent to move when working together in a shared workspace through signals provided by the robot. In this case, the workspace was the surface of a tabletop. Our study tested the effectiveness of three motion-based and three light-based intent signals as well as the overall level of comfort participants felt while working with the robot to sort colored blocks on the tabletop. Although not significant, our findings suggest that the light signal located closest to the workspace\u2014an LED bracelet located closest to the robot\u2019s end effector\u2014was the most noticeable and least confusing to participants. These findings can be leveraged to support human\u2013robot collaborations in shared spaces.",authors:"LeMasurier, Gregory; Bejerano, Gal; Albanese, Victoria; Parrillo, Jenna; Yanco, Holly A.; Amerson, Nicholas; Hetrick, Rebecca; Phillips, Elizabeth",venue:"ACM",year:2021,link:"https://doi.org/10.1145/3472223",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:51,citations:[],selected:!0,slug:"paper_51",title:"Towards situational awareness from robotic group motion",abstract:"The control of multiple robots in the context of tele-exploration tasks is often attentionally taxing, resulting in a loss of situational awareness for operators. Unmanned aerial vehicle swarms require significantly more multitasking than controlling a plane, thus making it necessary to devise intuitive feedback sources and control methods for these robots. The purpose of this article is to examine a swarm's nonverbal behaviour as a possible way to increase situational awareness and reduce the operators cognitive load by soliciting intuitions about the swarm's behaviour. To progress on the definition of a database of nonverbal expressions for robot swarms, we first define categories of communicative intents based on spontaneous descriptions of common swarm behaviours. The obtained typology confirms that the first two levels (as defined by Endsley: elements of environment and comprehension of the situation) can be shared through swarms motion-based communication. We then investigate group motion parameters potentially connected to these communicative intents. Results are that synchronized movement and tendency to form figures help convey meaningful information to the operator. We then discuss how this can be applied to realistic scenarios for the intuitive command of remote robotic teams.",authors:"Levillain, Florent; St-Onge, David; Beltrame, Giovanni; Zibetti, Elisabetta",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/RO-MAN46459.2019.8956381",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:52,citations:[],selected:!0,slug:"paper_52",title:"Mobile robot with eyeball expression as the preliminary-announcement and display of the robots following motion",abstract:"This paper explains the PMR-2R (prototype mobile robot \u20132 revised), the mobile robot with the eyeball expression as the preliminary-announcement and display of the robot\u2019s following motion. Firstly, we indicate the importance of the preliminary-announcement and display function of the mobile robot\u2019s following motion for the informational affinity between human being and a robot, with explaining the conventional methods and the related works. We show the proposed four methods which are categorized into two types: one type which indicates a state just after the moment and the other type which displays from the present to some future time continuously. Then we introduce the PMR-2R, which has the omni-directional display, the magicball, on which the eyeball expresses the robot\u2019s following direction of motion and the speed of motion at the same time. From the evaluation experiment, we confirmed the efficiency of the eyeball expression to transfer the information. We also obtained the announcement at around one or two second before the actual motion may be appropriate. And finally we compare the four types of eyeball expression: the one-eyeball type, the two-eyeball type, the will-o\u2019-the-wisp type, and the armor-helmet type. From the evaluation experiment, we have declared the importance to make the robot\u2019s front more intelligible especially to announce the robot\u2019s direction of motion.",authors:"Matsumaru, Takafumi; Iwase, Kazuya; Akiyama, Kyouhei; Kusada, Takashi; Ito, Tomotaka",venue:"Springer",year:2005,link:"https://doi.org/10.1007/s10514-005-0728-8",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:53,citations:[],selected:!0,slug:"paper_53",title:"Mobile robot with preliminary-announcement function of forthcoming motion using light-ray",abstract:"This paper discusses the design and the basic characteristic of the mobile robot PMR-1 with the preliminary-announcement and display function of the forthcoming operation (the direction of motion and the speed of motion) to the people around the robot by drawing a scheduled course on a running surface using light-ray. The laser pointer is used as a light source and the light from the laser pointer is reflected in a mirror. The light-ray is projected on a running surface and a scheduled course is drawn by rotating the reflector around the pan and the tilt axes. The preliminary-announcement and display unit of the developed mobile robot can indicate the operation until 3-second-later preliminarily, so the robot moves drawing the scheduled course from the present to 3-second-later. The experiment on coordination between the preliminary-announcement and the movement has been carried out, and we confirmed the correspondence of the announced course with the robot trajectory both in the case that the movement path is given beforehand and in the case that the robot is operated with manual input from a joystick in real-time. So we have validated the coordination algorithm between the preliminary-announcement and the real movement",authors:"Matsumaru, Takafumi",venue:"IEEE",year:2006,link:"https://doi.org/10.1109/IROS.2006.281981",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:54,citations:[],selected:!0,slug:"paper_54",title:"Mobile robot with preliminary-announcement and display function of forthcoming motion using projection equipment",abstract:"This paper discusses the mobile robot PMR-5 with the preliminary-announcement and display function which indicates the forthcoming operations to the people near the robot by using a projector. The projector is set on a mobile robot and a 2D frame is projected on a running surface. In the frame, not only the scheduled course but also the states of operation can be clearly announced as the information about movement. We examine the presentation of the states of operation such as stop or going back including the time information of the scheduled course on the developed robot. Scheduled course is expressed as the arrows considering the intelligibility at sight. Arrow expresses the direction of motion directly and the length of arrow can announce the speed of motion. Operation until 3-second-later is indicated and three arrows classified by color for each second are connected and displayed so these might show the changing of speed during 3-second period. The sign for spot revolution and the characters for stop and going back are also displayed. We exhibited the robot and about 200 visitors did the questionnaire evaluation. The average of 5-stage evaluation is 3.9 points and 4.5 points for the direction of motion and the speed of motion respectively. So we obtained the evaluation that it is intelligible in general",authors:"Matsumaru, Takafumi; Kusada, Takashi; Iwase, Kazuya",venue:"IEEE",year:2006,link:"https://doi.org/10.1109/ROMAN.2006.314368",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:55,citations:[],selected:!0,slug:"paper_55",title:"Mobile robot with preliminary-announcement and indication function of forthcoming operation using flat-panel display",abstract:"This research aims to propose the method and equipment to preliminary-announce and indicate the surrounding people both the speed of motion and the direction of motion of the mobile robot that moves on a two-dimensional plane. This paper discusses the mobile robot PMR-6, in which the liquid crystal display (LCD) is set up on the mobile unit, and the state of operation at 1.5 s before the actual motion is indicated. The basis of the content to display is 'arrow' considering the intelligibility for people even at first sight. The speed of motion is expressed as the size (length and width) of the arrow and its color based on traffic signal. The direction of motion is described with the curved condition of the arrow. The characters of STOP are displayed in red in case of stop. The robot was exhibited to the 2005 International Robot Exhibition held in Tokyo. About 200 visitors answered to the questionnaires. The average of five-stage evaluation is 3.56 and 3.97 points on the speed and on the direction respectively, so the method and expression were evaluated comparatively intelligible. As for the gender, the females appreciated about the speed of motion than the males on the whole. Concerning the age, some of the younger age and the upper age admired highly about the direction of motion than the middle age.",authors:"Matsumaru, Takafumi",venue:"IEEE",year:2007,link:"https://doi.org/10.1109/ROBOT.2007.363579",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:56,citations:[],selected:!0,slug:"paper_56",title:"Expression of intention by rotational head movements for teleoperated mobile robot",abstract:"We are studying a teleoperated mobile robot that provides useful information to a pedestrian. However, it is difficult for people to understand meanings of actions, motions or movements of many conventional robots. The purpose of this study is to improve pedestrian's impressions of a robot. Especially this paper describes people's understandability of robot behaviors when a robot turns around a corner or when a person and a robot pass each other in a corridor. Our robot shows its intention to make turn by rotating its head, as though a pedestrian shows a traveling direction by his/her gaze or face direction. The robot is teleoperated by an operator for safety in public spaces, and the direction of the robot head and the moving direction of the robot body are determined by an artificial potential field (APF) generated by a target position given by the operator, positions of obstacles and pedestrians. The APF for a pedestrian is generated based on her/his personal space of a person. Thus, the robot can express the intention of its action by rotating the head to look where it is going, when the robot changes its direction around pedestrians. The intention expression can be natural and understandable for them by the rotational movement of the head before the robot turns its body actually. Impression evaluation experiments with questionnaires were conducted under the two kinds of situations to reveal the validity and effectiveness of the intention expression by the robot's head rotation. Significant differences related to understandability and some impression words were observed between with and without rotating the head.",authors:"Mikawa, Masahiko; Yoshikawa, Yuriko; Fujisawa, Makoto",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/AMC.2019.8371097",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:57,citations:[],selected:!0,slug:"paper_57",title:"Meet Me Where i\u2019m Gazing: How Shared Attention Gaze Affects Human-Robot Handover Timing",abstract:'In this paper we provide empirical evidence that using humanlike gaze cues during human-robot handovers can improve the timing and perceived quality of the handover event. Handovers serve as the foundation of many human-robot tasks. Fluent, legible handover interactions require appropriate nonverbal cues to signal handover intent, location and timing. Inspired by observations of human-human handovers, we implemented gaze behaviors on a PR2 humanoid robot. The robot handed over water bottles to a total of 102 na\\"ive subjects while varying its gaze behaviour: no gaze, gaze designed to elicit shared attention at the handover location, and the shared attention gaze complemented with a turn-taking cue. We compared subject perception of and reaction time to the robot-initiated handovers across the three gaze conditions. Results indicate that subjects reach for the offered object significantly earlier when a robot provides a shared attention gaze cue during a handover. We also observed a statistical trend of subjects preferring handovers with turn-taking gaze cues over the other conditions. Our work demonstrates that gaze can play a key role in improving user experience of human-robot handovers, and help make handovers fast and fluent.',authors:"Moon, AJung; Troniak, Daniel M.; Gleeson, Brian; Pan, Matthew K.X.J.; Zheng, Minhua; Blumer, Benjamin A.; MacLean, Karon; Croft, Elizabeth A.",venue:"ACM",year:2014,link:"https://doi.org/10.1145/2559636.2559656",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:58,citations:[],selected:!0,slug:"paper_58",title:"Communicating Inferred Goals With Passive Augmented Reality and Active Haptic Feedback",abstract:"Robots learn as they interact with humans. Consider a human teleoperating an assistive robot arm: as the human guides and corrects the arm's motion, the robot gathers information about the human's desired task. But how does the human know what their robot has inferred? Today's approaches often focus on conveying intent: for instance, using legible motions or gestures to indicate what the robot is planning. However, closing the loop on robot inference requires more than just revealing the robot's current policy: the robot should also display the alternatives it thinks are likely, and prompt the human teacher when additional guidance is necessary. In this letter we propose a multimodal approach for communicating robot inference that combines both passive and active feedback. Specifically, we leverage information-rich augmented reality to passively visualize what the robot has inferred, and attention-grabbing haptic wristbands to actively prompt and direct the human's teaching. We apply our system to shared autonomy tasks where the robot must infer the human's goal in real-time. Within this context, we integrate passive and active modalities into a single algorithmic framework that determines when and which type of feedback to provide. Combining both passive and active feedback experimentally outperforms single modality baselines; during an in-person user study, we demonstrate that our integrated approach increases how efficiently humans teach the robot while simultaneously decreasing the amount of time humans spend interacting with the robot. Videos here: https://youtu.be/swq_u4iIP-g",authors:"Mullen, James; Mosier, Josh; Chakrabarti, Sounak; Chen, Anqi; White, Tyler; Losey, Dylan",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/LRA.2021.3111055",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Head-Attached",parent:"On-Human"},{name:"Hand-Held",parent:"On-Human"}]},{UID:59,citations:[],selected:!0,slug:"paper_59",title:"Nonverbal Leakage in Robots: Communication of Intentions through Seemingly Unintentional Behavior",abstract:'Human communication involves a number of nonverbal cues that are seemingly unintentional, unconscious, and automatic-both in their production and perception-and convey rich information on the emotional state and intentions of an individual. One family of such cues is called "nonverbal leakage." In this paper, we explore whether people can read nonverbal leakage cues-particularly gaze cues-in humanlike robots and make inferences on robots\u2019 intentions, and whether the physical design of the robot affects these inferences. We designed a gaze cue for Geminoid-a highly humanlike android-and Robovie-a robot with stylized, abstract humanlike features-that allowed the robots to "leak" information on what they might have in mind. In a controlled laboratory experiment, we asked participants to play a game of guessing with either of the robots and evaluated how the gaze cue affected participants\u2019 task performance. We found that the gaze cue did, in fact, lead to better performance, from which we infer that the cue led to attributions of mental states and intentionality. Our results have implications for robot design, particularly for designing expression of intentionality, and for our understanding of how people respond to human social cues when they are enacted by robots.',authors:"Mutlu, Bilge; Yamaoka, Fumitaka; Kanda, Takayuki; Ishiguro, Hiroshi; Hagita, Norihiro",venue:"ACM",year:2009,link:"https://doi.org/10.1145/1514095.1514110",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:60,citations:[],selected:!0,slug:"paper_60",title:"Visualizing Robot Intent for Object Handovers with Augmented Reality",abstract:"Humans are highly skilled in communicating their intent for when and where a handover would occur. However, even the state-of-the-art robotic implementations for handovers display a general lack of communication skills. This study aims to visualize the internal state and intent of robots for Human-to-Robot Handovers using Augmented Reality. Specifically, we aim to visualize 3D models of the object and the robotic gripper to communicate the robot's estimation of where the object is and the pose in which the robot intends to grasp the object. We tested this design via a user study with 16 participants, in which each participant handed over a cube-shaped object to the robot 12 times. Results show that visualizing robot intent using augmented reality substantially improves the subjective experience of the users for handovers. Results also indicate that the effectiveness of augmented reality is even more pronounced for the perceived safety and fluency of the interaction when the robot makes errors in localizing the object.",authors:"Newbury, Rhys; Cosgun, Akansel; Crowley-Davis, Tysha; Chan, Wesley P.; Drummond, Tom; Croft, Elizabeth",venue:"arXiv",year:2021,link:"https://doi.org/10.48550/arXiv.2103.04055",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Head-Attached",parent:"On-Human"}]},{UID:61,citations:[],selected:!0,slug:"paper_61",title:"Bio-inspired multi-robot communication through behavior recognition",abstract:"This paper focuses on enabling multi-robot teams to cooperatively perform tasks without the use of radio or acoustic communication. One key to more effective cooperative interaction in a multi-robot team is the ability to understand the behavior and intent of other robots. This is similar to the honey bee \u201cwaggle dance\u201d in which a bee can communicate the orientation and distance of a food source. In this similar manner, our heterogenous multi-robot team uses a specific behavior to indicate the location of mine-like objects (MLOs). Observed teammate action sequences can be learned to perform behavior recognition and task-assignment in the absence of communication. We apply Conditional Random Fields (CRFs) to perform behavior recognition as an approach to task monitoring in the absence of communication in a challenging underwater environment. In order to demonstrate the use of behavior recognition of an Autonomous Underwater Vehicle (AUV) in a cooperative task, we use trajectory based techniques for model generation and behavior discrimination in experiments using simulated scenario data. Results are presented demonstrating heterogenous teammate cooperation between an AUV and an Autonomous Surface Vehicle (ASV) using behavior recognition rather than radio or acoustic communication in a mine clearing task.",authors:"Novitzky, Michael; Pippin, Charles; Collins, Thomas R.; Balch, Tucker R.; West, Michael E.",venue:"IEEE",year:2012,link:"https://doi.org/10.1109/ROBIO.2012.6491061",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:62,citations:[],selected:!0,slug:"paper_62",title:"A Drink-Serving Mobile Social Robot Selects Who to Interact with Using Gaze",abstract:'Robots will soon deliver food and beverages in various environments. These robots will need to communicate their intention efficiently; for example, they should indicate who they are addressing. We conducted a real-world study of a water serving robot at a university cafeteria. The robot was operated in a Wizard-of-Oz manner. It approached and offered water to students having their lunch. Our analyses of the relationship between robot gaze direction and the likelihood that someone takes a drink show that if people do not already have a drink and the interaction is not dominated by an overly enthusiastic user, the robot\u2019s gaze behavior is effective in selecting an interaction partner even "in the wild".',authors:"Palinko, Oskar; Fischer, Kerstin; Ruiz Ramirez, Eduardo; Damsgaard Nissen, Lotte; Langedijk, Rosalyn M.",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3371382.3378339",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:63,citations:[],selected:!0,slug:"paper_63",title:"Fostering short-term human anticipatory behavior in human-robot collaboration",abstract:"The present study reports on a human-robot collaboration experiment involving an industrial task with the specific aim of exploring the effects of (i) fostering human anticipatory behavior towards the robot, through visual cues of the robot\u2019s next move and (ii) robot adaptiveness to the human actions through reducing its motion speed with respect to human movement\u2019s proximity. For investigating these effects a generic collaborative picking and sorting task was designed, implemented and tested by volunteer participants, in a Virtual Reality simulation environment. Results demonstrated that, showing robot\u2019s intent through anticipatory cues significantly increased team efficiency, human safety and collaborative fluency in conjunction with a positive subjective inclination towards the robot. Robot adaptiveness significantly increased human safety without decreasing task efficiency and fluency, compared to a control condition.",authors:"Psarakis, Loizos; Nathanael, Dimitris; Marmaras, Nicolas",venue:"ScienceDirect",year:2022,link:"https://doi.org/10.1016/j.ergon.2021.103241",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Head-Attached",parent:"On-Human"}]},{UID:64,citations:[],selected:!0,slug:"paper_64",title:"Communicating and controlling robot arm motion intent through mixed-reality head-mounted displays",abstract:"Efficient motion intent communication is necessary for safe and collaborative work environments with co-located humans and robots. Humans efficiently communicate their motion intent to other humans through gestures, gaze, and other non-verbal cues, and can replan their motions in response. However, robots often have difficulty using these methods. Many existing methods for robot motion intent communication rely on 2D displays, which require the human to continually pause their work to check a visualization. We propose a mixed-reality head-mounted display (HMD) visualization of the intended robot motion over the wearer\u2019s real-world view of the robot and its environment. In addition, our interface allows users to adjust the intended goal pose of the end effector using hand gestures. We describe its implementation, which connects a ROS-enabled robot to the HoloLens using ROS Reality, using MoveIt for motion planning, and using Unity to render the visualization. To evaluate the effectiveness of this system against a 2D display visualization and against no visualization, we asked 32 participants to label various arm trajectories as either colliding or non-colliding with blocks arranged on a table. We found a 15% increase in accuracy with a 38% decrease in the time it took to complete the task compared with the next best system. These results demonstrate that a mixed-reality HMD allows a human to determine where the robot is going to move more quickly and accurately than existing baselines.",authors:"Rosen, Eric; Whitney, David; Phillips, Elizabeth; Chien, Gary; Tompkin, James; Konidaris, George; Tellex, Stefanie",venue:"SAGE Publications",year:2019,link:"https://doi.org/10.1177/0278364919842925",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Head-Attached",parent:"On-Human"}]},{UID:65,citations:[],selected:!0,slug:"paper_65",title:"Third point of view augmented reality for robot intentions visualization",abstract:"Lightweight, head-up displays integrated in industrial helmets allow to provide contextual information for industrial scenarios such as in maintenance. Moving from single display and single camera solutions to stereo perception and display opens new interaction possibilities. In particular this paper addresses the case of information sharing by a Baxter robot displayed to the user overlooking at the real scene. System design and interaction ideas are being presented.",authors:"Ruffaldi, Emanuele; Brizzi, Filippo; Tecchia, Franco; Bacinelli, Sandro",venue:"Springer",year:2016,link:"https://doi.org/10.1007/978-3-319-40621-3_35",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Head-Attached",parent:"On-Human"}]},{UID:66,citations:[],selected:!0,slug:"paper_66",title:"Communicating affect via flight path: exploring use of the laban effort system for designing affective locomotion paths",abstract:"People and animals use various kinds of motion in a multitude of ways to communicate their ideas and affective state, such as their moods or emotions. Further, people attribute affect and personalities to movements of even non-life like entities based solely on the style of their motions, e.g., the locomotion style of a geometric shape (how it moves about) can be interpreted as being shy, aggressive, etc. We investigate how robots can leverage this locomotion-style communication channel for communication with people. Specifically, our work deals with designing stylistic flying-robot locomotion paths for communicating affective state. To author and unpack the parameters of affect-oriented flying-robot locomotion styles we employ the Laban Effort System, a standard method for interpreting human motion commonly used in the performing arts. This paper describes our adaption of the Laban Effort System to author motions for flying robots, and the results of a formal experiment that investigated how various Laban Effort System parameters influence people's perception of the resulting robotic motions. We summarize with a set of guidelines for aiding designers in using the Laban Effort System to author flying robot motions to elicit desired affective responses.",authors:"Sharma, Megha; Hildebrandt, Dale; Newman, Gem; Young, James E.; Eskicioglu, Rasit",venue:"IEEE",year:2013,link:"https://doi.org/10.1109/HRI.2013.6483602",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:67,citations:[],selected:!0,slug:"paper_67",title:"Effect of Expressive Lights on Human Perception and Interpretation of Functional Robot",abstract:"Because appearance-constrained robots lack expressiveness, human users often find it hard to understand their behavior and intentions. To address this, expressive lights are considered to be an effective means for such robots to communicate with people. However, existing studies mainly focus on specific tasks or goals, leaving the knowledge of how expressive lights affect people\u2019s perception still unknown. In this pilot study, we investigate such a question by using a Roomba robot. We designed two light expressions, namely, green and low-intensity (GL) and red and high-intensity (RH). We used open-ended questions to evaluate people\u2019s perception and interpretation of the robot, which showed different light expressions as a way to communicate. Our findings reveal that simple light expressions can allow people to construct rich and complex interpretations of a robot\u2019s behavior, and such interpretations are heavily biased by the design of expressive lights.",authors:"Song, Sichao; Yamada, Seiji",venue:"ACM",year:2018,link:"https://doi.org/10.1145/3170427.3188547",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:68,citations:[],selected:!0,slug:"paper_68",title:"Designing LED Lights for Communicating Gaze with Appearance-Constrained Robots",abstract:"Functional robots are generally restricted in appearance, thus lacking ways to express their intent. In human-human interaction, gaze is an important cue for providing information and regulating interaction. In this pilot study, we investigate how we can implement gaze behavior in functional robots since gaze communication can allow humans to read a robot's intent and adjust their behavior accordingly. We explore design principles based on LED lights as we consider LEDs to be easily installed in most robots while not introducing features that are too human-like (to prevent users from having high expectations). In the paper, we present a design interface that allows designers to explore the parameter space of an LED strip attached to a Roomba robot. We then summarize a set of design principles for optimally simulating light-based gazes. Finally, our suggested design is evaluated by a large group of participants, and their comments are discussed.",authors:"Song, Sichao; Yamada, Seiji",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ROMAN.2018.8525661",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:69,citations:[],selected:!0,slug:"paper_69",title:"Bioluminescence-Inspired Human-Robot Interaction: Designing Expressive Lights That Affect Human\u2019s Willingness to Interact with a Robot",abstract:"Bioluminescence is the production and emission of light by a living organism. It, as a means of communication, is of importance for the survival of various creatures. Inspired by bioluminescent light behaviors, we explore the design of expressive lights and evaluate the effect of such expressions on a human\xbbs perception of and attitude toward an appearance-constrained robot. Such robots are in urgent need of finding effective ways to present themselves and communicate their intentions due to a lack of social expressivity. We particularly focus on the expression of attractiveness and hostility because a robot would need to be able to attract or keep away human users in practical human-robot interaction (HRI) scenarios. In this work, we installed an LED lighting system on a Roomba robot and conducted a series of two experiments. We first worked through a structured approach to determine the best light expression designs for the robot to show attractiveness and hostility. This resulted in four recommended light expressions. Further, we performed a verification study to examine the effectiveness of such light expressions in a typical HRI context. On the basis of the findings, we offer design guidelines for expressive lights that HRI researchers and practitioners could readily employ.",authors:"Song, Sichao; Yamada, Seiji",venue:"ACM",year:2018,link:"https://doi.org/10.1145/3171221.3171249",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:70,citations:[],selected:!0,slug:"paper_70",title:"Visual Attention in Spoken Human-Robot Interaction",abstract:'Psycholinguistic studies of situated language processing have revealed that gaze in the visual environment is tightly coupled with both spoken language comprehension and production. It has also been established that interlocutors monitor the gaze of their partners, a phenomenon called "joint attention", as a further means for facilitating mutual understanding. We hypothesise that human-robot interaction will benefit when the robot\u2019s language-related gaze behaviour is similar to that of people, potentially providing the user with valuable non-verbal information concerning the robot\u2019s intended message or the robot\u2019s successful understanding. We report findings from two eye-tracking experiments demonstrating (1) that human gaze is modulated by both the robot speech and gaze, and (2) that human comprehension of robot speech is improved when the robot\u2019s real-time gaze behaviour is similar to that of humans.',authors:"Staudte, Maria; Crocker, Matthew W.",venue:"ACM",year:2009,link:"https://doi.org/10.1145/1514095.1514111",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:71,citations:[],selected:!0,slug:"paper_71",title:"Communication of Intent in Assistive Free Flyers",abstract:"Assistive free-flyers (AFFs) are an emerging robotic platform with unparalleled flight capabilities that appear uniquely suited to exploration, surveillance, inspection, and telepresence tasks. However, unconstrained aerial movements may make it difficult for colocated operators, collaborators, and observers to understand AFF intentions, potentially leading to difficulties understanding whether operator instructions are being executed properly or to safety concerns if future AFF motions are unknown or difficult to predict. To increase AFF usability when working in close proximity to users, we explore the design of natural and intuitive flight motions that may improve AFF abilities to communicate intent while simultaneously accomplishing task goals. We propose a formalism for representing AFF flight paths as a series of motion primitives and present two studies examining the effects of modifying the trajectories and velocities of these flight primitives based on natural motion principles. Our first study found that modified flight motions might allow AFFs to more effectively communicate intent and, in our second study, participants preferred interacting with an AFF that used a manipulated flight path, rated modified flight motions as more natural, and felt safer around an AFF with modified motion. Our proposed formalism and findings highlight the importance of robot motion in achieving effective human-robot interactions.",authors:"Szafir, Daniel; Mutlu, Bilge; Fong, Terrence",venue:"ACM",year:2014,link:"https://doi.org/10.1145/2559636.2559672",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:72,citations:[],selected:!0,slug:"paper_72",title:"Communicating Directionality in Flying Robots",abstract:"Small flying robots represent a rapidly emerging family of robotic technologies with aerial capabilities that enable unique forms of assistance in a variety of collaborative tasks. Such tasks will necessitate interaction with humans in close proximity, requiring that designers consider human perceptions regarding robots flying and acting within human environments. We explore the design space regarding explicit robot communication of flight intentions to nearby viewers. We apply design constraints to robot flight behaviors, using biological and airplane flight as inspiration, and develop a set of signaling mechanisms for visually communicating directionality while operating under such constraints. We implement our designs on two commercial flyers, requiring little modification to the base platforms, and evaluate each signaling mechanism, as well as a no-signaling baseline, in a user study in which participants were asked to predict robot intent. We found that three of our designs significantly improved viewer response time and accuracy over the baseline and that the form of the signal offered tradeoffs in precision, generalizability, and perceived robot usability.",authors:"Szafir, Daniel; Mutlu, Bilge; Fong, Terry",venue:"ACM",year:2015,link:"https://doi.org/10.1145/2696454.2696475",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:73,citations:[],selected:!0,slug:"paper_73",title:"Expressing thought: Improving robot readability with animation principles",abstract:"The animation techniques of anticipation and reaction can help create robot behaviors that are human readable such that people can figure out what the robot is doing, reasonably predict what the robot will do next, and ultimately interact with the robot in an effective way. By showing forethought before action and expressing a reaction to the task outcome (success or failure), we prototyped a set of human-robot interaction behaviors. In a 2 (forethought vs. none: between) \xd7 2 (reaction to outcome vs. none: between) \xd7 2 (success vs. failure task outcome: within) experiment, we tested the influences of forethought and reaction upon people's perceptions of the robot and the robot's readability. In this online video prototype experiment (N=273), we have found support for the hypothesis that perceptions of robots are influenced by robots showing forethought, the task outcome (success or failure), and showing goal-oriented reactions to those task outcomes. Implications for theory and design are discussed.",authors:"Takayama, Leila; Dooley, Doug; Ju, Wendy",venue:"IEEE",year:2011,link:"https://doi.org/10.1145/1957656.1957674",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:74,citations:[],selected:!0,slug:"paper_74",title:"The development and evaluation of Robot Light Skin: a novel robot signalling system to improve communication in industrial human\u2013robot collaboration",abstract:"In a human\u2013robot collaborative production system, the robot could make request for interaction or notify the human operator if an uncertainty arises. Conventional industrial tower lights were designed for generic machine signalling purposes which may not be the ultimate solution for robot signalling in a collaborative setting. In this type of system, human operators could be monitoring multiple robots while carrying out a manual task so it is important to minimise the diversion of their attention. This paper presents a novel robot signalling solution, the Robot Light Skin (RLS),which is an integrated signalling system that could be used on most articulated robots. Our experiment was conducted to validate this concept in terms of its effect on improving operator's reaction time, hit-rate, awareness and task performance. The results showed that participants reacted faster to the RLS as well as achieved higher hit-rate. An eye tracker was used in the experiment which shows a reduction in diversion away from the manual task when using the RLS. Future study should explore the effect of the RLS concept on large-scale systems and multi-robot systems.",authors:"Tang, Gilbert; Webb, Phil; Thrower, John",venue:"ScienceDirect",year:2019,link:"https://doi.org/10.1016/j.rcim.2018.08.005",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:75,citations:[],selected:!0,slug:"paper_75",title:"Intuitive and Safe Interaction in Multi-User Human Robot Collaboration Environments through Augmented Reality Displays",abstract:"As autonomous collaborative robots are more widely used in work environments alongside humans it is of great importance to facilitate the communication between people and robotic systems, in a way that promotes safety and productivity. To this end, we propose an Augmented Reality (AR) based system that allows workers in a human-robot collaborative environment to interact with a robot while also receiving information regarding the robot state and plans that relate to the human\u2019s safety and trust, such as the intended movement of the robotic arm or the navigation plan of the mobile platform. To evaluate the effectiveness of the proposed system we conducted experiments with 13 participants, where two users had to work in the same workspace while being assisted by a mobile manipulator. We measured the task completion time as well as the robot idle time using our AR-based human-robot interaction system and compared them to a conventional setup without the use of augmented reality. Additional, subjective evaluations related to user satisfaction, system usability, perceived safety and trust showed that users assessed the system in a positive way and preferred AR visualization over more traditional interfaces.",authors:"Tsamis, Georgios; Chantziaras, Georgios; Giakoumis, Dimitrios; Kostavelis, Ioannis; Kargakos, Andreas; Tsakiris, Athanasios; Tzovaras, Dimitrios",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/RO-MAN50785.2021.9515474",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Head-Attached",parent:"On-Human"}]},{UID:76,citations:[],selected:!0,slug:"paper_76",title:"Communicating Robot Motion Intent with Augmented Reality",abstract:"Humans coordinate teamwork by conveying intent through social cues, such as gestures and gaze behaviors. However, these methods may not be possible for appearance-constrained robots that lack anthropomorphic or zoomorphic features, such as aerial robots. We explore a new design space for communicating robot motion intent by investigating how augmented reality (AR) might mediate human-robot interactions. We develop a series of explicit and implicit designs for visually signaling robot motion intent using AR, which we evaluate in a user study. We found that several of our AR designs significantly improved objective task efficiency over a baseline in which users only received physically-embodied orientation cues. In addition, our designs offer several trade-offs in terms of intent clarity and user perceptions of the robot as a teammate.",authors:"Walker, Michael; Hedayati, Hooman; Lee, Jennifer; Szafir, Daniel",venue:"ACM",year:2018,link:"https://doi.org/10.1145/3171221.3171253",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Head-Attached",parent:"On-Human"}]},{UID:77,citations:[],selected:!0,slug:"paper_77",title:"Communicating robotic navigational intentions",abstract:"This paper presents a study on intention communication in a navigational context using a robotic wheelchair. The robotic wheelchair uses light projection to communicate its motion intentions. The novelty of the work is threefold: the communication of robot intentions to the passenger, the consideration of passenger and robot as a group (\u201cin-group\u201d) [1] who share motion intentions and the communication of the in-group intentions to other pedestrians (the \u201cout-group\u201d). A comparison in an autonomous navigation task where the robotic wheelchair autonomously navigates the environment with and without intention communication was performed showing that passengers and walking people found intention communication intuitive and helpful for passing by actions. Evaluation results significantly show human participant preference for having navigational intention communication for the wheelchair passenger and the person passing by it. Quantitative results show the motion of the person passing by the wheelchair with intention communication was significantly smoother compared to without intention communication.",authors:"Watanabe, Atsushi; Ikeda, Tetsushi; Morales, Yoichi; Shinozawa, Kazuhiko; Miyashita, Takahiro; Hagita, Norihiro",venue:"IEEE",year:2015,link:"https://doi.org/10.1109/IROS.2015.7354195",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Hand-Held",parent:"On-Human"},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:78,citations:[],selected:!0,slug:"paper_78",title:"A Laser Projection System for Robot Intention Communication and Human Robot Interaction",abstract:"In order to deploy service robots in environments where they encounter and/or cooperate with persons, one important key factor is human acceptance. Hence, information on which upcoming actions of the robot are based has to be made transparent and understandable to the human. However, considering the restricted power resources of mobile robot platforms, systems for visualization not only have to be expressive but also energy efficient. In this paper, we applied the well-known technique of laser scanning on a mobile robot to create a novel system for intention visualization and human-robot-interaction. We conducted user tests to compare our system to a low-power consuming LED video projector solution in order to evaluate the suitability for mobile platforms and to get human impressions of both systems. We can show that the presented system is preferred by most users in a dynamic test setup on a mobile platform.",authors:"Wengefeld, Tim; Hochemer, Dominik; Lewandowski, Benjamin; Kohler, Mona; Beer, Manuel; Gross, Horst-Michael",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/RO-MAN47096.2020.9223517",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:79,citations:[],selected:!0,slug:"paper_79",title:"Robot Gesture Sonification to Enhance Awareness of Robot Status and Enjoyment of Interaction",abstract:"We present a divergent approach to robotic sonification with the goal of improving the quality and safety of human-robot interactions. Sonification (turning data into sound) has been underutilized in robotics, and has broad potential to convey robotic movement and intentions to users without requiring visual engagement. We design and evaluate six different sonifications of movements for a robot with four degrees of freedom. Our sonification techniques include a direct mapping from each degree of freedom to pitch and timbre changes, emotion-based sound mappings, and velocity-based mappings using different types of sounds such as motors and music. We evaluate these sonifications using metrics for ease of use, enjoyment/appeal, and conveyance of movement information. Based on our results, we make recommendations to inform decisions for future robot sonification design. We suggest that when using sonification to improve safety of human-robot collaboration, it is necessary not only to convey sufficient information about movements, but also to convey that information in a pleasing and even social way to to enhance the human-robot relationship.",authors:"Zahray, Lisa; Savery, Richard; Syrkett, Liana; Weinberg, Gil",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/RO-MAN47096.2020.9223452",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Attached",parent:"On-Robot"}]},{UID:80,citations:[],selected:!0,slug:"paper_80",title:"Expressive robot motion timing",abstract:"Our goal is to enable robots to time their motion in a way that is purposefully expressive of their internal states, making them more transparent to people. We start by investigating what types of states motion timing is capable of expressing, focusing on robot manipulation and keeping the path constant while systematically varying the timing. We find that users naturally pick up on certain properties of the robot (like confidence), of the motion (like naturalness), or of the task (like the weight of the object that the robot is carrying). We then conduct a hypothesis-driven experiment to tease out the directions and magnitudes of these effects, and use our findings to develop candidate mathematical models for how users make these inferences from the timing. We find a strong correlation between the models and real user data, suggesting that robots can leverage these models to autonomously optimize the timing of their motion to be expressive.",authors:"Zhou, Allan; Hadfield-Menell, Dylan; Nagabandi, Anusha; Dragan, Anca D.",venue:"ACM",year:2017,link:"https://doi.org/10.1145/2909824.3020221",tags:[{name:"Intent Location",parent:null},{name:"On-Robot",parent:"Intent Location"},{name:"Robot-Only",parent:"On-Robot"}]},{UID:81,citations:[],selected:!0,slug:"paper_81",title:"Towards Explainable Shared Control using Augmented Reality",abstract:"Shared control plays a pivotal role in establishing effective human-robot interactions. Traditional control-sharing methods strive to complement a human's capabilities at safely completing a task, and thereby rely on users forming a mental model of the expected robot behaviour. However, these methods can often bewilder or frustrate users whenever their actions do not elicit the intended system response, forming a misalignment between the respective internal models of the robot and human. To resolve this model misalignment, we introduce Explainable Shared Control as a paradigm in which assistance and information feedback are jointly considered. Augmented reality is presented as an integral component of this paradigm, by visually unveiling the robot's inner workings to human operators. Explainable Shared Control is instantiated and tested for assistive navigation in a setup involving a robotic wheelchair and a Microsoft HoloLens with add-on eye tracking. Experimental results indicate that the introduced paradigm facilitates transparent assistance by improving recovery times from adverse events associated with model misalignment.",authors:"Zolotas, Mark; Demiris, Yiannis",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/IROS40897.2019.8968117",tags:[{name:"Intent Location",parent:null},{name:"On-Human",parent:"Intent Location"},{name:"Head-Attached",parent:"On-Human"}]}],taxonomy:[[{name:"Intent Location",expanded:!0,parent:null,level:1,start:24,stop:28}],[{name:"On-Human",expanded:!0,parent:"Intent Location",level:2,start:24,stop:25},{name:"On-World",expanded:!0,parent:"Intent Location",level:2,start:26,stop:26},{name:"On-Robot",expanded:!0,parent:"Intent Location",level:2,start:27,stop:28}],[{name:"Head-Attached",expanded:!0,parent:"On-Human",level:3,start:24,stop:24},{name:"Hand-Held",expanded:!0,parent:"On-Human",level:3,start:25,stop:26},{name:"Robot-Only",expanded:!0,parent:"On-Robot",level:3,start:27,stop:27},{name:"Robot-Attached",expanded:!0,parent:"On-Robot",level:3,start:28,stop:28}]]}]},337:function(e,t,n){e.exports=n(449)},342:function(e,t,n){},343:function(e,t,n){},390:function(e){e.exports=[{UID:5,pos:[45.90019226074219,11.80744457244873]},{UID:6,pos:[-18.60301971435547,85.84136199951172]},{UID:7,pos:[6.4254608154296875,-125.3141098022461]},{UID:8,pos:[3.247023105621338,-105.02702331542969]},{UID:9,pos:[48.304996490478516,76.95267486572266]},{UID:10,pos:[-1.503680944442749,43.41666030883789]},{UID:11,pos:[29.983654022216797,-28.2624454498291]},{UID:12,pos:[15.326433181762695,93.52660369873047]},{UID:13,pos:[-77.82498168945312,-12.859649658203125]},{UID:14,pos:[-131.37095642089844,40.06039047241211]},{UID:15,pos:[-36.70307159423828,-5.107583045959473]},{UID:16,pos:[-16.609750747680664,-58.38809585571289]},{UID:17,pos:[67.98916625976562,24.6405029296875]},{UID:18,pos:[37.61412811279297,-13.612528800964355]},{UID:19,pos:[-64.63665771484375,7.530231952667236]},{UID:20,pos:[-53.51640319824219,15.994963645935059]},{UID:21,pos:[-24.44536590576172,13.883286476135254]},{UID:22,pos:[65.67799377441406,-36.18730163574219]},{UID:23,pos:[7.373219013214111,-68.68936157226562]},{UID:24,pos:[81.1750717163086,-58.95478439331055]},{UID:25,pos:[1.1881678104400635,120.90461730957031]},{UID:26,pos:[-25.037948608398438,42.165550231933594]},{UID:27,pos:[52.3983154296875,55.70845031738281]},{UID:28,pos:[14.674687385559082,54.56484603881836]},{UID:29,pos:[-125.4712142944336,-6.549402713775635]},{UID:30,pos:[-43.13970947265625,39.00941467285156]},{UID:31,pos:[2.040534019470215,67.08483123779297]},{UID:32,pos:[-15.371975898742676,62.01467514038086]},{UID:33,pos:[-34.96826171875,-48.58651351928711]},{UID:34,pos:[-150.34835815429688,2.4458391666412354]},{UID:35,pos:[-73.8797836303711,45.39719009399414]},{UID:36,pos:[-59.59072494506836,80.76887512207031]},{UID:37,pos:[31.470279693603516,55.87773513793945]},{UID:38,pos:[-40.07778549194336,59.47133255004883]},{UID:39,pos:[68.2674331665039,-17.814035415649414]},{UID:40,pos:[85.09355163574219,-41.105499267578125]},{UID:41,pos:[5.79752779006958,-34.4417839050293]},{UID:42,pos:[-15.250393867492676,-2.7057266235351562]},{UID:43,pos:[73.05728149414062,-75.10035705566406]},{UID:44,pos:[22.326583862304688,9.76297664642334]},{UID:45,pos:[-102.40913391113281,70.37290954589844]},{UID:46,pos:[38.398216247558594,103.51252746582031]},{UID:47,pos:[-47.26205062866211,-27.18473243713379]},{UID:48,pos:[-75.70504760742188,78.38201904296875]},{UID:49,pos:[-11.208576202392578,28.829654693603516]},{UID:50,pos:[3.1167449951171875,13.677165985107422]},{UID:51,pos:[-95.16407775878906,11.207165718078613]},{UID:52,pos:[141.6609344482422,-61.036041259765625]},{UID:53,pos:[133.6681365966797,-27.24371337890625]},{UID:54,pos:[125.79167175292969,-42.65171432495117]},{UID:55,pos:[120.6871566772461,-56.176429748535156]},{UID:56,pos:[107.12606811523438,-82.17806243896484]},{UID:57,pos:[-51.96857452392578,-45.47879409790039]},{UID:58,pos:[16.966386795043945,-12.139812469482422]},{UID:59,pos:[-31.156978607177734,-28.033008575439453]},{UID:60,pos:[45.299278259277344,-47.328514099121094]},{UID:61,pos:[-101.91929626464844,-11.868230819702148]},{UID:62,pos:[-78.13699340820312,122.27196502685547]},{UID:63,pos:[11.411088943481445,31.728281021118164]},{UID:64,pos:[62.01409149169922,-.5698369741439819]},{UID:65,pos:[86.66459655761719,-14.64435863494873]},{UID:66,pos:[-106.309326171875,35.2706413269043]},{UID:67,pos:[-11.566534996032715,-107.13087463378906]},{UID:68,pos:[-24.762531280517578,-85.68598937988281]},{UID:69,pos:[-25.349294662475586,-114.95891571044922]},{UID:70,pos:[-53.7142219543457,-65.09370422363281]},{UID:71,pos:[-115.45210266113281,14.087030410766602]},{UID:72,pos:[-130.48220825195312,12.619328498840332]},{UID:73,pos:[-78.41954803466797,-67.92549133300781]},{UID:74,pos:[-40.81128692626953,106.38102722167969]},{UID:75,pos:[46.84365463256836,-28.639781951904297]},{UID:76,pos:[-1.680828332901001,-13.704047203063965]},{UID:77,pos:[42.138797760009766,-91.70773315429688]},{UID:78,pos:[83.11547088623047,12.544416427612305]},{UID:79,pos:[32.828041076660156,33.807411193847656]},{UID:80,pos:[28.268407821655273,72.75281524658203]},{UID:81,pos:[25.45851707458496,-47.85636520385742]}]},394:function(e){e.exports={nodes:[{UID:5,citations:[],selected:!0,slug:"paper_5",title:"Projecting robot intentions into human environments",abstract:"Trained human co-workers can often easily predict each other's intentions based on prior experience. When collaborating with a robot coworker, however, intentions are hard or impossible to infer. This difficulty of mental introspection makes human-robot collaboration challenging and can lead to dangerous misunderstandings. In this paper, we present a novel, object-aware projection technique that allows robots to visualize task information and intentions on physical objects in the environment. The approach uses modern object tracking methods in order to display information at specific spatial locations taking into account the pose and shape of surrounding objects. As a result, a human co-worker can be informed in a timely manner about the safety of the workspace, the site of next robot manipulation tasks, and next subtasks to perform. A preliminary usability study compares the approach to collaboration approaches based on monitors and printed text. The study indicates that, on average, the user effectiveness and satisfaction is higher with the projection based approach.",authors:"Andersen, Rasmus S.; Madsen, Ole; Moeslund, Thomas B.; Amor, Heni Ben",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745145",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"World Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"},{name:"Robot World Perception",parent:"State"},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:6,citations:[],selected:!0,slug:"paper_6",title:"Designing Multimodal Intent Communication Strategies for Conflict Avoidance in Industrial Human-Robot Teams",abstract:"Robot-to-human intent communication has been proposed as a method of enabling fluent coordination in human-robot teams. Prior research has focused on identifying modalities by which intent information can be accurately communicated, but has not yet studied whether intent communication enables fluent or safer coordination in human-robot teams in which intent communication is only supportive to the team's primary task. To address this question, we conduct a study (N = 29) in a mock collaborative manufacturing scenario in which motion-based and display-based intent communication approaches are evaluated under varying penalties for failing to coordinate safely. Subjective and objective measures of team fluency suggest that although intent communication supports fluent coordination, using a purely motion-based or a purely display-based approach may not be the most effective strategy. Although multimodal intent communication did not significantly improve upon unimodal approaches, merging both motion-based and display-based intent communication seems to combine the strengths of both approaches. Interestingly, results also suggest that contrary to theoretical predictions, the positive effect of intent communication is generally robust to teaming scenarios that require members to operate concurrently.",authors:"Aubert, Miles C.; Bader, Hayden; Hauser, Kris",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ROMAN.2018.8525557",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"},{name:"Attention",parent:"Intent Type"},{name:"Robot-Focused",parent:"Attention"}]},{UID:7,citations:[],selected:!0,slug:"paper_7",title:"Legible light communications for factory robots",abstract:"This work focuses on methods to improve mobile robot legibility in factories using lights. Implementation and evaluation were done at a robotics company that manufactures factory robots that work in human spaces. Three new sets of communicative lights were created and tested on the robots, integrated into the company's software stack and compared to the industry default lights that currently exist on the robots. All three newly designed light sets outperformed the industry default. Insights from this work have been integrated into software releases across North America.",authors:"Bacula, Alexandra; Mercer, Jason; Knight, Heather",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3371382.3378305",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:8,citations:[],selected:!0,slug:"paper_8",title:"Enhancing human understanding of a mobile robot\u2019s state and actions using expressive lights",abstract:"In order to be successfully integrated into human-populated environments, mobile robots need to express relevant information about their state to the outside world. In particular, animated lights are a promising way to express hidden robot state information such that it is visible at a distance. In this work, we present an online study to evaluate the effect of robot communication through expressive lights on people's understanding of the robot's state and actions. In our study, we use the CoBot mobile service robot with our light interface, designed to express relevant robot information to humans. We evaluate three designed light animations on three corresponding scenarios for each, for a total of nine scenarios. Our results suggest that expressive lights can play a significant role in helping people accurately hypothesize about a mobile robot's state and actions from afar when minimal contextual clues are present. We conclude that lights could be generally used as an effective non-verbal communication modality for mobile robots in the absence of, or as a complement to, other modalities.",authors:"Baraka, Kim; Rosenthal, Stephanie; Veloso, Manuela",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745187",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"},{name:"World-Centered",parent:"Instruction"}]},{UID:9,citations:[],selected:!0,slug:"paper_9",title:"A flexible optimization-based method for synthesizing intent-expressive robot arm motion",abstract:"We present an approach to synthesize robot arm trajectories that effectively communicate the robot\u2019s intent to a human collaborator while achieving task goals. Our approach uses nonlinear constrained optimization to encode task requirements and desired motion properties. Our implementation allows for a wide range of constraints and objectives. We introduce a novel objective function to optimize robot arm motions for intent-expressiveness that works in a range of scenarios and robot arm types. Our formulation supports experimentation with different theories of how viewers interpret robot motion. Through a series of human-subject experiments on real and simulated robots, we demonstrate that our method leads to improved collaborative performance against other methods, including the current state of the art. These experiments also show how our perception heuristic can affect collaborative outcomes.",authors:"Bodden, Christopher; Rakita, Daniel; Mutlu, Bilge; Gleicher, Michael",venue:"SAGE Publications",year:2018,link:"https://doi.org/10.1177/0278364918792295",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:10,citations:[],selected:!0,slug:"paper_10",title:"Transparent Robot Behavior by Adding Intuitive Visual and Acoustic Feedback to Motion Replanning",abstract:"Nowadays robots are able to work safely close to humans. They are light-weight, intrinsically safe and capable of avoiding obstacles as well as understand and predict human motions. In this collaborative scenario, the communication between humans and robots is a fundamental aspect to achieve good efficiency and ergonomics in the task execution. A lot of research has been made related to robot understanding and prediction of the human behavior, allowing the robot to replan its motion trajectories. This work is focused on the communication of the robot's intentions to the human to make its goals and planned trajectories easily understandable. Visual and acoustic information has been added to give the human an intuitive feedback to immediately understand the robot's plan. This allows a better interaction and makes the humans feel more comfortable, without any feeling of anxiety related to the unpredictability of the robot motion. Experiments have been conducted in a collaborative assembly scenario. The results of these tests were collected in questionnaires, in which the humans reported the differences and improvements they experienced using the feedback communication system.",authors:"Bolano, Gabriele; Roennau, Arne; Dillmann, Ruediger",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ROMAN.2018.8525671",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"Attention",parent:"Intent Type"},{name:"Robot-Focused",parent:"Attention"}]},{UID:11,citations:[],selected:!0,slug:"paper_11",title:"Deploying Multi-Modal Communication Using Augmented Reality in a Shared Workspace",abstract:"Robots are no longer working isolated in safety fences and Human-Robot Collaboration (HRC) is becoming one of the most promising topic of research to improve the efficiency in many application scenarios. Sharing the same workspace, both human and robot should clearly understand the intentions and motions of each other, in order to enable an efficient and effective interaction. In this work we propose an AR-based system to show the robot planned motion and target to the worker. We focused on representing this information in an intuitive way for inexperienced users. We introduced a multi-modal communication feedback in order to enable the user to agree with or change the robot plan using gestures and speech. The effectiveness of the system has been evaluated with test cases performed by a group of testers with no robotic experience. The results showed that the system helped the user to better understand the robot intentions and planned motion, improving the ergonomics and trust in the interaction. Furthermore, the evaluation included the rating of the different input modalities provided, in order to compare the different ways of communication proposed.",authors:"Bolano, Gabriele; Fu, Yuchao; Roennau, Arne; Dillmann, Ruediger",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/UR52253.2021.9494689",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:12,citations:[],selected:!0,slug:"paper_12",title:"Using Spatial and Temporal Contrast for Fluent Robot-Human Hand-Overs",abstract:"For robots to get integrated in daily tasks assisting humans, robot-human interactions will need to reach a level of fluency close to that of human-human interactions. In this paper we address the fluency of robot-human hand-overs. From an observational study with our robot HERB, we identify the key problems with a baseline hand-over action. We find that the failure to convey the intention of handing over causes delays in the transfer, while the lack of an intuitive signal to indicate timing of the hand-over causes early, unsuccessful attempts to take the object. We propose to address these problems with the use of spatial contrast, in the form of distinct hand-over poses, and temporal contrast, in the form of unambiguous transitions to the hand-over pose. We conduct a survey to identify distinct hand-over poses, and determine variables of the pose that have most communicative potential for the intent of handing over. We present an experiment that analyzes the effect of the two types of contrast on the fluency of hand-overs. We find that temporal contrast is particularly useful in improving fluency by eliminating early attempts of the human.",authors:"Cakmak, Maya; Srinivasa, Siddhartha S.; Lee, Min Kyung; Kiesler, Sara; Forlizzi, Jodi",venue:"ACM",year:2011,link:"https://doi.org/10.1145/1957656.1957823",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:13,citations:[],selected:!0,slug:"paper_13",title:"Communication Through Motion: Legibility of Multi-Robot Systems",abstract:"The interaction between a user and a multi-robot system in a shared environment is a relatively uncharted topic. But, as these types of systems will increase in the future years, an efficient way of communication is necessary. To this aim, it is interesting to discover if a multi-robot system can communicate its intentions exploiting only some motion-variables, which are characteristics of the motion of the robots. This study is about the legibility of a multi-robot system: in particular, we focus on the influence of these motion-variables on the legibility of more than one group of robots that move in a shared environment with the user. These motion-variables are: trajectory, dispersion and stiffness. They are generally used to define the motion of a group of mobile robots. Trajectory and dispersion were found relevant for the correctness of the communication between the user and the multi-robot system, while stiffness was found relevant for the rapidity of communication. The analysis of the influence of the motion-variables was carried out with an ANOVA (analysis of variance) based on a series of data coming from an experimental campaign conducted in a virtual reality set-up.",authors:"Capelli, Beatrice; Secchi, Cristian; Sabattini, Lorenzo",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/MRS.2019.8901100",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:14,citations:[],selected:!0,slug:"paper_14",title:"Emotion encoding in human-drone interaction",abstract:"Drones are becoming more popular and may soon be ubiquitous. As they enter our everyday environments, it becomes critical to ensure their usability through natural Human-Drone Interaction (HDI). Previous work in Human-Robot Interaction (HRI) shows that adding an emotional component is part of the key to success in robots' acceptability. We believe the adoption of personal drones would also benefit from adding an emotional component. This work defines a range of personality traits and emotional attributes that can be encoded in drones through their flight paths. We present a user study (N=20) and show how well three defined emotional states can be recognized. We draw conclusions on interaction techniques with drones and feedback strategies that use the drone's flight path and speed.",authors:"Cauchard, Jessica R.; Zhai, Kevin Y.; Spadafora, Marco; Landay, James A.",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/HRI.2016.7451761",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:15,citations:[],selected:!0,slug:"paper_15",title:"Using nonverbal signals to request help during human-robot collaboration",abstract:"Non-humanoid robots are becoming increasingly utilized for collaborative tasks that rely on each collaborator's ability to effectively convey their mental state while accurately estimating and interpreting their partner's knowledge, intent, and actions. During these tasks, it may be beneficial or even necessary for the human collaborator to assist the robot. Consequently, we explore the use of nonverbal signals to request help during a collaborative task. We focus on light and sound as they are commonly used communication channels across many domains. This paper analyzes the effectiveness of three nonverbal help signals that vary in urgency. Our results show that these signals significantly influence the human collaborator's and their perception of the collaboration.",authors:"Cha, Elizabeth; Mataric, Maja",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/IROS.2016.7759744",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"Robot-Focused",parent:"Attention"},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:16,citations:[],selected:!0,slug:"paper_16",title:"Bi-directional navigation intent communication using spatial augmented reality and eye-tracking glasses for improved safety in human\u2013robot interaction",abstract:"Safety, legibility and efficiency are essential for autonomous mobile robots that interact with humans. A key factor in this respect is bi-directional communication of navigation intent, which we focus on in this article with a particular view on industrial logistic applications. In the direction robot-to-human, we study how a robot can communicate its navigation intent using Spatial Augmented Reality (SAR) such that humans can intuitively understand the robot\u2019s intention and feel safe in the vicinity of robots. We conducted experiments with an autonomous forklift that projects various patterns on the shared floor space to convey its navigation intentions. We analyzed trajectories and eye gaze patterns of humans while interacting with an autonomous forklift and carried out stimulated recall interviews (SRI) in order to identify desirable features for projection of robot intentions. In the direction human-to-robot, we argue that robots in human co-habited environments need human-aware task and motion planning to support safety and efficiency, ideally responding to people\u2019s motion intentions as soon as they can be inferred from human cues. Eye gaze can convey information about intentions beyond what can be inferred from the trajectory and head pose of a person. Hence, we propose eye-tracking glasses as safety equipment in industrial environments shared by humans and robots. In this work, we investigate the possibility of human-to-robot implicit intention transference solely from eye gaze data and evaluate how the observed eye gaze patterns of the participants relate to their navigation decisions. We again analyzed trajectories and eye gaze patterns of humans while interacting with an autonomous forklift for clues that could reveal direction intent. Our analysis shows that people primarily gazed on that side of the robot they ultimately decided to pass by. We discuss implications of these results and relate to a control approach that uses human gaze for early obstacle avoidance.",authors:"Chadalavada, Ravi Teja; Andreasson, Henrik; Schindler, Maike; Palm, Rainer; Lilienthal, Achim J.",venue:"ScienceDirect",year:2020,link:"https://doi.org/10.1016/j.rcim.2019.101830",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:17,citations:[],selected:!0,slug:"paper_17",title:"Projection-Aware Task Planning and Execution for Human-in-the-Loop Operation of Robots in a Mixed-Reality Workspace",abstract:"Recent advances in mixed-reality technologies have renewed interest in alternative modes of communication for human-robot interaction. However, most of the work in this direction has been confined to tasks such as teleoperation, simulation or explication of individual actions of a robot. In this paper, we will discuss how the capability to project intentions affect the task planning capabilities of a robot. Specifically, we will start with a discussion on how projection actions can be used to reveal information regarding the future intentions of the robot at the time of task execution. We will then pose a new planning paradigm - projection-aware planning - whereby a robot can trade off its plan cost with its ability to reveal its intentions using its projection actions. We will demonstrate each of these scenarios with the help of a joint human-robot activity using the HoloLens.",authors:"Chakraborti, Tathagata; Sreedharan, Sarath; Kulkarni, Anagha; Kambhampati, Subbarao",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/IROS.2018.8593830",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"World Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot World Perception",parent:"State"},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:18,citations:[],selected:!0,slug:"paper_18",title:"Negotiation based Human-Robot Collaboration via Augmented Reality",abstract:"Effective human-robot collaboration (HRC) requires extensive communication among the human and robot teammates, because their actions can potentially produce conflicts, synergies, or both. We develop a novel augmented reality (AR) interface to bridge the communication gap between human and robot teammates. Building on our AR interface, we develop an AR-mediated, negotiation-based (ARN) framework for HRC. We have conducted experiments both in simulation and on real robots in an office environment, where multiple mobile robots work on delivery tasks. The robots could not complete the tasks on their own, but sometimes need help from their human teammate, rendering human-robot collaboration necessary. Results suggest that ARN significantly reduced the human-robot team's task completion time compared to a non-AR baseline approach.",authors:"Chandan, Kishan; Kudalkar, Vidisha; Li, Xiang; Zhang, Shiqi",venue:"arXiv",year:2019,link:"https://doi.org/10.48550/arXiv.1909.11227",tags:[{name:"Intent Type",parent:null},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:19,citations:[],selected:!0,slug:"paper_19",title:"Avoiding Human-Robot Collisions Using Haptic Communication",abstract:"Fully autonomous navigation in populated environments is still a challenging problem for mobile robots. This paper explores the idea of using active human-robot communication to facilitate navigation tasks. We propose to convey a robot's intent to human users via a wearable haptic interface. The interface can display distinct haptic cues by modulating vibration amplitudes and patterns. We applied the concept to a single human/single robot orthogonal encounter scenario, where one of the two parties has to yield the right of way to avoid collision. Under certain conditions, the robot's intent (to yield to the human or not) is revealed to the human via the haptic interface prior to the interaction. We conducted an experiment with 10 users, in which the robot was teleoperated as a substitute for autonomy. Results show that, when given priority, users become more risk-accepting and use different strategies to navigate the collision scenario than when the robot takes priority or there is no haptic communication channel. In addition, we propose a social-force based model to predict human movement during navigation. The effect of communication can be explained as a shift in the user's safety buffer and expectation of the robot's future velocity.",authors:"Che, Yuhang; Sun, Cuthbert T.; Okamura, Allison M.",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ICRA.2018.8460946",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"Robot-Focused",parent:"Attention"}]},{UID:20,citations:[],selected:!0,slug:"paper_20",title:"Efficient and Trustworthy Social Navigation via Explicit and Implicit Robot\u2013Human Communication",abstract:"In this article, we present a planning framework that uses a combination of implicit (robot motion) and explicit (visual/audio/haptic feedback) communication during mobile robot navigation. First, we developed a model that approximates both continuous movements and discrete behavior modes in human navigation, considering the effects of implicit and explicit communication on human decision-making. The model approximates the human as an optimal agent, with a reward function obtained through inverse reinforcement learning. Second, a planner uses this model to generate communicative actions that maximize the robot's transparency and efficiency. We implemented the planner on a mobile robot, using a wearable haptic device for explicit communication. In a user study of an indoor human-robot pair orthogonal crossing situation, the robot is able to actively communicate its intent to users in order to avoid collisions and facilitate efficient trajectories. Results show that the planner generated plans that are easier to understand, reduce users` effort, and increase users' trust of the robot, compared to simply performing collision avoidance. The key contribution of this article is the integration and analysis of explicit communication (together with implicit communication) for social navigation.",authors:"Che, Yuhang; Okamura, Allison M.; Sadigh, Dorsa",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/TRO.2020.2964824",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:21,citations:[],selected:!0,slug:"paper_21",title:"Touched by a Robot: An Investigation of Subjective Responses to Robot-Initiated Touch",abstract:"By initiating physical contact with people, robots can be more useful. For example, a robotic caregiver might make contact to provide physical assistance or facilitate communication. So as to better understand how people respond to robot-initiated touch, we conducted a 2x2 between-subjects experiment with 56 people in which a robotic nurse autonomously touched and wiped the subject\u2019s forearm. Our independent variables were whether or not the robot verbally warned the person before contact, and whether the robot verbally indicated that the touch was intended to clean the person\u2019s skin (instrumental touch) or to provide comfort (affective touch). On average, regardless of the treatment, participants had a generally positive subjective response. However, with instrumental touch people responded significantly more favorably. Since the physical behavior of the robot was the same for all trials, our results demonstrate that the perceived intent of the robot can significantly influence a person\u2019s subjective response to robot-initiated touch. Our results suggest that roboticists should consider this factor in addition to the mechanics of physical interaction. Unexpectedly, we found that participants tended to respond more favorably without a verbal warning. Although inconclusive, our results suggest that verbal warnings prior to contact should be carefully designed, if used at all.",authors:"Chen, Tiffany L.; King, Chih-Hung; Thomaz, Andrea L.; Kemp, Charles C.",venue:"ACM",year:2011,link:"https://doi.org/10.1145/1957656.1957818",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:22,citations:[],selected:!0,slug:"paper_22",title:"Dynamic Path Visualization for Human-Robot Collaboration",abstract:"Augmented reality technology can enable robots to visualize their future actions giving users crucial information to avoid collisions and other conflicting actions. Although a robot\u2019s entire action plan could be visualized (such as the output of a navigational planner), how far into the future it is appropriate to display the robot\u2019s plan is unknown. We developed a dynamic path visualizer that projects the robot\u2019s motion intent at varying lengths depending on the complexity of the upcoming path. We tested our approach in a virtual game where participants were tasked to collect and deliver gems to a robot that moves randomly towards a grid of markers in a confined area. Preliminary results on a small sample size indicate no significant effect on task performance; however, open-ended responses reveal participants preference towards visuals that show longer path projections.",authors:"Cleaver, Andre; Tang, Darren Vincent; Chen, Victoria; Short, Elaine Schaertl; Sinapov, Jivko",venue:"ACM",year:2021,link:"https://doi.org/10.1145/3434074.3447188",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:23,citations:[],selected:!0,slug:"paper_23",title:"MIRO: A Versatile Biomimetic Edutainment Robot",abstract:"Here we present MIRO, a companion robot designed to engage users in science and robotics via edutainment. MIRO is a robot that is biomimetic in aesthetics, morphology, behaviour, and control architecture. In this paper, we review how these design choices affect its suitability for a companionship role. In particular, we consider how MIRO\u2019s emulation of familiar mammalian body language as one component of a broader biomimetic expressive system provides effective communication of emotional state and intent. We go on to discuss how these features contribute to MIRO\u2019s potential in other domains such as healthcare, education, and research.",authors:"Collins, Emily C.; Prescott, Tony J.; Mitchinson, Ben; Conran, Sebastian",venue:"ACM",year:2015,link:"https://doi.org/10.1145/2832932.2832978",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:24,citations:[],selected:!0,slug:"paper_24",title:"Spatial augmented reality as a method for a mobile robot to communicate intended movement",abstract:"Our work evaluates a mobile robot\u2019s ability to communicate intended movements to humans via projection of visual arrows and a simplified map. Humans utilize a variety of techniques to signal intended movement in a co-occupied space. We evaluated an augmented reality projection provided by the robot. The projection is on the floor and consists of arrows and a simplified map. Two pilots and one quasi-experiment were conducted to examine the effectiveness of visual projection of arrows by a robot for signaling intended movement. The pilot work demonstrates the effectiveness of utilizing arrows as a communication medium. The experiment examined the effectiveness of a simplified map and arrows for signaling the short-, mid-range, and long-term intended movement. Two pilot experiments confirm that arrows are an effective symbol for a robot to use to signal intent. A field experiment demonstrates that a robot can use a projected arrow and simplified map to signal its intended movement and people understand the projection for upcoming short-, medium-, and long-term movement. Augmented reality, such as projected arrows and simplified map, are an effective tool for robots to use when signaling their upcoming movement to humans. Telepresence robots in organizations, museum docents, information kiosks, hospital assistants, factories, and as members of search and rescue teams are typical applications where mobile robots reside and interact with people.",authors:"Coovert, Michael D.; Lee, Tiffany; Shindev, Ivan; Sun, Yu",venue:"ScienceDirect",year:2014,link:"https://doi.org/10.1016/j.chb.2014.02.001",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot World Perception",parent:"State"}]},{UID:25,citations:[],selected:!0,slug:"paper_25",title:"Multimodal Interaction with an Autonomous Forklift",abstract:'We describe a multimodal framework for interacting with an autonomous robotic forklift. A key element enabling effective interaction is a wireless, handheld tablet with which a human supervisor can command the forklift using speech and sketch. Most current sketch interfaces treat the canvas as a blank slate. In contrast, our interface uses live and synthesized camera images from the forklift as a canvas, and augments them with object and obstacle information from the world. This connection enables users to "draw on the world," enabling a simpler set of sketched gestures. Our interface supports commands that include summoning the forklift and directing it to lift, transport, and place loads of palletized cargo. We describe an exploratory evaluation of the system designed to identify areas for detailed study.Our framework incorporates external signaling to interact with humans near the vehicle. The robot uses audible and visual annunciation to convey its current state and intended actions. The system also provides seamless autonomy handoff: any human can take control of the robot by entering its cabin, at which point the forklift can be operated manually until the human exits.',authors:"Correa, Andrew; Walter, Matthew R.; Fletcher, Luke; Glass, Jim; Teller, Seth; Davis, Randall",venue:"ACM",year:2010,link:"https://doi.org/10.1109/HRI.2010.5453188",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"},{name:"Robot World Perception",parent:"State"}]},{UID:26,citations:[],selected:!0,slug:"paper_26",title:"LED Strip Based Robot Movement Intention Signs for Human-Robot Interactions",abstract:"As a new kind of robots, called cooperative robots, are more commonly used in industry, a new way of communication is becoming more important due to the increasing number of closer cooperation between human and robots. This paper proposes the idea behind a novel method of using visual communication between cobots and humans focusing mainly on the field of industrial robotics. This device can decrease the mental stress experienced by the coworker and can increase the trust resulting in a closer to ergonomic workspace from the coworker viewpoint. Other possible usage is also discussed.",authors:"Domonkos, Mark; Dombi, Zoltan; Botzheim, Janos",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/CINTI51262.2020.9305854",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:27,citations:[],selected:!0,slug:"paper_27",title:"Generating legible motion",abstract:"Legible motion \u2014 motion that communicates its intent to a human observer \u2014 is crucial for enabling seamless human-robot collaboration. In this paper, we propose a functional gradient optimization technique for autonomously generating legible motion. Our algorithm optimizes a legibility metric inspired by the psychology of action interpretation in humans, resulting in motion trajectories that purposefully deviate from what an observer would expect in order to better convey intent. A trust region constraint on the optimization ensures that the motion does not become too surprising or unpredictable to the observer. Our studies with novice users that evaluate the resulting trajectories support the applicability of our method and of such a trust region. They show that within the region, legibility as measured in practice does significantly increase. Outside of it, however, the trajectory becomes confusing and the users\u2019 confidence in knowing the robot\u2019s intent significantly decreases.",authors:"Dragan, Anca and Srinivasa, Siddhartha",venue:"roboticsproceedings.org",year:2013,link:"http://www.roboticsproceedings.org/rss09/p24.pdf",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:28,citations:[],selected:!0,slug:"paper_28",title:"Effects of Robot Motion on Human-Robot Collaboration",abstract:"Most motion in robotics is purely functional, planned to achieve the goal and avoid collisions. Such motion is great in isolation, but collaboration affords a human who is watching the motion and making inferences about it, trying to coordinate with the robot to achieve the task. This paper analyzes the benefit of planning motion that explicitly enables the collaborator\u2019s inferences on the success of physical collaboration, as measured by both objective and subjective metrics. Results suggest that legible motion, planned to clearly express the robot\u2019s intent, leads to more fluent collaborations than predictable motion, planned to match the collaborator\u2019s expectations. Furthermore, purely functional motion can harm coordination, which negatively affects both task efficiency, as well as the participants\u2019 perception of the collaboration.",authors:"Dragan, Anca D.; Bauman, Shira; Forlizzi, Jodi; Srinivasa, Siddhartha S.",venue:"ACM",year:2015,link:"https://doi.org/10.1145/2696454.2696473",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:29,citations:[],selected:!0,slug:"paper_29",title:"Investigation of Communicative Flight Paths for Small Unmanned Aerial Systems * This work was supported by NSF NRI 1638099",abstract:"This project seeks to generate small Unmanned Aerial System (sUAS) flight paths that are broadly understood by the general population and can communicate states about both the sUAS and its understanding of the world. Previous work in sUAS flight paths has sought to communicate intent, destination, or emotion of the system without focusing on concrete states (e.g., low battery, landing, etc.). This work leverages biologically-based flight paths and experimental methodologies from human-human and human-humanoid robot interactions to assess the understanding of avian flight paths to communicate sUAS states to novice users. If successful, this work should inform: the human-robot interaction community about the perception of flight paths, sUAS manufacturers on how their systems could communicate with both operators and bystanders, and end users on ways to communicate with others when flying systems in public spaces. General design implications and future directions of work are suggested to build on the results here, which suggest that novice users gravitate towards labels they understand (draw attention and landing) while avoiding more technical labels (lost sensor).",authors:"Duncan, Brittany A.; Beachly, Evan; Bevins, Alisha; Elbaum, Sebasitan; Detweiler, Carrick",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ICRA.2018.8462871",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:30,citations:[],selected:!0,slug:"paper_30",title:"Follow me: Communicating intentions with a spherical robot",abstract:"In recent years, robots have gradually become incorporated in our society and therefore play more relevant role in social environments. These robots vary in form, some being more anthropomorphic than others. This, creates a need to study their interaction with the world. In this paper we used Sphero and BB-8, two robots with a simple spherical body devoid of verbal and other complex communication methods, to investigate how they can communicate intention to people. A set of behaviors based on pet behaviors was designed and tested in a controlled experiment, where the robot's aim was to convince a participant to follow it. We concluded that the use of these behaviors allows a robot to effectively communicate intention as well as create a bond with the participant, who would treat it as an equal, thereby engaging it in social interactions such as playing with it or talking to it.",authors:"Faria, Miguel; Costigliola, Andrea; Alves-Oliveira, Patricia; Paiva, Ana",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745189",tags:[{name:"Intent Type",parent:null},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:31,citations:[],selected:!0,slug:"paper_31",title:"\u201cMe and you together\u201d movement impact in multi-user collaboration tasks",abstract:"This paper presents a study on collaborative manipulation between an autonomous robot and multiple users. We investigate how different motion types impact people's ability to understand the robot's goals in a multi-user scenario. We propose an approach based on Collaborative Probabilistic Movement Primitives to generate the robot's movements, exploiting predictability and legibility of movement to express intentions through motion. We compare the impact on the interaction of using only either predictable or legible movements, and propose a third approach - hybrid motion - that selects, in each situation, whether to execute a predictable motion or a legible motion, depending on what the robot perceives as more efficient for the multi-user collaboration effort. To test the impact of the three motion types in the context of a collaborative task, we run a user study using a Baxter robot that autonomously serves cups of water to three users upon request. Our results show that, in the particular case where all users simultaneously request water, the hybrid motion performs better than the other two.",authors:"Faria, Miguel; Silva, Rui; Alves-Oliveira, Patricia; Melo, Francisco S.; Paiva, Ana",venue:"IEEE",year:2017,link:"https://doi.org/10.1109/IROS.2017.8206109",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:32,citations:[],selected:!0,slug:"paper_32",title:"Understanding Robots: Making Robots More Legible in Multi-Party Interactions",abstract:"In this work we explore implicit communication between humans and robots\u2014through movement\u2014in multi-party (or multi-user) interactions. In particular, we investigate how a robot can move to better convey its intentions using legible movements in multi-party interactions. Current research on the application of legible movements has focused on single-user interactions, causing a vacuum of knowledge regarding the impact of such movements in multi-party interactions. We propose a novel approach that extends the notion of legible motion to multi-party settings, by considering that legibility depends on all human users involved in the interaction, and should take into consideration how each of them perceives the robot\u2019s movements from their respective points-of-view. We show, through simulation and a user study, that our proposed model of multi-user legibility leads to movements that, on average, optimize the legibility of the motion as perceived by the group of users. Our model creates movements that allow each human to more quickly and confidently understand what are the robot\u2019s intentions, thus creating safer, clearer and more efficient interactions and collaborations.",authors:"Faria, Miguel; Melo, Francisco S.; Paiva, Ana",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/RO-MAN50785.2021.9515485",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:33,citations:[],selected:!0,slug:"paper_33",title:"Between legibility and contact: The role of gaze in robot approach",abstract:"In this paper, we explore experimentally the possible tradeoff between gaze to the user and gaze to the path in robot approach. While some previous work indicates that gaze towards the user increases perceived safety because the user feels recognized, other work indicates that it is legibility of the robot's actions that put users at ease. If the robot does not drive up to the person in a straight line directly, the robot can either continuously look at the person and thus maintain eye contact, or indicate its path through its gaze behavior, increasing legibility. In an experiment with N=36 participants, we tested the tradeoff between legibility and eye contact. The behavioral results show that users are significantly more at ease with the robot that gazes at them than with the robot that looks where it is going, measured by the number of instances of glances away from the robot. Likewise, the participants rate the robot that looks at them continuously as more intelligent and more cooperative. Thus, participants value mutual gaze higher than legibility.",authors:"Fischer, Kerstin; Jensen, Lars C.; Suvei, Stefan-Daniel; Bodenhagen, Leon",venue:"IEEE",year:2016,link:"https://doi.org/10.1109/ROMAN.2016.7745186",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:34,citations:[],selected:!0,slug:"paper_34",title:"Investigation of Unmanned Aerial Vehicle Gesture Perceptibility and Impact of Viewpoint Variance<sup>*</sup>",abstract:"Unmanned Aerial Vehicle (UAV) flight paths have been shown to communicate meaning to human observers, similar to human gestural communication. This paper presents the results of a UAV gesture perception study designed to assess how observer viewpoint perspective may impact how humans perceive the shape of UAV gestural motion. Robot gesture designers have demonstrated that robots can indeed communicate meaning through gesture; however, many of these results are limited to an idealized range of viewer perspectives and do not consider how the perception of a robot gesture may suffer from obfuscation or self-occlusion from some viewpoints. This paper presents the results of three online user-studies that examine participants' ability to accurately perceive the intended shape of two-dimensional UAV gestures from varying viewer perspectives. We used a logistic regression model to characterize participant gesture classification accuracy, demonstrating that viewer perspective does impact how participants perceive the shape of UAV gestures. Our results yielded a viewpoint angle threshold from beyond which participants were able to assess the intended shape of a gesture's motion with 90% accuracy. We also introduce a perceptibility score to capture user confidence, time to decision, and accuracy in labeling and to understand how differences in flight paths impact perception across viewpoints. These findings will enable UAV gesture systems that, with a high degree of confidence, ensure gesture motions can be accurately perceived by human observers.",authors:"Fletcher, Paul; Luther, Angeline; Duncan, Brittany; Detweiler, Carrick",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/ICRA48506.2021.9561094",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:35,citations:[],selected:!0,slug:"paper_35",title:"Augmenting the audio-based expression modality of a non-affective robot",abstract:"This paper investigates the potential benefits of augmenting audio-based affective means of expression to strengthen the perceived intentions of a robot. Robots are often viewed as being simple machines with limied capabilities of communication. Changing how a robot is perceived, towards a more affective interpretation of its intentions, requires careful consideration of the means of expression available to the robot. It also requires alignment between these means to ensure they work in coordination with each other to make the robot easier to understand. As an effort to strengthen the affective interpretation of a soft robotic arm robot, we altered its overall expression by changing the available audio-based expression modalities. The system mitigatedthe naturally occurring noise from actuators and pneumatic systems and used a custom sound that supported the movement of the robot. The robot was tested by interacting with human observers (n=78) and was perceived as being significantly more curious, happy and less angry when augmented by audio that aligned with the naturally occurred robot sounds. The results show that the audio-based expression modality of robots is a valuable communication tool to consider augmenting when designing robots that convey affective information.",authors:"Frederiksen, Morten Roed; Stoey, Kasper",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/ACII.2019.8925510",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:36,citations:[],selected:!0,slug:"paper_36",title:"Touch-based information transfer from a robot modeled on the hearing dog",abstract:"Research on physical human-robot interaction has been attracting attention recently, focusing on robot embodiment. The work reported here proposes Active Touch Communication Robot (AcToR), a robot that is modeled on the hearing dog. A hearing dog is a type of dog assist people who are deaf or hard of hearing by alerting their handler to important sounds. AcToR uses the sense of touch to notify a human of the intention to transfer information. For example, when AcToR detects that a cell phone that is in another location has received a call, AcToR moves to the user's location and makes contact with the user's body to notify the user of the incoming call. The AcToR robot is based on the Roomba<sup>\xae</sup> and uses the Roomba's bumper and contact sensors to detect contact. This paper reports the results of psychological experiments using the AcToR robot that indicate the feasibility of using touch to transfer information from a robot to a person.",authors:"Furuhashi, Michihiko; Nakamura, Tsuyoshi; Kanoh, Masayoshi; Yamada, Koji",venue:"IEEE",year:2015,link:"https://doi.org/10.1109/FUZZ-IEEE.2015.7337981",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"Robot-Focused",parent:"Attention"},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:37,citations:[],selected:!0,slug:"paper_37",title:"Generating anticipation in robot motion",abstract:"Robots that display anticipatory motion provide their human partners with greater time to respond in interactive tasks because human partners are aware of robot intent earlier. We create anticipatory motion autonomously from a single motion exemplar by extracting hand and body symbols that communicate motion intent and moving them earlier in the motion. We validate that our algorithm extracts the most salient frame (i.e. the correct symbol) which is the most informative about motion intent to human observers. Furthermore, we show that anticipatory variants allow humans to discern motion intent sooner than motions without anticipation, and that humans are able to reliably predict motion intent prior to the symbol frame when motion is anticipatory. Finally, we quantified the time range for robot motion when humans can perceive intent more accurately and the collaborative social benefits of anticipatory motion are greatest.",authors:"Gielniak, Michael J.; Thomaz, Andrea L.",venue:"IEEE",year:2011,link:"https://doi.org/10.1109/ROMAN.2011.6005255",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:38,citations:[],selected:!0,slug:"paper_38",title:"Robopal: Modeling Role Transitions in Human-Robot Interaction",abstract:"We have developed a new communication robot, Robopal, which is an indoor/outdoor robot for use in human-robot interaction research in the context of daily life. Robopal's intended applications involve leading and/or following a human to a destination. Preliminary experiments have been conducted to study nonverbal cues associated with leading and following behavior, and it has been observed that some behaviors, such as glancing towards the leader or follower, appear to be role-dependent. A system for representing these behaviors with a state transition model is described, based on four kinds of interaction roles: directive, responsive, collaborative, and independent. It is proposed that behavior modeling can be simplified by using this system to represent changes in the roles the robot and human play in an interaction, and by associating appropriate behaviors to each role",authors:"Glas, Dylan F.; Miyashita, Takahiro; Ishiguro, Hiroshi; Hagita, Norihiro",venue:"IEEE",year:2007,link:"https://doi.org/10.1109/ROBOT.2007.363636",tags:[{name:"Intent Type",parent:null},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:39,citations:[],selected:!0,slug:"paper_39",title:"Mind the ARm: Realtime Visualization of Robot Motion Intent in Head-Mounted Augmented Reality",abstract:"Established safety sensor technology shuts down industrial robots when a collision is detected, causing preventable loss of productivity. To minimize downtime, we implemented three Augmented Reality (AR) visualizations (Path, Preview, and Volume) which allow users to understand robot motion intent and give way to the robot. We compare the different visualizations in a user study in which a small cognitive task is performed in a shared workspace. We found that Preview and Path required significantly longer head rotations to perceive robot motion intent. Volume, however, required the shortest head rotation and was perceived as most safe, enabling closer proximity of the robot arm before one left the shared workspace without causing shutdowns.",authors:"Gruenefeld, Uwe; Pr\xe4del, Lars; Illing, Jannike; Stratmann, Tim; Drolshagen, Sandra; Pfingsthorn, Max",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3404983.3405509",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:40,citations:[],selected:!0,slug:"paper_40",title:"Seeing Thru Walls: Visualizing Mobile Robots in Augmented Reality",abstract:"We present an approach for visualizing mobile robots through an Augmented Reality headset when there is no line-of-sight visibility between the robot and the human. Three elements are visualized in Augmented Reality: 1) Robot\u2019s 3D model to indicate its position, 2) An arrow emanating from the robot to indicate its planned movement direction, and 3) A 2D grid to represent the ground plane. We conduct a user study with 18 participants, in which each participant are asked to retrieve objects, one at a time, from stations at the two sides of a T-junction at the end of a hallway where a mobile robot is roaming. The results show that visualizations improved the perceived safety and efficiency of the task and led to participants being more comfortable with the robot within their personal spaces. Furthermore, visualizing the motion intent in addition to the robot model was found to be more effective than visualizing the robot model alone. The proposed system can improve the safety of automated warehouses by increasing the visibility and predictability of robots.",authors:"Gu, Morris; Cosgun, Akansel; Chan, Wesley P.; Drummond, Tom; Croft, Elizabeth",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/RO-MAN50785.2021.9515322",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:41,citations:[],selected:!0,slug:"paper_41",title:"Projection mapping implementation: Enabling direct externalization of perception results and action intent to improve robot explainability",abstract:"Existing research on non-verbal cues, e.g., eye gaze or arm movement, may not accurately present a robot's internal states such as perception results and action intent. Projecting the states directly onto a robot's operating environment has the advantages of being direct, accurate, and more salient, eliminating mental inference about the robot's intention. However, there is a lack of tools for projection mapping in robotics, compared to established motion planning libraries (e.g., MoveIt). In this paper, we detail the implementation of projection mapping to enable researchers and practitioners to push the boundaries for better interaction between robots and humans. We also provide practical documentation and code for a sample manipulation projection mapping on GitHub: github.com/uml-robotics/projection_mapping.",authors:"Han, Zhao; Wilkinson, Alexander; Parrillo, Jenna; Allspaw, Jordan; Yanco, Holly A.",venue:"arXiv",year:2020,link:"https://doi.org/10.48550/arXiv.2010.02263\n",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot World Perception",parent:"State"}]},{UID:42,citations:[],selected:!0,slug:"paper_42",title:"Investigating the Effectiveness of Different Interaction Modalities for Spatial Human-Robot Interaction",abstract:"With the increasing use of social robots in real environments, one of the areas of research requiring more attention is the study of human-robot interaction (HRI) when a person and robot are moving close to each other. Understanding effective ways to design how a robot should communicate its intention during dynamic movement is based on what people\u2019s expectations are and how they interpret different cues from the robot. Building on the existing literature, we tested a range of non-verbal cues such as eye contact, gaze and head nodding as part of the robot\u2019s behaviour during close proximate passing. The research aimed to investigate the effects of these cues, as well as their combination with body posture, on the efficiency of passing and the quality of HRI. Our results show that the combination of eye contact and the robot turning sideways is the most effective and appropriate compared to other modalities.",authors:"He, Jinying; van Maris, Anouk; Caleb-Solly, Praminda",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3371382.3378273",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:43,citations:[],selected:!0,slug:"paper_43",title:"Hey Robot, Which Way Are You Going? Nonverbal Motion Legibility Cues for Human-Robot Spatial Interaction",abstract:"Mobile robots have recently been deployed in public spaces such as shopping malls, airports, and urban sidewalks. Most of these robots are designed with human-aware motion planning capabilities but are not designed to communicate with pedestrians. Pedestrians that encounter these robots without prior understanding of the robots' behaviour can experience discomfort, confusion, and delayed social acceptance. In this work we designed and evaluated nonverbal robot motion legibility cues, which communicate a mobile robot's motion intention to pedestrians. We compared a motion legibility cue using Projected Arrows to one using Flashing Lights. We designed the cues to communicate path information, goal information, or both, and explored different Robot Movement Scenarios. We conducted an online user study with 229 participants using videos of the motion legibility cues. Our results show that the absence of cues was not socially acceptable, and that Projected Arrows were the more socially acceptable cue in most experimental conditions. We conclude that the presence and choice of motion legibility cues can positively influence robots' acceptance and successful deployment in public spaces.",authors:"Hetherington, Nicholas J.; Croft, Elizabeth A.; van der Loos, H. MachielF.",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/LRA.2021.3068708",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:44,citations:[],selected:!0,slug:"paper_44",title:"Legible robot pointing",abstract:"Good communication is critical to seamless human-robot interaction. Among numerous communication channels, here we focus on gestures, and in particular on spacial deixis: pointing at objects in the environment in order to reference them. We propose a mathematical model that enables robots to generate pointing configurations that make the goal object as clear as possible - pointing configurations that are legible. We study the implications of legibility on pointing, e.g. that the robot will sometimes need to trade off efficiency for the sake of clarity. Finally, we test how well our model works in practice in a series of user studies, showing that the resulting pointing configurations make the goal object easier to infer for novice users.",authors:"Holladay, Rachel M.; Dragan, Anca D.; Srinivasa, Siddhartha S.",venue:"IEEE",year:2014,link:"https://doi.org/10.1109/ROMAN.2014.6926256",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:45,citations:[],selected:!0,slug:"paper_45",title:"Auditory display of directions and states for mobile systems",abstract:"Auditory displays for mobile systems, such as service robots, have been developed. The design of directional sounds and of additional sounds for robot states (e.g., Heavy Load), as well as the design of more complicated robot sound tracks are explained. Basic musical elements and robot movement sounds have been combined. Two experimental studies, on the understandability of the directional sounds and the robot state sounds as well as on the auditory perception of intended robot trajectories in a simulated supermarket scenario, are described. Subjective evaluations of sound characteristics such as urgency, expressiveness, and annoyance have been performed by non-musicians and musicians. These experimental results are compared with the diagrams which have been computed with two wavelet techniques for time-frequency analyses.",authors:"Johannsen, Gunnar",venue:"Georgia Institute of Technology",year:2002,link:"https://smartech.gatech.edu/handle/1853/51337",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:46,citations:[],selected:!0,slug:"paper_46",title:"Communicative Cues for Reach-to-Grasp Motions: From Humans to Robots",abstract:"Intent communication is an important challenge in the context of human-robot interaction. The aim of this work is to identify subtle non-verbal cues that make communication among humans fluent and using them to generate intent expressive robot motion. A human-human reach-to-grasp experiment ( n =14) identified two temporal and two spatial cues: (1) relative time to reach maximum hand aperture ( MA ), (2) overall motion duration ( OT ), (3) exaggeration in motion ( Exg ), and (4) change in grasp modality ( GM ). Results showed there was statistically significant difference in the temporal cues between no-intention and intention conditions. A follow-up experiment ( n =30) was conducted based on these results. Reach-to-grasp motions of a simulated robot containing different cue combinations were shown to the participants. They were asked to guess the target object during robot\u2019s motion, based on the assumption that intent expressive motion would result in earlier and more accurate guesses. Results showed that, OT, GM and several cue combinations led to faster and more accurate guesses which imply they can be used to generate communicative motion. However, MA had no effect, and surprisingly Exg had a negative effect on expressiveness.",authors:"Keb\xfcde, Dogancan; Eteke, Cem; Sezgin, Tevfik Metin; Akg\xfcn, Bars",venue:"ACM",year:2018,link:"https://dl.acm.org/doi/10.5555/3237383.3237830",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:47,citations:[],selected:!0,slug:"paper_47",title:"Nonverbal Robot-Group Interaction Using an Imitated Gaze Cue",abstract:"Ensuring that a particular and unsuspecting member of a group is the recipient of a salient-item hand-over is a complicated interaction. The robot must effectively, expediently and reliably communicate its intentions to advert any tendency within the group towards antinormative behaviour. In this paper, we study how a robot can establish the participant roles of such an interaction using imitated social and contextual cues. We designed two gaze cues, the first was designed to discourage antinormative behaviour through individualising a particular member of the group and the other to the contrary. We designed and conducted a field experiment (456 participants in 64 trials) in which small groups of people (between 3 and 20 people) assembled in front of the robot, which then attempted to pass a salient object to a particular group member by presenting a physical cue, followed by one of two variations of a gaze cue. Our results showed that presenting the individualising cue had a significant (z=3.733, p=0.0002) effect on the robot\u2019s ability to ensure that an arbitrary group member did not take the salient object and that the selected participant did.",authors:"Kirchner, Nathan; Alempijevic, Alen; Dissanayake, Gamini",venue:"ACM",year:2011,link:"https://doi.org/10.1145/1957656.1957824",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:48,citations:[],selected:!0,slug:"paper_48",title:"Hey! There is someone at your door. A hearing robot using visual communication signals of hearing dogs to communicate intent",abstract:"This paper presents a study of the readability of dog-inspired visual communication signals in a human-robot interaction scenario. This study was motivated by specially trained hearing dogs which provide assistance to their deaf owners by using visual communication signals to lead them to the sound source. For our human-robot interaction scenario, a robot was used in place of a hearing dog to lead participants to two different sound sources. The robot was preprogrammed with dog-inspired behaviors, controlled by a wizard who directly implemented the dog behavioral strategy on the robot during the trial. By using dog-inspired visual communication signals as a means of communication, the robot was able to lead participants to the sound sources (the microwave door, the front door). Findings indicate that untrained participants could correctly interpret the robot's intentions. Head movements and gaze directions were important for communicating the robot's intention using visual communication signals.",authors:"Koay, K. L.; Lakatos, G.; Syrdal, D. S.; Gacsi, M.; Bereczky, B.; Dautenhahn, K.; Miklosi, A.; Walters, M. L.",venue:"IEEE",year:2013,link:"https://doi.org/10.1109/ALIFE.2013.6602436",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"Robot-Focused",parent:"Attention"},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:49,citations:[],selected:!0,slug:"paper_49",title:"Effects of Integrated Intent Recognition and Communication on Human-Robot Collaboration",abstract:"Human-robot interaction research to date has investigated intent recognition and communication separately. In this paper, we explore the effects of integrating both the robot's ability to generate intentional motion and predict the human's motion in a collaborative physical task. We implemented an intent recognition system to recognize the human partner's hand motion intent and a motion planner system to enable the robot to communicate its intent by using legible and predictable motion. We tested this bi-directional intent system in a 2-way within-subjects user study. Results suggest that an integrated intent recognition and communication system may facilitate more collaborative behavior among team members.",authors:"Lee Chang, Mai; Gutierrez, Reymundo A.; Khante, Priyanka; Schaertl Short, Elaine; Lockerd Thomaz, Andrea",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/IROS.2018.8593359",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:50,citations:[],selected:!0,slug:"paper_50",title:"Methods for Expressing Robot Intent for Human\u2013Robot Collaboration in Shared Workspaces",abstract:"Human\u2013robot collaboration is becoming increasingly common in factories around the world; accordingly, we need to improve the interaction experiences between humans and robots working in these spaces. In this article, we report on a user study that investigated methods for providing information to a person about a robot\u2019s intent to move when working together in a shared workspace through signals provided by the robot. In this case, the workspace was the surface of a tabletop. Our study tested the effectiveness of three motion-based and three light-based intent signals as well as the overall level of comfort participants felt while working with the robot to sort colored blocks on the tabletop. Although not significant, our findings suggest that the light signal located closest to the workspace\u2014an LED bracelet located closest to the robot\u2019s end effector\u2014was the most noticeable and least confusing to participants. These findings can be leveraged to support human\u2013robot collaborations in shared spaces.",authors:"LeMasurier, Gregory; Bejerano, Gal; Albanese, Victoria; Parrillo, Jenna; Yanco, Holly A.; Amerson, Nicholas; Hetrick, Rebecca; Phillips, Elizabeth",venue:"ACM",year:2021,link:"https://doi.org/10.1145/3472223",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:51,citations:[],selected:!0,slug:"paper_51",title:"Towards situational awareness from robotic group motion",abstract:"The control of multiple robots in the context of tele-exploration tasks is often attentionally taxing, resulting in a loss of situational awareness for operators. Unmanned aerial vehicle swarms require significantly more multitasking than controlling a plane, thus making it necessary to devise intuitive feedback sources and control methods for these robots. The purpose of this article is to examine a swarm's nonverbal behaviour as a possible way to increase situational awareness and reduce the operators cognitive load by soliciting intuitions about the swarm's behaviour. To progress on the definition of a database of nonverbal expressions for robot swarms, we first define categories of communicative intents based on spontaneous descriptions of common swarm behaviours. The obtained typology confirms that the first two levels (as defined by Endsley: elements of environment and comprehension of the situation) can be shared through swarms motion-based communication. We then investigate group motion parameters potentially connected to these communicative intents. Results are that synchronized movement and tendency to form figures help convey meaningful information to the operator. We then discuss how this can be applied to realistic scenarios for the intuitive command of remote robotic teams.",authors:"Levillain, Florent; St-Onge, David; Beltrame, Giovanni; Zibetti, Elisabetta",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/RO-MAN46459.2019.8956381",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"World-Focused",parent:"Attention"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:52,citations:[],selected:!0,slug:"paper_52",title:"Mobile robot with eyeball expression as the preliminary-announcement and display of the robots following motion",abstract:"This paper explains the PMR-2R (prototype mobile robot \u20132 revised), the mobile robot with the eyeball expression as the preliminary-announcement and display of the robot\u2019s following motion. Firstly, we indicate the importance of the preliminary-announcement and display function of the mobile robot\u2019s following motion for the informational affinity between human being and a robot, with explaining the conventional methods and the related works. We show the proposed four methods which are categorized into two types: one type which indicates a state just after the moment and the other type which displays from the present to some future time continuously. Then we introduce the PMR-2R, which has the omni-directional display, the magicball, on which the eyeball expresses the robot\u2019s following direction of motion and the speed of motion at the same time. From the evaluation experiment, we confirmed the efficiency of the eyeball expression to transfer the information. We also obtained the announcement at around one or two second before the actual motion may be appropriate. And finally we compare the four types of eyeball expression: the one-eyeball type, the two-eyeball type, the will-o\u2019-the-wisp type, and the armor-helmet type. From the evaluation experiment, we have declared the importance to make the robot\u2019s front more intelligible especially to announce the robot\u2019s direction of motion.",authors:"Matsumaru, Takafumi; Iwase, Kazuya; Akiyama, Kyouhei; Kusada, Takashi; Ito, Tomotaka",venue:"Springer",year:2005,link:"https://doi.org/10.1007/s10514-005-0728-8",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:53,citations:[],selected:!0,slug:"paper_53",title:"Mobile robot with preliminary-announcement function of forthcoming motion using light-ray",abstract:"This paper discusses the design and the basic characteristic of the mobile robot PMR-1 with the preliminary-announcement and display function of the forthcoming operation (the direction of motion and the speed of motion) to the people around the robot by drawing a scheduled course on a running surface using light-ray. The laser pointer is used as a light source and the light from the laser pointer is reflected in a mirror. The light-ray is projected on a running surface and a scheduled course is drawn by rotating the reflector around the pan and the tilt axes. The preliminary-announcement and display unit of the developed mobile robot can indicate the operation until 3-second-later preliminarily, so the robot moves drawing the scheduled course from the present to 3-second-later. The experiment on coordination between the preliminary-announcement and the movement has been carried out, and we confirmed the correspondence of the announced course with the robot trajectory both in the case that the movement path is given beforehand and in the case that the robot is operated with manual input from a joystick in real-time. So we have validated the coordination algorithm between the preliminary-announcement and the real movement",authors:"Matsumaru, Takafumi",venue:"IEEE",year:2006,link:"https://doi.org/10.1109/IROS.2006.281981",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:54,citations:[],selected:!0,slug:"paper_54",title:"Mobile robot with preliminary-announcement and display function of forthcoming motion using projection equipment",abstract:"This paper discusses the mobile robot PMR-5 with the preliminary-announcement and display function which indicates the forthcoming operations to the people near the robot by using a projector. The projector is set on a mobile robot and a 2D frame is projected on a running surface. In the frame, not only the scheduled course but also the states of operation can be clearly announced as the information about movement. We examine the presentation of the states of operation such as stop or going back including the time information of the scheduled course on the developed robot. Scheduled course is expressed as the arrows considering the intelligibility at sight. Arrow expresses the direction of motion directly and the length of arrow can announce the speed of motion. Operation until 3-second-later is indicated and three arrows classified by color for each second are connected and displayed so these might show the changing of speed during 3-second period. The sign for spot revolution and the characters for stop and going back are also displayed. We exhibited the robot and about 200 visitors did the questionnaire evaluation. The average of 5-stage evaluation is 3.9 points and 4.5 points for the direction of motion and the speed of motion respectively. So we obtained the evaluation that it is intelligible in general",authors:"Matsumaru, Takafumi; Kusada, Takashi; Iwase, Kazuya",venue:"IEEE",year:2006,link:"https://doi.org/10.1109/ROMAN.2006.314368",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:55,citations:[],selected:!0,slug:"paper_55",title:"Mobile robot with preliminary-announcement and indication function of forthcoming operation using flat-panel display",abstract:"This research aims to propose the method and equipment to preliminary-announce and indicate the surrounding people both the speed of motion and the direction of motion of the mobile robot that moves on a two-dimensional plane. This paper discusses the mobile robot PMR-6, in which the liquid crystal display (LCD) is set up on the mobile unit, and the state of operation at 1.5 s before the actual motion is indicated. The basis of the content to display is 'arrow' considering the intelligibility for people even at first sight. The speed of motion is expressed as the size (length and width) of the arrow and its color based on traffic signal. The direction of motion is described with the curved condition of the arrow. The characters of STOP are displayed in red in case of stop. The robot was exhibited to the 2005 International Robot Exhibition held in Tokyo. About 200 visitors answered to the questionnaires. The average of five-stage evaluation is 3.56 and 3.97 points on the speed and on the direction respectively, so the method and expression were evaluated comparatively intelligible. As for the gender, the females appreciated about the speed of motion than the males on the whole. Concerning the age, some of the younger age and the upper age admired highly about the direction of motion than the middle age.",authors:"Matsumaru, Takafumi",venue:"IEEE",year:2007,link:"https://doi.org/10.1109/ROBOT.2007.363579",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:56,citations:[],selected:!0,slug:"paper_56",title:"Expression of intention by rotational head movements for teleoperated mobile robot",abstract:"We are studying a teleoperated mobile robot that provides useful information to a pedestrian. However, it is difficult for people to understand meanings of actions, motions or movements of many conventional robots. The purpose of this study is to improve pedestrian's impressions of a robot. Especially this paper describes people's understandability of robot behaviors when a robot turns around a corner or when a person and a robot pass each other in a corridor. Our robot shows its intention to make turn by rotating its head, as though a pedestrian shows a traveling direction by his/her gaze or face direction. The robot is teleoperated by an operator for safety in public spaces, and the direction of the robot head and the moving direction of the robot body are determined by an artificial potential field (APF) generated by a target position given by the operator, positions of obstacles and pedestrians. The APF for a pedestrian is generated based on her/his personal space of a person. Thus, the robot can express the intention of its action by rotating the head to look where it is going, when the robot changes its direction around pedestrians. The intention expression can be natural and understandable for them by the rotational movement of the head before the robot turns its body actually. Impression evaluation experiments with questionnaires were conducted under the two kinds of situations to reveal the validity and effectiveness of the intention expression by the robot's head rotation. Significant differences related to understandability and some impression words were observed between with and without rotating the head.",authors:"Mikawa, Masahiko; Yoshikawa, Yuriko; Fujisawa, Makoto",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/AMC.2019.8371097",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:57,citations:[],selected:!0,slug:"paper_57",title:"Meet Me Where i\u2019m Gazing: How Shared Attention Gaze Affects Human-Robot Handover Timing",abstract:'In this paper we provide empirical evidence that using humanlike gaze cues during human-robot handovers can improve the timing and perceived quality of the handover event. Handovers serve as the foundation of many human-robot tasks. Fluent, legible handover interactions require appropriate nonverbal cues to signal handover intent, location and timing. Inspired by observations of human-human handovers, we implemented gaze behaviors on a PR2 humanoid robot. The robot handed over water bottles to a total of 102 na\\"ive subjects while varying its gaze behaviour: no gaze, gaze designed to elicit shared attention at the handover location, and the shared attention gaze complemented with a turn-taking cue. We compared subject perception of and reaction time to the robot-initiated handovers across the three gaze conditions. Results indicate that subjects reach for the offered object significantly earlier when a robot provides a shared attention gaze cue during a handover. We also observed a statistical trend of subjects preferring handovers with turn-taking gaze cues over the other conditions. Our work demonstrates that gaze can play a key role in improving user experience of human-robot handovers, and help make handovers fast and fluent.',authors:"Moon, AJung; Troniak, Daniel M.; Gleeson, Brian; Pan, Matthew K.X.J.; Zheng, Minhua; Blumer, Benjamin A.; MacLean, Karon; Croft, Elizabeth A.",venue:"ACM",year:2014,link:"https://doi.org/10.1145/2559636.2559656",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:58,citations:[],selected:!0,slug:"paper_58",title:"Communicating Inferred Goals With Passive Augmented Reality and Active Haptic Feedback",abstract:"Robots learn as they interact with humans. Consider a human teleoperating an assistive robot arm: as the human guides and corrects the arm's motion, the robot gathers information about the human's desired task. But how does the human know what their robot has inferred? Today's approaches often focus on conveying intent: for instance, using legible motions or gestures to indicate what the robot is planning. However, closing the loop on robot inference requires more than just revealing the robot's current policy: the robot should also display the alternatives it thinks are likely, and prompt the human teacher when additional guidance is necessary. In this letter we propose a multimodal approach for communicating robot inference that combines both passive and active feedback. Specifically, we leverage information-rich augmented reality to passively visualize what the robot has inferred, and attention-grabbing haptic wristbands to actively prompt and direct the human's teaching. We apply our system to shared autonomy tasks where the robot must infer the human's goal in real-time. Within this context, we integrate passive and active modalities into a single algorithmic framework that determines when and which type of feedback to provide. Combining both passive and active feedback experimentally outperforms single modality baselines; during an in-person user study, we demonstrate that our integrated approach increases how efficiently humans teach the robot while simultaneously decreasing the amount of time humans spend interacting with the robot. Videos here: https://youtu.be/swq_u4iIP-g",authors:"Mullen, James; Mosier, Josh; Chakrabarti, Sounak; Chen, Anqi; White, Tyler; Losey, Dylan",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/LRA.2021.3111055",tags:[{name:"Intent Type",parent:null},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:59,citations:[],selected:!0,slug:"paper_59",title:"Nonverbal Leakage in Robots: Communication of Intentions through Seemingly Unintentional Behavior",abstract:'Human communication involves a number of nonverbal cues that are seemingly unintentional, unconscious, and automatic-both in their production and perception-and convey rich information on the emotional state and intentions of an individual. One family of such cues is called "nonverbal leakage." In this paper, we explore whether people can read nonverbal leakage cues-particularly gaze cues-in humanlike robots and make inferences on robots\u2019 intentions, and whether the physical design of the robot affects these inferences. We designed a gaze cue for Geminoid-a highly humanlike android-and Robovie-a robot with stylized, abstract humanlike features-that allowed the robots to "leak" information on what they might have in mind. In a controlled laboratory experiment, we asked participants to play a game of guessing with either of the robots and evaluated how the gaze cue affected participants\u2019 task performance. We found that the gaze cue did, in fact, lead to better performance, from which we infer that the cue led to attributions of mental states and intentionality. Our results have implications for robot design, particularly for designing expression of intentionality, and for our understanding of how people respond to human social cues when they are enacted by robots.',authors:"Mutlu, Bilge; Yamaoka, Fumitaka; Kanda, Takayuki; Ishiguro, Hiroshi; Hagita, Norihiro",venue:"ACM",year:2009,link:"https://doi.org/10.1145/1514095.1514110",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"World-Focused",parent:"Attention"}]},{UID:60,citations:[],selected:!0,slug:"paper_60",title:"Visualizing Robot Intent for Object Handovers with Augmented Reality",abstract:"Humans are highly skilled in communicating their intent for when and where a handover would occur. However, even the state-of-the-art robotic implementations for handovers display a general lack of communication skills. This study aims to visualize the internal state and intent of robots for Human-to-Robot Handovers using Augmented Reality. Specifically, we aim to visualize 3D models of the object and the robotic gripper to communicate the robot's estimation of where the object is and the pose in which the robot intends to grasp the object. We tested this design via a user study with 16 participants, in which each participant handed over a cube-shaped object to the robot 12 times. Results show that visualizing robot intent using augmented reality substantially improves the subjective experience of the users for handovers. Results also indicate that the effectiveness of augmented reality is even more pronounced for the perceived safety and fluency of the interaction when the robot makes errors in localizing the object.",authors:"Newbury, Rhys; Cosgun, Akansel; Crowley-Davis, Tysha; Chan, Wesley P.; Drummond, Tom; Croft, Elizabeth",venue:"arXiv",year:2021,link:"https://doi.org/10.48550/arXiv.2103.04055",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:61,citations:[],selected:!0,slug:"paper_61",title:"Bio-inspired multi-robot communication through behavior recognition",abstract:"This paper focuses on enabling multi-robot teams to cooperatively perform tasks without the use of radio or acoustic communication. One key to more effective cooperative interaction in a multi-robot team is the ability to understand the behavior and intent of other robots. This is similar to the honey bee \u201cwaggle dance\u201d in which a bee can communicate the orientation and distance of a food source. In this similar manner, our heterogenous multi-robot team uses a specific behavior to indicate the location of mine-like objects (MLOs). Observed teammate action sequences can be learned to perform behavior recognition and task-assignment in the absence of communication. We apply Conditional Random Fields (CRFs) to perform behavior recognition as an approach to task monitoring in the absence of communication in a challenging underwater environment. In order to demonstrate the use of behavior recognition of an Autonomous Underwater Vehicle (AUV) in a cooperative task, we use trajectory based techniques for model generation and behavior discrimination in experiments using simulated scenario data. Results are presented demonstrating heterogenous teammate cooperation between an AUV and an Autonomous Surface Vehicle (ASV) using behavior recognition rather than radio or acoustic communication in a mine clearing task.",authors:"Novitzky, Michael; Pippin, Charles; Collins, Thomas R.; Balch, Tucker R.; West, Michael E.",venue:"IEEE",year:2012,link:"https://doi.org/10.1109/ROBIO.2012.6491061",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:62,citations:[],selected:!0,slug:"paper_62",title:"A Drink-Serving Mobile Social Robot Selects Who to Interact with Using Gaze",abstract:'Robots will soon deliver food and beverages in various environments. These robots will need to communicate their intention efficiently; for example, they should indicate who they are addressing. We conducted a real-world study of a water serving robot at a university cafeteria. The robot was operated in a Wizard-of-Oz manner. It approached and offered water to students having their lunch. Our analyses of the relationship between robot gaze direction and the likelihood that someone takes a drink show that if people do not already have a drink and the interaction is not dominated by an overly enthusiastic user, the robot\u2019s gaze behavior is effective in selecting an interaction partner even "in the wild".',authors:"Palinko, Oskar; Fischer, Kerstin; Ruiz Ramirez, Eduardo; Damsgaard Nissen, Lotte; Langedijk, Rosalyn M.",venue:"ACM",year:2020,link:"https://doi.org/10.1145/3371382.3378339",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"}]},{UID:63,citations:[],selected:!0,slug:"paper_63",title:"Fostering short-term human anticipatory behavior in human-robot collaboration",abstract:"The present study reports on a human-robot collaboration experiment involving an industrial task with the specific aim of exploring the effects of (i) fostering human anticipatory behavior towards the robot, through visual cues of the robot\u2019s next move and (ii) robot adaptiveness to the human actions through reducing its motion speed with respect to human movement\u2019s proximity. For investigating these effects a generic collaborative picking and sorting task was designed, implemented and tested by volunteer participants, in a Virtual Reality simulation environment. Results demonstrated that, showing robot\u2019s intent through anticipatory cues significantly increased team efficiency, human safety and collaborative fluency in conjunction with a positive subjective inclination towards the robot. Robot adaptiveness significantly increased human safety without decreasing task efficiency and fluency, compared to a control condition.",authors:"Psarakis, Loizos; Nathanael, Dimitris; Marmaras, Nicolas",venue:"ScienceDirect",year:2022,link:"https://doi.org/10.1016/j.ergon.2021.103241",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"World Actions",parent:"Motion"},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:64,citations:[],selected:!0,slug:"paper_64",title:"Communicating and controlling robot arm motion intent through mixed-reality head-mounted displays",abstract:"Efficient motion intent communication is necessary for safe and collaborative work environments with co-located humans and robots. Humans efficiently communicate their motion intent to other humans through gestures, gaze, and other non-verbal cues, and can replan their motions in response. However, robots often have difficulty using these methods. Many existing methods for robot motion intent communication rely on 2D displays, which require the human to continually pause their work to check a visualization. We propose a mixed-reality head-mounted display (HMD) visualization of the intended robot motion over the wearer\u2019s real-world view of the robot and its environment. In addition, our interface allows users to adjust the intended goal pose of the end effector using hand gestures. We describe its implementation, which connects a ROS-enabled robot to the HoloLens using ROS Reality, using MoveIt for motion planning, and using Unity to render the visualization. To evaluate the effectiveness of this system against a 2D display visualization and against no visualization, we asked 32 participants to label various arm trajectories as either colliding or non-colliding with blocks arranged on a table. We found a 15% increase in accuracy with a 38% decrease in the time it took to complete the task compared with the next best system. These results demonstrate that a mixed-reality HMD allows a human to determine where the robot is going to move more quickly and accurately than existing baselines.",authors:"Rosen, Eric; Whitney, David; Phillips, Elizabeth; Chien, Gary; Tompkin, James; Konidaris, George; Tellex, Stefanie",venue:"SAGE Publications",year:2019,link:"https://doi.org/10.1177/0278364919842925",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:65,citations:[],selected:!0,slug:"paper_65",title:"Third point of view augmented reality for robot intentions visualization",abstract:"Lightweight, head-up displays integrated in industrial helmets allow to provide contextual information for industrial scenarios such as in maintenance. Moving from single display and single camera solutions to stereo perception and display opens new interaction possibilities. In particular this paper addresses the case of information sharing by a Baxter robot displayed to the user overlooking at the real scene. System design and interaction ideas are being presented.",authors:"Ruffaldi, Emanuele; Brizzi, Filippo; Tecchia, Franco; Bacinelli, Sandro",venue:"Springer",year:2016,link:"https://doi.org/10.1007/978-3-319-40621-3_35",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot World Perception",parent:"State"}]},{UID:66,citations:[],selected:!0,slug:"paper_66",title:"Communicating affect via flight path: exploring use of the laban effort system for designing affective locomotion paths",abstract:"People and animals use various kinds of motion in a multitude of ways to communicate their ideas and affective state, such as their moods or emotions. Further, people attribute affect and personalities to movements of even non-life like entities based solely on the style of their motions, e.g., the locomotion style of a geometric shape (how it moves about) can be interpreted as being shy, aggressive, etc. We investigate how robots can leverage this locomotion-style communication channel for communication with people. Specifically, our work deals with designing stylistic flying-robot locomotion paths for communicating affective state. To author and unpack the parameters of affect-oriented flying-robot locomotion styles we employ the Laban Effort System, a standard method for interpreting human motion commonly used in the performing arts. This paper describes our adaption of the Laban Effort System to author motions for flying robots, and the results of a formal experiment that investigated how various Laban Effort System parameters influence people's perception of the resulting robotic motions. We summarize with a set of guidelines for aiding designers in using the Laban Effort System to author flying robot motions to elicit desired affective responses.",authors:"Sharma, Megha; Hildebrandt, Dale; Newman, Gem; Young, James E.; Eskicioglu, Rasit",venue:"IEEE",year:2013,link:"https://doi.org/10.1109/HRI.2013.6483602",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:67,citations:[],selected:!0,slug:"paper_67",title:"Effect of Expressive Lights on Human Perception and Interpretation of Functional Robot",abstract:"Because appearance-constrained robots lack expressiveness, human users often find it hard to understand their behavior and intentions. To address this, expressive lights are considered to be an effective means for such robots to communicate with people. However, existing studies mainly focus on specific tasks or goals, leaving the knowledge of how expressive lights affect people\u2019s perception still unknown. In this pilot study, we investigate such a question by using a Roomba robot. We designed two light expressions, namely, green and low-intensity (GL) and red and high-intensity (RH). We used open-ended questions to evaluate people\u2019s perception and interpretation of the robot, which showed different light expressions as a way to communicate. Our findings reveal that simple light expressions can allow people to construct rich and complex interpretations of a robot\u2019s behavior, and such interpretations are heavily biased by the design of expressive lights.",authors:"Song, Sichao; Yamada, Seiji",venue:"ACM",year:2018,link:"https://doi.org/10.1145/3170427.3188547",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:68,citations:[],selected:!0,slug:"paper_68",title:"Designing LED Lights for Communicating Gaze with Appearance-Constrained Robots",abstract:"Functional robots are generally restricted in appearance, thus lacking ways to express their intent. In human-human interaction, gaze is an important cue for providing information and regulating interaction. In this pilot study, we investigate how we can implement gaze behavior in functional robots since gaze communication can allow humans to read a robot's intent and adjust their behavior accordingly. We explore design principles based on LED lights as we consider LEDs to be easily installed in most robots while not introducing features that are too human-like (to prevent users from having high expectations). In the paper, we present a design interface that allows designers to explore the parameter space of an LED strip attached to a Roomba robot. We then summarize a set of design principles for optimally simulating light-based gazes. Finally, our suggested design is evaluated by a large group of participants, and their comments are discussed.",authors:"Song, Sichao; Yamada, Seiji",venue:"IEEE",year:2018,link:"https://doi.org/10.1109/ROMAN.2018.8525661",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"World-Focused",parent:"Attention"}]},{UID:69,citations:[],selected:!0,slug:"paper_69",title:"Bioluminescence-Inspired Human-Robot Interaction: Designing Expressive Lights That Affect Human\u2019s Willingness to Interact with a Robot",abstract:"Bioluminescence is the production and emission of light by a living organism. It, as a means of communication, is of importance for the survival of various creatures. Inspired by bioluminescent light behaviors, we explore the design of expressive lights and evaluate the effect of such expressions on a human\xbbs perception of and attitude toward an appearance-constrained robot. Such robots are in urgent need of finding effective ways to present themselves and communicate their intentions due to a lack of social expressivity. We particularly focus on the expression of attractiveness and hostility because a robot would need to be able to attract or keep away human users in practical human-robot interaction (HRI) scenarios. In this work, we installed an LED lighting system on a Roomba robot and conducted a series of two experiments. We first worked through a structured approach to determine the best light expression designs for the robot to show attractiveness and hostility. This resulted in four recommended light expressions. Further, we performed a verification study to examine the effectiveness of such light expressions in a typical HRI context. On the basis of the findings, we offer design guidelines for expressive lights that HRI researchers and practitioners could readily employ.",authors:"Song, Sichao; Yamada, Seiji",venue:"ACM",year:2018,link:"https://doi.org/10.1145/3171221.3171249",tags:[{name:"Intent Type",parent:null},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:70,citations:[],selected:!0,slug:"paper_70",title:"Visual Attention in Spoken Human-Robot Interaction",abstract:'Psycholinguistic studies of situated language processing have revealed that gaze in the visual environment is tightly coupled with both spoken language comprehension and production. It has also been established that interlocutors monitor the gaze of their partners, a phenomenon called "joint attention", as a further means for facilitating mutual understanding. We hypothesise that human-robot interaction will benefit when the robot\u2019s language-related gaze behaviour is similar to that of people, potentially providing the user with valuable non-verbal information concerning the robot\u2019s intended message or the robot\u2019s successful understanding. We report findings from two eye-tracking experiments demonstrating (1) that human gaze is modulated by both the robot speech and gaze, and (2) that human comprehension of robot speech is improved when the robot\u2019s real-time gaze behaviour is similar to that of humans.',authors:"Staudte, Maria; Crocker, Matthew W.",venue:"ACM",year:2009,link:"https://doi.org/10.1145/1514095.1514111",tags:[{name:"Intent Type",parent:null},{name:"Attention",parent:"Intent Type"},{name:"World-Focused",parent:"Attention"}]},{UID:71,citations:[],selected:!0,slug:"paper_71",title:"Communication of Intent in Assistive Free Flyers",abstract:"Assistive free-flyers (AFFs) are an emerging robotic platform with unparalleled flight capabilities that appear uniquely suited to exploration, surveillance, inspection, and telepresence tasks. However, unconstrained aerial movements may make it difficult for colocated operators, collaborators, and observers to understand AFF intentions, potentially leading to difficulties understanding whether operator instructions are being executed properly or to safety concerns if future AFF motions are unknown or difficult to predict. To increase AFF usability when working in close proximity to users, we explore the design of natural and intuitive flight motions that may improve AFF abilities to communicate intent while simultaneously accomplishing task goals. We propose a formalism for representing AFF flight paths as a series of motion primitives and present two studies examining the effects of modifying the trajectories and velocities of these flight primitives based on natural motion principles. Our first study found that modified flight motions might allow AFFs to more effectively communicate intent and, in our second study, participants preferred interacting with an AFF that used a manipulated flight path, rated modified flight motions as more natural, and felt safer around an AFF with modified motion. Our proposed formalism and findings highlight the importance of robot motion in achieving effective human-robot interactions.",authors:"Szafir, Daniel; Mutlu, Bilge; Fong, Terrence",venue:"ACM",year:2014,link:"https://doi.org/10.1145/2559636.2559672",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:72,citations:[],selected:!0,slug:"paper_72",title:"Communicating Directionality in Flying Robots",abstract:"Small flying robots represent a rapidly emerging family of robotic technologies with aerial capabilities that enable unique forms of assistance in a variety of collaborative tasks. Such tasks will necessitate interaction with humans in close proximity, requiring that designers consider human perceptions regarding robots flying and acting within human environments. We explore the design space regarding explicit robot communication of flight intentions to nearby viewers. We apply design constraints to robot flight behaviors, using biological and airplane flight as inspiration, and develop a set of signaling mechanisms for visually communicating directionality while operating under such constraints. We implement our designs on two commercial flyers, requiring little modification to the base platforms, and evaluate each signaling mechanism, as well as a no-signaling baseline, in a user study in which participants were asked to predict robot intent. We found that three of our designs significantly improved viewer response time and accuracy over the baseline and that the form of the signal offered tradeoffs in precision, generalizability, and perceived robot usability.",authors:"Szafir, Daniel; Mutlu, Bilge; Fong, Terry",venue:"ACM",year:2015,link:"https://doi.org/10.1145/2696454.2696475",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:73,citations:[],selected:!0,slug:"paper_73",title:"Expressing thought: Improving robot readability with animation principles",abstract:"The animation techniques of anticipation and reaction can help create robot behaviors that are human readable such that people can figure out what the robot is doing, reasonably predict what the robot will do next, and ultimately interact with the robot in an effective way. By showing forethought before action and expressing a reaction to the task outcome (success or failure), we prototyped a set of human-robot interaction behaviors. In a 2 (forethought vs. none: between) \xd7 2 (reaction to outcome vs. none: between) \xd7 2 (success vs. failure task outcome: within) experiment, we tested the influences of forethought and reaction upon people's perceptions of the robot and the robot's readability. In this online video prototype experiment (N=273), we have found support for the hypothesis that perceptions of robots are influenced by robots showing forethought, the task outcome (success or failure), and showing goal-oriented reactions to those task outcomes. Implications for theory and design are discussed.",authors:"Takayama, Leila; Dooley, Doug; Ju, Wendy",venue:"IEEE",year:2011,link:"https://doi.org/10.1145/1957656.1957674",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:74,citations:[],selected:!0,slug:"paper_74",title:"The development and evaluation of Robot Light Skin: a novel robot signalling system to improve communication in industrial human\u2013robot collaboration",abstract:"In a human\u2013robot collaborative production system, the robot could make request for interaction or notify the human operator if an uncertainty arises. Conventional industrial tower lights were designed for generic machine signalling purposes which may not be the ultimate solution for robot signalling in a collaborative setting. In this type of system, human operators could be monitoring multiple robots while carrying out a manual task so it is important to minimise the diversion of their attention. This paper presents a novel robot signalling solution, the Robot Light Skin (RLS),which is an integrated signalling system that could be used on most articulated robots. Our experiment was conducted to validate this concept in terms of its effect on improving operator's reaction time, hit-rate, awareness and task performance. The results showed that participants reacted faster to the RLS as well as achieved higher hit-rate. An eye tracker was used in the experiment which shows a reduction in diversion away from the manual task when using the RLS. Future study should explore the effect of the RLS concept on large-scale systems and multi-robot systems.",authors:"Tang, Gilbert; Webb, Phil; Thrower, John",venue:"ScienceDirect",year:2019,link:"https://doi.org/10.1016/j.rcim.2018.08.005",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"},{name:"Instruction",parent:"Intent Type"},{name:"Robot-Centered",parent:"Instruction"}]},{UID:75,citations:[],selected:!0,slug:"paper_75",title:"Intuitive and Safe Interaction in Multi-User Human Robot Collaboration Environments through Augmented Reality Displays",abstract:"As autonomous collaborative robots are more widely used in work environments alongside humans it is of great importance to facilitate the communication between people and robotic systems, in a way that promotes safety and productivity. To this end, we propose an Augmented Reality (AR) based system that allows workers in a human-robot collaborative environment to interact with a robot while also receiving information regarding the robot state and plans that relate to the human\u2019s safety and trust, such as the intended movement of the robotic arm or the navigation plan of the mobile platform. To evaluate the effectiveness of the proposed system we conducted experiments with 13 participants, where two users had to work in the same workspace while being assisted by a mobile manipulator. We measured the task completion time as well as the robot idle time using our AR-based human-robot interaction system and compared them to a conventional setup without the use of augmented reality. Additional, subjective evaluations related to user satisfaction, system usability, perceived safety and trust showed that users assessed the system in a positive way and preferred AR visualization over more traditional interfaces.",authors:"Tsamis, Georgios; Chantziaras, Georgios; Giakoumis, Dimitrios; Kostavelis, Ioannis; Kargakos, Andreas; Tsakiris, Athanasios; Tzovaras, Dimitrios",venue:"IEEE",year:2021,link:"https://doi.org/10.1109/RO-MAN50785.2021.9515474",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:76,citations:[],selected:!0,slug:"paper_76",title:"Communicating Robot Motion Intent with Augmented Reality",abstract:"Humans coordinate teamwork by conveying intent through social cues, such as gestures and gaze behaviors. However, these methods may not be possible for appearance-constrained robots that lack anthropomorphic or zoomorphic features, such as aerial robots. We explore a new design space for communicating robot motion intent by investigating how augmented reality (AR) might mediate human-robot interactions. We develop a series of explicit and implicit designs for visually signaling robot motion intent using AR, which we evaluate in a user study. We found that several of our AR designs significantly improved objective task efficiency over a baseline in which users only received physically-embodied orientation cues. In addition, our designs offer several trade-offs in terms of intent clarity and user perceptions of the robot as a teammate.",authors:"Walker, Michael; Hedayati, Hooman; Lee, Jennifer; Szafir, Daniel",venue:"ACM",year:2018,link:"https://doi.org/10.1145/3171221.3171253",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:77,citations:[],selected:!0,slug:"paper_77",title:"Communicating robotic navigational intentions",abstract:"This paper presents a study on intention communication in a navigational context using a robotic wheelchair. The robotic wheelchair uses light projection to communicate its motion intentions. The novelty of the work is threefold: the communication of robot intentions to the passenger, the consideration of passenger and robot as a group (\u201cin-group\u201d) [1] who share motion intentions and the communication of the in-group intentions to other pedestrians (the \u201cout-group\u201d). A comparison in an autonomous navigation task where the robotic wheelchair autonomously navigates the environment with and without intention communication was performed showing that passengers and walking people found intention communication intuitive and helpful for passing by actions. Evaluation results significantly show human participant preference for having navigational intention communication for the wheelchair passenger and the person passing by it. Quantitative results show the motion of the person passing by the wheelchair with intention communication was significantly smoother compared to without intention communication.",authors:"Watanabe, Atsushi; Ikeda, Tetsushi; Morales, Yoichi; Shinozawa, Kazuhiko; Miyashita, Takahiro; Hagita, Norihiro",venue:"IEEE",year:2015,link:"https://doi.org/10.1109/IROS.2015.7354195",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:78,citations:[],selected:!0,slug:"paper_78",title:"A Laser Projection System for Robot Intention Communication and Human Robot Interaction",abstract:"In order to deploy service robots in environments where they encounter and/or cooperate with persons, one important key factor is human acceptance. Hence, information on which upcoming actions of the robot are based has to be made transparent and understandable to the human. However, considering the restricted power resources of mobile robot platforms, systems for visualization not only have to be expressive but also energy efficient. In this paper, we applied the well-known technique of laser scanning on a mobile robot to create a novel system for intention visualization and human-robot-interaction. We conducted user tests to compare our system to a low-power consuming LED video projector solution in order to evaluate the suitability for mobile platforms and to get human impressions of both systems. We can show that the presented system is preferred by most users in a dynamic test setup on a mobile platform.",authors:"Wengefeld, Tim; Hochemer, Dominik; Lewandowski, Benjamin; Kohler, Mona; Beer, Manuel; Gross, Horst-Michael",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/RO-MAN47096.2020.9223517",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"},{name:"Robot World Perception",parent:"State"},{name:"Instruction",parent:"Intent Type"},{name:"World-Centered",parent:"Instruction"}]},{UID:79,citations:[],selected:!0,slug:"paper_79",title:"Robot Gesture Sonification to Enhance Awareness of Robot Status and Enjoyment of Interaction",abstract:"We present a divergent approach to robotic sonification with the goal of improving the quality and safety of human-robot interactions. Sonification (turning data into sound) has been underutilized in robotics, and has broad potential to convey robotic movement and intentions to users without requiring visual engagement. We design and evaluate six different sonifications of movements for a robot with four degrees of freedom. Our sonification techniques include a direct mapping from each degree of freedom to pitch and timbre changes, emotion-based sound mappings, and velocity-based mappings using different types of sounds such as motors and music. We evaluate these sonifications using metrics for ease of use, enjoyment/appeal, and conveyance of movement information. Based on our results, we make recommendations to inform decisions for future robot sonification design. We suggest that when using sonification to improve safety of human-robot collaboration, it is necessary not only to convey sufficient information about movements, but also to convey that information in a pleasing and even social way to to enhance the human-robot relationship.",authors:"Zahray, Lisa; Savery, Richard; Syrkett, Liana; Weinberg, Gil",venue:"IEEE",year:2020,link:"https://doi.org/10.1109/RO-MAN47096.2020.9223452",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"}]},{UID:80,citations:[],selected:!0,slug:"paper_80",title:"Expressive robot motion timing",abstract:"Our goal is to enable robots to time their motion in a way that is purposefully expressive of their internal states, making them more transparent to people. We start by investigating what types of states motion timing is capable of expressing, focusing on robot manipulation and keeping the path constant while systematically varying the timing. We find that users naturally pick up on certain properties of the robot (like confidence), of the motion (like naturalness), or of the task (like the weight of the object that the robot is carrying). We then conduct a hypothesis-driven experiment to tease out the directions and magnitudes of these effects, and use our findings to develop candidate mathematical models for how users make these inferences from the timing. We find a strong correlation between the models and real user data, suggesting that robots can leverage these models to autonomously optimize the timing of their motion to be expressive.",authors:"Zhou, Allan; Hadfield-Menell, Dylan; Nagabandi, Anusha; Dragan, Anca D.",venue:"ACM",year:2017,link:"https://doi.org/10.1145/2909824.3020221",tags:[{name:"Intent Type",parent:null},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"}]},{UID:81,citations:[],selected:!0,slug:"paper_81",title:"Towards Explainable Shared Control using Augmented Reality",abstract:"Shared control plays a pivotal role in establishing effective human-robot interactions. Traditional control-sharing methods strive to complement a human's capabilities at safely completing a task, and thereby rely on users forming a mental model of the expected robot behaviour. However, these methods can often bewilder or frustrate users whenever their actions do not elicit the intended system response, forming a misalignment between the respective internal models of the robot and human. To resolve this model misalignment, we introduce Explainable Shared Control as a paradigm in which assistance and information feedback are jointly considered. Augmented reality is presented as an integral component of this paradigm, by visually unveiling the robot's inner workings to human operators. Explainable Shared Control is instantiated and tested for assistive navigation in a setup involving a robotic wheelchair and a Microsoft HoloLens with add-on eye tracking. Experimental results indicate that the introduced paradigm facilitates transparent assistance by improving recovery times from adverse events associated with model misalignment.",authors:"Zolotas, Mark; Demiris, Yiannis",venue:"IEEE",year:2019,link:"https://doi.org/10.1109/IROS40897.2019.8968117",tags:[{name:"Intent Type",parent:null},{name:"Motion",parent:"Intent Type"},{name:"Robot Self-Actions",parent:"Motion"},{name:"State",parent:"Intent Type"},{name:"Robot Self-Perception",parent:"State"},{name:"Robot World Perception",parent:"State"}]}],links:[{source:7,target:8},{source:7,target:72},{source:12,target:59},{source:14,target:72},{source:14,target:71},{source:14,target:66},{source:22,target:18},{source:22,target:37},{source:22,target:64},{source:22,target:66},{source:22,target:71},{source:22,target:76},{source:22,target:77},{source:22,target:77},{source:25,target:55},{source:25,target:52},{source:27,target:37},{source:27,target:73},{source:28,target:27},{source:28,target:37},{source:28,target:73},{source:28,target:27},{source:28,target:37},{source:36,target:21},{source:36,target:21},{source:39,target:17},{source:41,target:5},{source:41,target:17},{source:41,target:24},{source:41,target:57},{source:41,target:64},{source:41,target:72},{source:41,target:77},{source:41,target:77},{source:42,target:57},{source:43,target:33},{source:43,target:53},{source:43,target:24},{source:46,target:28},{source:46,target:27},{source:46,target:28},{source:46,target:27},{source:48,target:73},{source:50,target:5},{source:60,target:64},{source:63,target:12},{source:63,target:28},{source:63,target:37},{source:68,target:72},{source:68,target:57},{source:68,target:57},{source:67,target:14},{source:71,target:66},{source:71,target:73},{source:72,target:66},{source:72,target:71},{source:72,target:73},{source:76,target:5},{source:76,target:8},{source:76,target:37},{source:76,target:65},{source:76,target:71},{source:76,target:72},{source:76,target:73},{source:76,target:77},{source:76,target:65},{source:76,target:77}]}},402:function(e){e.exports=[{UID:5,pos:[45.90019226074219,11.80744457244873]},{UID:6,pos:[-18.60301971435547,85.84136199951172]},{UID:7,pos:[6.4254608154296875,-125.3141098022461]},{UID:8,pos:[3.247023105621338,-105.02702331542969]},{UID:9,pos:[48.304996490478516,76.95267486572266]},{UID:10,pos:[-1.503680944442749,43.41666030883789]},{UID:11,pos:[29.983654022216797,-28.2624454498291]},{UID:12,pos:[15.326433181762695,93.52660369873047]},{UID:13,pos:[-77.82498168945312,-12.859649658203125]},{UID:14,pos:[-131.37095642089844,40.06039047241211]},{UID:15,pos:[-36.70307159423828,-5.107583045959473]},{UID:16,pos:[-16.609750747680664,-58.38809585571289]},{UID:17,pos:[67.98916625976562,24.6405029296875]},{UID:18,pos:[37.61412811279297,-13.612528800964355]},{UID:19,pos:[-64.63665771484375,7.530231952667236]},{UID:20,pos:[-53.51640319824219,15.994963645935059]},{UID:21,pos:[-24.44536590576172,13.883286476135254]},{UID:22,pos:[65.67799377441406,-36.18730163574219]},{UID:23,pos:[7.373219013214111,-68.68936157226562]},{UID:24,pos:[81.1750717163086,-58.95478439331055]},{UID:25,pos:[1.1881678104400635,120.90461730957031]},{UID:26,pos:[-25.037948608398438,42.165550231933594]},{UID:27,pos:[52.3983154296875,55.70845031738281]},{UID:28,pos:[14.674687385559082,54.56484603881836]},{UID:29,pos:[-125.4712142944336,-6.549402713775635]},{UID:30,pos:[-43.13970947265625,39.00941467285156]},{UID:31,pos:[2.040534019470215,67.08483123779297]},{UID:32,pos:[-15.371975898742676,62.01467514038086]},{UID:33,pos:[-34.96826171875,-48.58651351928711]},{UID:34,pos:[-150.34835815429688,2.4458391666412354]},{UID:35,pos:[-73.8797836303711,45.39719009399414]},{UID:36,pos:[-59.59072494506836,80.76887512207031]},{UID:37,pos:[31.470279693603516,55.87773513793945]},{UID:38,pos:[-40.07778549194336,59.47133255004883]},{UID:39,pos:[68.2674331665039,-17.814035415649414]},{UID:40,pos:[85.09355163574219,-41.105499267578125]},{UID:41,pos:[5.79752779006958,-34.4417839050293]},{UID:42,pos:[-15.250393867492676,-2.7057266235351562]},{UID:43,pos:[73.05728149414062,-75.10035705566406]},{UID:44,pos:[22.326583862304688,9.76297664642334]},{UID:45,pos:[-102.40913391113281,70.37290954589844]},{UID:46,pos:[38.398216247558594,103.51252746582031]},{UID:47,pos:[-47.26205062866211,-27.18473243713379]},{UID:48,pos:[-75.70504760742188,78.38201904296875]},{UID:49,pos:[-11.208576202392578,28.829654693603516]},{UID:50,pos:[3.1167449951171875,13.677165985107422]},{UID:51,pos:[-95.16407775878906,11.207165718078613]},{UID:52,pos:[141.6609344482422,-61.036041259765625]},{UID:53,pos:[133.6681365966797,-27.24371337890625]},{UID:54,pos:[125.79167175292969,-42.65171432495117]},{UID:55,pos:[120.6871566772461,-56.176429748535156]},{UID:56,pos:[107.12606811523438,-82.17806243896484]},{UID:57,pos:[-51.96857452392578,-45.47879409790039]},{UID:58,pos:[16.966386795043945,-12.139812469482422]},{UID:59,pos:[-31.156978607177734,-28.033008575439453]},{UID:60,pos:[45.299278259277344,-47.328514099121094]},{UID:61,pos:[-101.91929626464844,-11.868230819702148]},{UID:62,pos:[-78.13699340820312,122.27196502685547]},{UID:63,pos:[11.411088943481445,31.728281021118164]},{UID:64,pos:[62.01409149169922,-.5698369741439819]},{UID:65,pos:[86.66459655761719,-14.64435863494873]},{UID:66,pos:[-106.309326171875,35.2706413269043]},{UID:67,pos:[-11.566534996032715,-107.13087463378906]},{UID:68,pos:[-24.762531280517578,-85.68598937988281]},{UID:69,pos:[-25.349294662475586,-114.95891571044922]},{UID:70,pos:[-53.7142219543457,-65.09370422363281]},{UID:71,pos:[-115.45210266113281,14.087030410766602]},{UID:72,pos:[-130.48220825195312,12.619328498840332]},{UID:73,pos:[-78.41954803466797,-67.92549133300781]},{UID:74,pos:[-40.81128692626953,106.38102722167969]},{UID:75,pos:[46.84365463256836,-28.639781951904297]},{UID:76,pos:[-1.680828332901001,-13.704047203063965]},{UID:77,pos:[42.138797760009766,-91.70773315429688]},{UID:78,pos:[83.11547088623047,12.544416427612305]},{UID:79,pos:[32.828041076660156,33.807411193847656]},{UID:80,pos:[28.268407821655273,72.75281524658203]},{UID:81,pos:[25.45851707458496,-47.85636520385742]}]},449:function(e,t,n){"use strict";n.r(t);var a=n(0),o=n.n(a),i=n(236),r=n.n(i),s=(n(342),n(25)),l=n(26),c=n(28),h=n(29),m=(n(343),n(250)),d=n(58),p=n(237),u=n(185),g=n(8),f=n(14),b=n(12),y=n(20),v=n(13),w=n(480),I=n(481),k=n(485),S=n(489),T=n(51),R=n(452),x=n(483),D=n(484),A=n(466),E=n(118),M=(n(177),n(108)),O={red:60,magenta:50,purple:40,blue:35,cyan:30,teal:25,green:15,gray:10,"cool-gray":5,"warm-gray":0},U={md:50,sm:0};function j(e,t){return[e.name,e.parent].join("-child-of-")}function C(e,t){if(!e)return[];if(1===e.level)return[];var n=e.parent;return t.slice(0,e.level-1).reverse().map(function(e,a){var o=e.filter(function(e){return e.name===n});return n=o[0]?o[0].parent:t[0].name,o}).reverse()}function z(e,t){return e?e.level>=t.length?[]:t[e.level].filter(function(t){return t.parent===e.name}):[]}var _=function(e){return o.a.createElement("p",{className:"paper"},e.paper.title," ",o.a.createElement("em",null,"by ",e.paper.authors),". ",e.paper.venue," (",e.paper.year,")"," ",e.paper.link&&o.a.createElement(R.a,{className:"hover-cursor",href:e.paper.link,target:"_blank"},o.a.createElement("span",{className:"download-icon"},o.a.createElement(g.md,null))))},W=function(e){return o.a.createElement(x.a,{className:"paper-tile"},o.a.createElement(_,{paper:e.paper}))},H=function(e){Object(c.a)(n,e);var t=Object(h.a)(n);function n(e){var a;return Object(s.a)(this,n),(a=t.call(this,e)).state={data:e.data,years:e.years,slide_on:!1,options:{toolbar:{controls:[]},height:"75px",grid:{x:{enabled:!1},y:{enabled:!1}},axes:{bottom:{visible:!1,title:"2019 Annual Sales Figures",mapsTo:"date",scaleType:"time"},left:{visible:!1,mapsTo:"value",scaleType:"linear"}},color:{gradient:{enabled:!0}},points:{enabled:!1},legend:{enabled:!1}}},a}return Object(l.a)(n,[{key:"componentDidUpdate",value:function(e,t){this.props.data!==e.data?this.setState(Object(y.a)(Object(y.a)({},this.state),{},{data:this.props.data,years:this.props.years})):this.props.years!==e.years&&this.setState(Object(y.a)(Object(y.a)({},this.state),{},{years:this.props.years})),this.props.slide_on!==e.slide_on&&this.setState(Object(y.a)(Object(y.a)({},this.state),{},{slide_on:this.props.slide_on}))}},{key:"handleSimulate",value:function(e){this.props.handleSimulate(e.value)}},{key:"getTimeline",value:function(e){var t=(new Date).getFullYear(),n=Object(T.a)(Array(t+1-M.min_year).keys()).map(function(e){return e+M.min_year}),a={},o=this.state.data;return n.forEach(function(e){a[e]=0}),o&&o.forEach(function(e){a[parseInt(e.year)]+=1}),a=Object.keys(a).map(function(e){return{group:1,date:e,value:a[e]}})}},{key:"render",value:function(){return o.a.createElement(o.a.Fragment,null,o.a.createElement("div",{className:"bx--col-lg-6"},o.a.createElement(D.a,{labelText:M.metadata.acronym+" through the years",hideTextInput:!0,onChange:this.handleSimulate.bind(this),min:this.state.years.min_val,max:this.state.years.max_val,value:this.state.years.cur_val,step:1})),o.a.createElement("div",{className:"bx--col-lg-6 "+(!this.state.slide_on&&" display_none")},o.a.createElement(E.AreaChart,{data:this.getTimeline(),options:this.state.options})))}}]),n}(o.a.Component),L=function(e){Object(c.a)(n,e);var t=Object(h.a)(n);function n(e){var a;return Object(s.a)(this,n),(a=t.call(this,e)).state={data:e.data,selected_tags:[],filter_tags:[]},a}return Object(l.a)(n,[{key:"componentDidMount",value:function(e){this.updateTagNumbers()}},{key:"updateTagSelection",value:function(e){this.props.updateTagSelection(this.state.filter_tags)}},{key:"componentDidUpdate",value:function(e,t){var n=this;this.props.data!==e.data&&this.setState({data:this.props.data},function(){n.updateTagNumbers()})}},{key:"determineTagSize",value:function(e){var t=this,n=Object.keys(this.state.selected_tags).reduce(function(e,n){return e+t.state.selected_tags[n]},0),a=this.state.selected_tags[e]>n/Object.keys(this.state.selected_tags).length,o=0,i="sm";return a&&(o=50),Object.keys(U).some(function(e){return o>=U[e]&&(i=e,!0)}),i}},{key:"determineTagColor",value:function(e){var t=this,n=Object.keys(this.state.selected_tags).reduce(function(e,n){return e+t.state.selected_tags[n]},0),a=500*this.state.selected_tags[e]/n,o="sm";return Object.keys(O).some(function(e){return a>=O[e]&&(o=e,!0)}),o}},{key:"onClickTag",value:function(e){var t=this,n=this.state.filter_tags;-1===n.indexOf(e)&&(n.push(e),this.setState(Object(y.a)(Object(y.a)({},this.state),{},{filter_tags:n}),function(){t.updateTagSelection()}))}},{key:"onCloseTag",value:function(e){var t=this,n=this.state.filter_tags,a=n.indexOf(e);a>-1&&(n.splice(a,1),this.setState(Object(y.a)(Object(y.a)({},this.state),{},{filter_tags:n}),function(){t.updateTagSelection()}))}},{key:"updateTagNumbers",value:function(){var e=[],t=this.state.data.reduce(function(e,t){return e=e||t.selected},!1);this.state.data.forEach(function(n){n.tags.forEach(function(a){var o=a.name;n.selected||!t?Object.keys(e).indexOf(o)>-1?e[o]=e[o]+1:o&&(e[o]=1):Object.keys(e).indexOf(o)>-1&&(e[o]=e[o]-1,e[o]||delete e[o])})}),this.setState(Object(y.a)(Object(y.a)({},this.state),{},{selected_tags:e}))}},{key:"objectSort",value:function(e){var t=[];for(var n in e)t.push([n,e[n]]);return t.sort(function(e,t){return t[1]-e[1]}),t.map(function(e){return e[0]})}},{key:"render",value:function(){var e=this;return o.a.createElement(o.a.Fragment,null,this.objectSort(this.state.selected_tags).map(function(t,n){return o.a.createElement(A.a,{filter:e.state.filter_tags.indexOf(t)>-1,onClose:e.onCloseTag.bind(e,t),onClick:e.onClickTag.bind(e,t),key:n,type:e.determineTagColor(t),size:e.determineTagSize(t)},t)}))}}]),n}(o.a.Component),F=n(120),P=n(108),N=n(390),B=n(179).find(function(e){return e.name===P.views.find(function(e){return"Taxonomy"===e.name}).default_tab}).data,q=function(e){Object(c.a)(n,e);var t=Object(h.a)(n);function n(e){var a;return Object(s.a)(this,n),(a=t.call(this,e)).handleMouseDown=function(e){e.evt.shiftKey?a.setState(Object(y.a)(Object(y.a)({},a.state),{},{config:Object(y.a)(Object(y.a)({},a.state.config),{},{is_draggable:!1})}),function(){if(null===e.currentTarget.mouseClickEndShape){e.target.getStage().container().style.cursor="grab";var t=e.target.getStage().getPointerPosition(),n=t.x,o=t.y;a.setState(Object(y.a)(Object(y.a)({},a.state),{},{annotations:[],newAnnotation:[{x:n,y:o,width:0,height:0,key:"0"}]}))}}):a.setState(Object(y.a)(Object(y.a)({},a.state),{},{config:Object(y.a)(Object(y.a)({},a.state.config),{},{is_draggable:!0})}))},a.handleMouseUp=function(e){if(e.target.getStage().container().style.cursor="",a.state.newAnnotation.length){var t=a.state.newAnnotation[0].x,n=a.state.newAnnotation[0].y,o=e.target.getStage().getPointerPosition(),i=o.x,r=o.y;a.setState(Object(y.a)(Object(y.a)({},a.state),{},{annotations:[],newAnnotation:[]}));var s=a.state.cached_data.map(function(e){var a=!1;return a=i>t?e.x>t&&e.x<i:e.x<t&&e.x>i,a=r>n?a&&e.y>n&&e.y<r:a&&e.y<n&&e.y>r,Object(y.a)(Object(y.a)({},e),{},{selected:a})});s.filter(function(e){return e.selected}).length&&a.setState(Object(y.a)(Object(y.a)({},a.state),{},{data:s.filter(function(e){return e.selected}),cached_data:s}),function(){a.updateSelectedTab()})}},a.handleMouseMove=function(e){if(a.state.newAnnotation.length){var t=a.state.newAnnotation[0].x,n=a.state.newAnnotation[0].y,o=e.target.getStage().getPointerPosition(),i={x:t,y:n,width:o.x-t,height:o.y-n,key:"0"};a.setState(Object(y.a)(Object(y.a)({},a.state),{},{annotations:[i],newAnnotation:[i]}))}},a.ref=o.a.createRef(),a.stageRef=o.a.createRef(),a.state={data:B,cached_data:B,hoverText:null,annotations:[],newAnnotation:[],config:{is_draggable:!0,stageHeight:100,stageWidth:100},years:a.props.years},a.updateSelectedTab(),a}return Object(l.a)(n,[{key:"componentDidMount",value:function(e){var t=this,n=this.ref.current.offsetHeight,a=this.ref.current.offsetWidth,o=Math.min.apply(Math,Object(T.a)(N.map(function(e){return e.pos[0]}))),i=Math.max.apply(Math,Object(T.a)(N.map(function(e){return e.pos[0]}))),r=Math.min.apply(Math,Object(T.a)(N.map(function(e){return e.pos[1]}))),s=Math.max.apply(Math,Object(T.a)(N.map(function(e){return e.pos[1]}))),l=N.map(function(e){var t=e;return t.pos[0]=.8*a*(-o+e.pos[0])/(i-o),t.pos[1]=.8*n*(-r+e.pos[1])/(s-r),t});this.lastCenter=null,this.lastDist=0;var c=B.map(function(e){var n=l.filter(function(t){return t.UID===e.UID})[0],a=e;return a.x=25+t.applyScalingX(1,n.pos[0]),a.y=25+t.applyScalingY(1,n.pos[1]),a.selected=!1,a});this.ref.current&&this.setState(Object(y.a)(Object(y.a)({},this.state),{},{data:c,cached_data:c,config:Object(y.a)(Object(y.a)({},this.state.config),{},{stageHeight:this.ref.current.offsetHeight,stageWidth:this.ref.current.offsetWidth})}))}},{key:"componentDidUpdate",value:function(e,t){this.props.props!==e.props&&this.setState(Object(y.a)(Object(y.a)({},this.state),{},{data:this.props.props,years:this.props.years}))}},{key:"getDistance",value:function(e,t){return Math.sqrt(Math.pow(t.x-e.x,2)+Math.pow(t.y-e.y,2))}},{key:"getCenter",value:function(e,t){return{x:(e.x+t.x)/2,y:(e.y+t.y)/2}}},{key:"applyScalingX",value:function(e,t){return e*t}},{key:"applyScalingY",value:function(e,t){return e*t}},{key:"updateSelectedTab",value:function(e){this.props.updateSelectedTab(this.state.data,[])}},{key:"onNodeHover",value:function(e,t){var n=this.state.cached_data.find(function(t){return t.UID===e}),a=t.target.getStage().container();this.state.newAnnotation.length||(a.style.cursor="pointer"),this.setState(Object(y.a)(Object(y.a)({},this.state),{},{hoverText:o.a.createElement(_,{paper:n})}))}},{key:"offNodeHover",value:function(e){var t=e.target.getStage().container();this.state.newAnnotation.length||(t.style.cursor=""),this.setState(Object(y.a)(Object(y.a)({},this.state),{},{hoverText:null}))}},{key:"updateTagSelection",value:function(e){var t=this,n=this.state.cached_data.filter(function(t){return e.filter(function(e){return new Set(t.tags.map(function(e){return e.name})).has(e)}).length>0||0===e.length});this.setState(Object(y.a)(Object(y.a)({},this.state),{},{data:n}),function(){t.updateSelectedTab()})}},{key:"selectNode",value:function(e){var t=this,n=this.state.cached_data.map(function(t){return t.UID===e&&(t.selected=!t.selected),t});this.state.data.reduce(function(e,t){return e=e||t.selected},!1)?this.setState(Object(y.a)(Object(y.a)({},this.state),{},{data:n.filter(function(e){return e.selected})}),function(){t.updateSelectedTab()}):this.setState(Object(y.a)(Object(y.a)({},this.state),{},{data:this.state.cached_data}),function(){t.updateSelectedTab()})}},{key:"zoomStage",value:function(e){if(e.evt.preventDefault(),this.stageRef.current){var t=this.stageRef.current,n=t.scaleX(),a=t.getPointerPosition(),o=a.x,i=a.y,r={x:(o-t.x())/n,y:(i-t.y())/n},s=e.evt.deltaY>0?1.01*n:n/1.01;t.scale({x:s,y:s});var l={x:o-r.x*s,y:i-r.y*s};t.position(l),t.batchDraw()}}},{key:"handleTouch",value:function(e){e.evt.preventDefault();var t=e.evt.touches[0],n=e.evt.touches[1],a=this.stageRef.current;if(null!==a&&t&&n){a.isDragging()&&a.stopDrag();var o={x:t.clientX,y:t.clientY},i={x:n.clientX,y:n.clientY};if(!this.lastCenter)return void(this.lastCenter=this.getCenter(o,i));var r=this.getCenter(o,i),s=this.getDistance(o,i);this.lastDist||(this.lastDist=s);var l={x:(r.x-a.x())/a.scaleX(),y:(r.y-a.y())/a.scaleX()},c=a.scaleX()*(s/this.lastDist);a.scaleX(c),a.scaleY(c);var h=r.x-this.lastCenter.x,m=r.y-this.lastCenter.y,d={x:r.x-l.x*c+h,y:r.y-l.y*c+m};a.position(d),a.batchDraw(),this.lastDist=s,this.lastCenter=r}}},{key:"handleTouchEnd",value:function(){this.lastCenter=null,this.lastDist=0}},{key:"mouseUpGlobal",value:function(e){this.setState(Object(y.a)(Object(y.a)({},this.state),{},{slide_on:!1}))}},{key:"handleSimulate",value:function(e){var t=this;this.setState(Object(y.a)(Object(y.a)({},this.state),{},{slide_on:!0}),function(){t.props.handleSimulate(e)})}},{key:"render",value:function(){var e=this;return o.a.createElement("div",{onMouseUp:this.mouseUpGlobal.bind(this)},o.a.createElement(k.a,{subtitle:o.a.createElement("span",null,"This view into document similarity space was inspired from"," ",o.a.createElement(R.a,{href:"https://github.com/Mini-Conf/Mini-Conf",target:"_blank"},"Miniconf"),". Each dot is a paper. If you ",o.a.createElement("b",null,"hover")," over a dot, you see the corresponding paper. To learn more about an area of the plot,"," ",o.a.createElement("b",null,"Select")," one or more papers by clicking on the dots or by pressing SHIFT and dragging a rectangle over them."),title:"Document Embeddings",kind:"info",lowContrast:!0}),o.a.createElement("div",{className:"bx--container"},o.a.createElement("div",{className:"bx--row"},o.a.createElement(H,{data:this.state.data,years:this.state.years,handleSimulate:this.handleSimulate.bind(this),slide_on:this.state.slide_on}),o.a.createElement("div",{className:"bx--col-lg-6 "+(this.state.slide_on&&" display_none"),style:{height:"75px"}},o.a.createElement("span",null,this.state.hoverText)))),o.a.createElement("div",{className:"bx--col-lg-16",style:{height:"40vh"},ref:this.ref},o.a.createElement(F.d,{onMouseDown:this.handleMouseDown,onMouseUp:this.handleMouseUp,onMouseMove:this.handleMouseMove,width:this.state.config.stageWidth,height:this.state.config.stageHeight,draggable:this.state.config.is_draggable,onWheel:this.zoomStage.bind(this),onTouchMove:this.handleTouch.bind(this),onTouchEnd:this.handleTouchEnd.bind(this),ref:this.stageRef},o.a.createElement(F.b,null,this.state.annotations.map(function(e,t){return o.a.createElement(F.c,{key:t,x:e.x,y:e.y,width:e.width,height:e.height,fill:"#0f62fe",opacity:.7})}),this.state.cached_data.map(function(t){return o.a.createElement(F.a,{key:t.UID,x:t.x,y:t.y,radius:5,fill:"#e4e4e4",onMouseDown:e.selectNode.bind(e,t.UID),onMouseEnter:e.onNodeHover.bind(e,t.UID),onMouseLeave:e.offNodeHover.bind(e)})}),this.state.data.map(function(t){return o.a.createElement(F.a,{key:t.UID,x:t.x,y:t.y,radius:5,fill:t.selected?"#0f62fe":"#d2e6ff",onMouseDown:e.selectNode.bind(e,t.UID),onMouseEnter:e.onNodeHover.bind(e,t.UID),onMouseLeave:e.offNodeHover.bind(e)})})))),o.a.createElement("div",{className:"bx--container"},o.a.createElement("div",{className:"bx--row"},o.a.createElement("div",{className:"bx--col-lg-12"},o.a.createElement(L,{data:this.state.data,updateTagSelection:this.updateTagSelection.bind(this)})))))}}]),n}(o.a.Component),G=n(56);var K=n(394),V=800,J=600,Y=function(e){Object(c.a)(n,e);var t=Object(h.a)(n);function n(e){var a;return Object(s.a)(this,n),(a=t.call(this,e)).svg=o.a.createRef(),a.state={},a}return Object(l.a)(n,[{key:"componentDidUpdate",value:function(e,t){this.props!==e&&(this.chart=function(e){var t=e.nodes,n=e.links,a=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},o=a.nodeId,i=void 0===o?function(e){return e.UID}:o,r=a.nodeGroup,s=a.nodeGroups,l=a.nodeTitle,c=a.nodeFill,h=void 0===c?"currentColor":c,m=a.nodeStroke,d=void 0===m?"#fff":m,p=a.nodeStrokeWidth,u=void 0===p?1.5:p,g=a.nodeStrokeOpacity,f=void 0===g?1:g,b=a.nodeRadius,y=void 0===b?5:b,v=a.nodeStrength,w=a.linkSource,I=void 0===w?function(e){return e.source}:w,k=a.linkTarget,S=void 0===k?function(e){return e.target}:k,T=a.linkStroke,R=void 0===T?"#999":T,x=a.linkStrokeOpacity,D=void 0===x?.6:x,A=a.linkStrokeWidth,E=void 0===A?1.5:A,M=a.linkStrokeLinecap,O=void 0===M?"round":M,U=a.linkStrength,j=a.colors,C=void 0===j?G.k:j,z=a.width,_=void 0===z?640:z,W=a.height,H=void 0===W?400:W,L=a.invalidation,F=G.i(t,i).map(ae),P=G.i(n,I).map(ae),N=G.i(n,S).map(ae);void 0===l&&(l=function(e,t){return F[t]});var B=null==l?null:G.i(t,l),q=null==r?null:G.i(t,r).map(ae),K="function"!==typeof E?null:G.i(n,E),V="function"!==typeof R?null:G.i(n,R);t=G.i(t,function(e,t){return{id:F[t]}}),n=G.i(n,function(e,t){return{source:P[t],target:N[t]}}),q&&void 0===s&&(s=G.l(q));var J=null==r?null:G.j(s,C),Y=G.e(),Z=G.d(n).id(function(e){var t=e.index;return F[t]});void 0!==v&&Y.strength(v),void 0!==U&&Z.strength(U);var X=G.f(t).force("link",Z).force("charge",Y).force("center",G.c()).on("tick",function(){ee.attr("x1",function(e){return e.source.x}).attr("y1",function(e){return e.source.y}).attr("x2",function(e){return e.target.x}).attr("y2",function(e){return e.target.y}),te.attr("cx",function(e){return e.x}).attr("cy",function(e){return e.y})}).force("x",G.g()).force("y",G.h()),Q=G.a("svg").attr("width",_).attr("height",H).attr("viewBox",[-_/2,-H/2,_,H]).attr("style","max-width: 100%; height: auto; height: intrinsic;"),$=Q.append("rect").attr("width",_).attr("height",H).attr("x",-_/2).attr("y",-H/2).style("fill","none").style("pointer-events","all"),ee=Q.append("g").attr("stroke","function"!==typeof R?R:null).attr("stroke-opacity",D).attr("stroke-width","function"!==typeof E?E:null).attr("stroke-linecap",O).selectAll("line").data(n).join("line"),te=Q.append("g").attr("fill",h).attr("stroke",d).attr("stroke-opacity",f).attr("stroke-width",u).selectAll("circle").data(t).join("circle").attr("r",y).call(function(e){return G.b().on("start",function(t){t.active||e.alphaTarget(.3).restart(),t.subject.fx=t.subject.x,t.subject.fy=t.subject.y}).on("drag",function(e){e.subject.fx=e.x,e.subject.fy=e.y}).on("end",function(t){t.active||e.alphaTarget(0),t.subject.fx=null,t.subject.fy=null})}(X)),ne=G.m().scaleExtent([.5,64]).on("zoom",function(e){te.attr("transform",e.transform),ee.attr("transform",e.transform)});function ae(e){return null!==e&&"object"===typeof e?e.valueOf():e}return $.call(ne).call(ne.translateTo,0,0),K&&ee.attr("stroke-width",function(e){var t=e.index;return K[t]}),V&&ee.attr("stroke",function(e){var t=e.index;return V[t]}),q&&te.attr("fill",function(e){var t=e.index;return J(q[t])}),B&&te.append("title").text(function(e){var t=e.index;return B[t]}),null!=L&&L.then(function(){return X.stop()}),Object.assign(Q.node(),{scales:{color:J}})}(this.props.data,{nodeId:function(e){return e.UID},nodeGroup:function(e){return e.group},nodeTitle:function(e){return"".concat(e.title,". ").concat(e.authors,". ").concat(e.venue,". (").concat(e.year,")")},linkStrokeWidth:function(e){return Math.sqrt(e.value)},width:this.props.width,height:this.props.height}),this.svg.current.innerHTML="",this.svg.current.appendChild(this.chart))}},{key:"render",value:function(){return o.a.createElement("div",{ref:this.svg})}}]),n}(o.a.Component),Z=function(e){Object(c.a)(n,e);var t=Object(h.a)(n);function n(e){var a;return Object(s.a)(this,n),(a=t.call(this,e)).state={node_data:K.nodes,link_data:a.filter_links_by_nodes(K.nodes,K.links),width:V,height:J,years:a.props.years},a.ref=o.a.createRef(),a.updateSelectedTab(),a}return Object(l.a)(n,[{key:"componentDidMount",value:function(e){this.ref.current&&this.setState(Object(y.a)(Object(y.a)({},this.state),{},{width:this.ref.current.offsetWidth,height:this.ref.current.offsetHeight}))}},{key:"componentDidUpdate",value:function(e,t){if(this.props.props!==e.props){var n=this.props.props,a=this.filter_links_by_nodes(n,K.links);this.setState(Object(y.a)(Object(y.a)({},this.state),{},{node_data:n,link_data:a,years:this.props.years}))}}},{key:"updateSelectedTab",value:function(e){this.props.updateSelectedTab(this.state.node_data,[])}},{key:"mouseUpGlobal",value:function(e){this.setState(Object(y.a)(Object(y.a)({},this.state),{},{slide_on:!1}))}},{key:"handleSimulate",value:function(e){var t=this;this.setState(Object(y.a)(Object(y.a)({},this.state),{},{slide_on:!0}),function(){t.props.handleSimulate(e)})}},{key:"filter_links_by_nodes",value:function(e,t){return t.filter(function(t){var n=e.reduce(function(e,n){return e||t.source===n.UID},!1);return n=n&&e.reduce(function(e,n){return e||t.target===n.UID},!1)})}},{key:"updateTagSelection",value:function(e){var t=this,n=K.nodes.filter(function(t){return e.filter(function(e){return new Set(t.tags.map(function(e){return e.name})).has(e)}).length>0||0===e.length});this.setState(Object(y.a)(Object(y.a)({},this.state),{},{node_data:n,link_data:this.filter_links_by_nodes(n,K.links)}),function(){t.updateSelectedTab()})}},{key:"render",value:function(){return o.a.createElement(o.a.Fragment,null,o.a.createElement(k.a,{subtitle:o.a.createElement("span",null,"This network shows how papers here point to each other. Hover over a node to see its identity. The connections are parsed automatically from PDFs."," ",o.a.createElement("strong",null,"This process is somewhat noisy.")," We hope to improve it over time."),title:"Citation Network",kind:"info",lowContrast:!0}),o.a.createElement("div",{className:"bx--container",onMouseUp:this.mouseUpGlobal.bind(this)},o.a.createElement("div",{className:"bx--row"},o.a.createElement(H,{data:this.state.node_data,years:this.state.years,handleSimulate:this.handleSimulate.bind(this),slide_on:this.state.slide_on})),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("div",{ref:this.ref,style:{height:"50vh"}},o.a.createElement(Y,{data:{nodes:this.state.node_data,links:this.state.link_data},width:this.state.width,height:this.state.height})),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("div",{className:"bx--container"},o.a.createElement("div",{className:"bx--row"},o.a.createElement("div",{className:"bx--col-lg-12"},o.a.createElement(L,{data:this.state.node_data,updateTagSelection:this.updateTagSelection.bind(this)}))))))}}]),n}(o.a.Component),X=n(161),Q=n.n(X),$=n(164),ee=n.n($),te=n(128),ne=n(11),ae=n(467),oe=n(468),ie=n(469),re=n(486),se=n(450),le=n(470),ce=n(477),he=n(487),me=n(478),de=n(488),pe=n(108),ue=pe.views.filter(function(e){return"Taxonomy"===e.name})[0],ge=n(179),fe=2,be=function(e){Object(c.a)(n,e);var t=Object(h.a)(n);function n(e){var a;return Object(s.a)(this,n),(a=t.call(this,e)).onClickModalNode=function(e){a.setState(Object(y.a)(Object(y.a)({},a.state),{},{modal:e}))},a.onClickModalClose=function(e){a.setState(Object(y.a)(Object(y.a)({},a.state),{},{modal:!1}))},a.handleSliderChange=function(e){a.setState(Object(y.a)(Object(y.a)({},a.state),{},{config:Object(y.a)(Object(y.a)({},a.state.config),{},{slider:e.value})}))},a.getPapersWithTag=function(e){return a.state.paper_data.filter(function(t){return t.tags.map(function(e){return j(e)}).indexOf(j(e,a.state.taxonomy_data))>-1})},a.tranformData2Tree=function(e){var t=!0,n=a.state.config.plot_options.level,o=a.state.config.plot_options.level+1,i=a.state.taxonomy_data.slice(n-1,o),r=i[0].map(function(e,n){var o=i[1].filter(function(t){return t.parent===e.name}).map(function(e){var n=a.getPapersWithTag(e).length;return t=t&&Boolean(n),{name:e.name,value:n||0,showLabel:!0}});return{name:e.name,children:o}});a.setState(Object(y.a)(Object(y.a)({},a.state),{},{config:Object(y.a)(Object(y.a)({},a.state.config),{},{plot_options:Object(y.a)(Object(y.a)({},a.state.config.plot_options),{},{draw_treemap:!1,draw_circlemap:!1})})}),function(){a.setState(Object(y.a)(Object(y.a)({},a.state),{},{taxonomy_data_fancy:r,config:Object(y.a)(Object(y.a)({},a.state.config),{},{plot_options:Object(y.a)(Object(y.a)({},a.state.config.plot_options),{},{draw_treemap:!0,draw_circlemap:!0})})}))})},a.determineExpandButton=function(e){return e?ne.u:b.O},a.onClimb=function(e){var t=a.state.config.plot_options.level;t+="climb-up"===e.currentTarget.name?1:-1,t=Math.min(a.state.taxonomy_data.length-1,t),t=Math.max(1,t),a.setState(Object(y.a)(Object(y.a)({},a.state),{},{config:Object(y.a)(Object(y.a)({},a.state.config),{},{plot_options:Object(y.a)(Object(y.a)({},a.state.config.plot_options),{},{level:t})})}),function(){a.tranformData2Tree()})},a.ref=o.a.createRef(),a.state={data:ge,active_tab:ue.default_tab,taxonomy_data:[],paper_data:[],taxonomy_data_fancy:[],modal:!1,config:{nodeHeight:50,nodeWidth:200,nodeGapHoriontal:250,nodeGapVertical:100,slider:12,vertical_offset:0,plot_options:{title:"",draw_treemap:!0,draw_circlemap:!0,level:fe,canvasZoom:{enabled:!0},height:"400px",width:"100%",axis:{legend:{position:"TOP"}}},modal_timeline:{legend:{enabled:!1},grid:{x:{enabled:!1},y:{enabled:!1}},axes:{left:{mapsTo:"year",scaleType:"labels"},top:{mapsTo:"value"}},height:"1000px",width:"90%"}}},a}return Object(l.a)(n,[{key:"componentDidMount",value:function(e){var t=this;if(this.ref.current){var n=this.ref.current.offsetWidth;this.setState(Object(y.a)(Object(y.a)({},this.state),{},{config:Object(y.a)(Object(y.a)({},this.state.config),{},{nodeWidth:n/5,nodeGapHoriontal:n/4})}),function(){t.switchTabs(t.state.active_tab)})}}},{key:"componentDidUpdate",value:function(e,t){var n=this;this.props.props!==e.props&&this.setState(Object(y.a)(Object(y.a)({},this.state),{},{paper_data:this.props.props,years:this.props.years}),function(){n.tranformData2Tree()})}},{key:"switchTabs",value:function(e){var t=this,n=ge.filter(function(t){return t.name===e})[0],a=ue.tabs.filter(function(t){return t.tab_name===e})[0],o=n.taxonomy.map(function(e,t){var a=[];return e.forEach(function(e){z(e,n.taxonomy).length||(e.expanded=!1),a.push(e)}),a}),i=a.fancy_chart_default_level?a.fancy_chart_default_level:Math.min(fe,o.length-1);this.setState(Object(y.a)(Object(y.a)({},this.state),{},{active_tab:e,taxonomy_data:o,paper_data:n.data,config:Object(y.a)(Object(y.a)({},this.state.config),{},{vertical_offset:a.taxonomy.columns.start,plot_options:Object(y.a)(Object(y.a)({},this.state.config.plot_options),{},{level:i})})}),function(){t.updateSelectedTab(),t.tranformData2Tree()})}},{key:"updateSelectedTab",value:function(e){this.props.updateSelectedTab(this.state.paper_data,this.state.taxonomy_data)}},{key:"updateSelectedTags",value:function(e){this.props.updateSelectedTags(function(e){var t=e.split("-child-of-");return{name:t[0],parent:t[1]}}(e.currentTarget.id))}},{key:"onClickExpandNode",value:function(e){var t=this,n=[],a=this.state.taxonomy_data.map(function(a,o){return a.map(function(a,i){j(a)===j(e)&&(a.expanded=!a.expanded,n.push(a));var r=null;if(o>0&&o<t.state.taxonomy_data.length-1&&(r=t.state.taxonomy_data[o-1].find(function(e){return e.name===a.parent})),r){var s=n.find(function(e){return j(e)===j(r)});s&&(a.expanded=s.expanded,n.push(a))}return a})});this.setState(Object(y.a)(Object(y.a)({},this.state),{},{taxonomy_data:a}))}},{key:"getTimeline",value:function(e){var t=this,n=(new Date).getFullYear(),a=Object(T.a)(Array(n+1-pe.min_year).keys()).map(function(e){return e+pe.min_year}),o={},i=this.state.paper_data.filter(function(e){return e.tags.map(function(e){return j(e)}).indexOf(j(t.state.modal))>-1});return a.forEach(function(e){o[e]=0}),i&&i.forEach(function(e){o[parseInt(e.year)]+=1}),o=Object.keys(o).map(function(e){return{year:e,value:o[e]}})}},{key:"renderParents",value:function(e){var t=this,n=C(e,this.state.taxonomy_data);return n.length?o.a.createElement(ae.a,{noTrailingSlash:!0},n.map(function(e,n){return e.map(function(e,n){return o.a.createElement(oe.a,{className:"hover-cursor",onClick:t.onClickModalNode.bind(t,e),key:n},e.name)})}),o.a.createElement(oe.a,{isCurrentPage:!0},e.name)):[]}},{key:"toggleLevel",value:function(e,t){var n=this,a=this.state.taxonomy_data.map(function(a,o){var i=a;return o>=e&&o<n.state.taxonomy_data.length-1&&(i=a.map(function(e){return e.expanded=t,e})),i});this.setState(Object(y.a)(Object(y.a)({},this.state),{},{taxonomy_data:a}))}},{key:"render",value:function(){var e=this,t={},n=this.state.taxonomy_data.map(function(n,a){var i=0;return n.map(function(r,s){var l=null;if(a>0&&(l=e.state.taxonomy_data[a-1].find(function(e){return e.name===r.parent})),!l||l.expanded){var c=a+1,h=n.filter(function(e,t){return t<s}).filter(function(e){return e.expanded}),m=n.filter(function(e,t){return t<s}).filter(function(e){return!e.expanded}),d=0;if(0===h.length&&r.parent){var p=C(r,e.state.taxonomy_data),u=p[p.length-1][0];d=t[j(u)].temp_y+n.filter(function(e){return e.parent===u.name}).indexOf(r)}else{for(;c<e.state.taxonomy_data.length;){var g=[];e.state.taxonomy_data[c].filter(function(e){return new Set(h.map(function(e){return e.name})).has(e.parent)}).forEach(function(e){e.expanded?g.push(e):m.push(e)}),h=g,c++}d=m.length}i&&d===i&&(d+=n.indexOf(r)),i=d;var f=(r.level-1)*e.state.config.nodeGapHoriontal,b=(d+.5)*e.state.config.nodeGapVertical;return t[j(r)]={temp_y:d,y:b},o.a.createElement("foreignObject",{style:{overflow:"visible"},key:"node_".concat(s),transform:"translate(".concat(f,", ").concat(b,")"),height:e.state.config.nodeHeight,width:e.state.config.nodeWidth},o.a.createElement(o.a.Fragment,null,o.a.createElement(ee.a,{onClick:e.onClickModalNode.bind(e,r)},o.a.createElement($.CardNodeColumn,null,o.a.createElement($.CardNodeTitle,{className:"text-overflow"},r.name))),z(r,e.state.taxonomy_data).length>0&&o.a.createElement(S.a,{onClick:e.onClickExpandNode.bind(e,r),kind:"ghost",className:"expand-button",renderIcon:e.determineExpandButton(r.expanded),iconDescription:"Expand",hasIconOnly:!0}),o.a.createElement(ie.a,{labelText:o.a.createElement("span",{className:"text-blue"},e.getPapersWithTag(r).length),id:j(r,e.state.taxonomy_data),onClick:e.updateSelectedTags.bind(e)})))}return o.a.createElement("span",{key:s})})}),a=this.state.taxonomy_data.filter(function(e,t){return t>0}).map(function(n,a){return n.map(function(n,i){var r=0,s=0,l=e.state.taxonomy_data[a].find(function(e){return e.name===n.parent});if(l&&l.expanded){r=(l.level-1)*e.state.config.nodeGapHoriontal,s=t[j(l)].y;var c={x:r-1+e.state.config.nodeWidth/2,y:s+e.state.config.nodeHeight/2},h={x:(n.level-1)*e.state.config.nodeGapHoriontal,y:t[j(n)].y+e.state.config.nodeHeight/2};return o.a.createElement(Q.a,{key:"link_".concat(i),source:c,target:h,path:Object(te.buildElbowPathString)(c,h),variant:"dash-md"})}return o.a.createElement("span",{key:i})})}),i=this.state.taxonomy_data.filter(function(t,n){return n<e.state.taxonomy_data.length-1}).map(function(t,n){var a=n*e.state.config.nodeGapHoriontal,i=!0;return n>0&&(i=e.state.taxonomy_data[n-1].reduce(function(e,t){return e=e||t.expanded},!1)),i?o.a.createElement("foreignObject",{style:{overflow:"visible"},key:n,transform:"translate(".concat(a,", ").concat(0,")"),height:e.state.config.nodeHeight,width:e.state.config.nodeWidth},o.a.createElement(re.a,{key:n,labelA:"Expand",labelB:"Collapse",onToggle:e.toggleLevel.bind(e,n),size:"sm","aria-label":"expand or collapse level",defaultToggled:!0,id:n.toString()})):o.a.createElement("span",{key:n})}),r=ue.tabs.map(function(t,r){return o.a.createElement("div",{className:"tab-content"},t.title_text&&o.a.createElement(o.a.Fragment,null,o.a.createElement("h6",null,t.title_text),o.a.createElement("br",null)),t.tab_name===e.state.active_tab&&o.a.createElement("div",null,o.a.createElement(se.a,{align:"start"},t.fancy_chart&&o.a.createElement(le.a,{title:"Treemap View",className:"full-accordion",open:!0},o.a.createElement("div",{className:"bx--container"},o.a.createElement("div",{className:"bx--row"},o.a.createElement("div",{style:{borderRight:"1pt solid silver"}},o.a.createElement("h6",null,"Relative Zoom"),o.a.createElement(D.a,{hideTextInput:!0,id:"slider",max:16,min:0,step:1,onChange:e.handleSliderChange.bind(e),value:e.state.config.slider})),o.a.createElement("div",{style:{marginLeft:"10px"}},o.a.createElement("div",null,o.a.createElement("h6",{style:{marginBottom:"5px"}},"Climb Hierarchy"),o.a.createElement(S.a,{onClick:e.onClimb.bind(e),name:"climb-down",kind:"ghost",className:"navigation-buttons",renderIcon:v.Y,iconDescription:"Navigate Up",size:"sm",disabled:1===e.state.config.plot_options.level,hasIconOnly:!0}),o.a.createElement(S.a,{onClick:e.onClimb.bind(e),name:"climb-up",kind:"ghost",className:"navigation-buttons",renderIcon:v.db,iconDescription:"Navigate Down",size:"sm",disabled:e.state.config.plot_options.level===e.state.taxonomy_data.length-1,hasIconOnly:!0})))),o.a.createElement("div",{className:"bx--row"},o.a.createElement("div",{className:"bx--col-lg-"+e.state.config.slider},e.state.config.plot_options.draw_treemap&&o.a.createElement(E.TreemapChart,{data:e.state.taxonomy_data_fancy,options:e.state.config.plot_options})),o.a.createElement("div",{className:"bx--col-lg-"+(16-e.state.config.slider).toString()},e.state.config.plot_options.draw_circlemap&&o.a.createElement(E.CirclePackChart,{data:e.state.taxonomy_data_fancy,options:e.state.config.plot_options},">"))))),o.a.createElement(le.a,{title:"Hierarchy View",className:"full-accordion",open:!0},o.a.createElement("div",{ref:e.ref},o.a.createElement("svg",{height:"10000px",width:"100%"},i,a,n)))),o.a.createElement(ce.a,{modalHeading:e.renderParents(e.state.modal),modalLabel:"Taxonomy View of VAM-HRI Interation Design Elements",passiveModal:!0,hasScrollingContent:!0,open:Boolean(e.state.modal),onRequestClose:e.onClickModalClose.bind(e),size:"lg","aria-label":"",style:{height:"100%"}},o.a.createElement("div",{className:"bx--container"},o.a.createElement("div",{className:"bx--row"},o.a.createElement("div",{className:"bx--col-lg-10"},e.state.modal&&o.a.createElement(o.a.Fragment,null,o.a.createElement("h4",null,o.a.createElement("span",{style:{color:"gray"}},"Category: ")," ",e.state.modal.name),o.a.createElement("hr",null),e.state.modal.abstract&&o.a.createElement(o.a.Fragment,null,o.a.createElement("p",null,e.state.modal.abstract),o.a.createElement("br",null),o.a.createElement("br",null))),z(e.state.modal,e.state.taxonomy_data).length>0&&o.a.createElement(o.a.Fragment,null,"Children:"," ",z(e.state.modal,e.state.taxonomy_data).map(function(t,n){return o.a.createElement("span",{key:n},n>0&&" | ",o.a.createElement(R.a,{onClick:e.onClickModalNode.bind(e,t)},t.name))})),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement(S.a,{style:{marginRight:"10px"},kind:"primary",size:"field",renderIcon:g.Z,iconDescription:"Add",href:pe.metadata.primary_link,target:"_blank"},"Read More"),o.a.createElement(S.a,{kind:"tertiary",renderIcon:f.g,size:"field",href:pe.metadata.link_to_contribute,target:"_blank"},"Contribute"),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement(he.e,null,o.a.createElement(he.c,null,o.a.createElement(he.d,null,o.a.createElement(he.b,{head:!0},"Papers in this Category"))),o.a.createElement(he.a,null,e.state.paper_data.filter(function(t){return t.tags.map(function(e){return j(e)}).indexOf(j(e.state.modal))>-1}).map(function(e){return o.a.createElement(he.d,{key:e.UID},o.a.createElement(he.b,null,o.a.createElement(_,{paper:e})),o.a.createElement(he.b,null))})))),o.a.createElement("div",{className:"bx--col-lg-6"},e.state.modal&&o.a.createElement(E.SimpleBarChart,{data:e.getTimeline(),options:e.state.config.modal_timeline})))))))});return o.a.createElement("div",null,1===ue.tabs.length&&o.a.createElement(o.a.Fragment,null,r[0]),ue.tabs.length>1&&o.a.createElement(me.a,{scrollIntoView:!1,selected:ue.tabs.map(function(e){return ue.default_tab===e.tab_name}).indexOf(!0)},ue.tabs.map(function(t,n){return o.a.createElement(de.a,{key:n,id:t.tab_name,label:t.tab_name,disabled:t.disabled,onClick:e.switchTabs.bind(e,t.tab_name)},r[n])})))}}]),n}(o.a.Component),ye=n(15),ve=n(472),we=n(475),Ie=n(453),ke=n(476),Se=n(187),Te=n.n(Se),Re=n(108),xe=Re.views.filter(function(e){return"Insights"===e.name})[0],De=n(402),Ae=n(179),Ee=(Ae=Ae.find(function(e){return e.name===Re.views.find(function(e){return"Taxonomy"===e.name}).default_tab})).data,Me=Ae.taxonomy,Oe=[];Me.forEach(function(e,t){e.forEach(function(e,t){var n=[];e.parent?Oe.forEach(function(t,n){if(e.parent===t[t.length-1]){var a=t.map(function(e){return e});a.push(e.name),Oe.push(a)}}):(n.push(e.name),Oe.push(n))})}),Oe=Oe.map(function(e,t){return{id:t,text:e.join(" > ")}});var Ue=function(e){Object(c.a)(n,e);var t=Object(h.a)(n);function n(e){var a;return Object(s.a)(this,n),(a=t.call(this,e)).ref=o.a.createRef(),a.state={embeddings:De,paper_data:Ee,imagination:e.data,loading:!1,error:!1,rendered:!1},a}return Object(l.a)(n,[{key:"componentDidUpdate",value:function(e,t){var n=this;this.props.data!==e.data&&this.setState(Object(y.a)(Object(y.a)({},this.state),{},{paper_data:this.props.paper_data,imagination:this.props.data}),function(){n.componentDidMount()})}},{key:"componentDidMount",value:function(){var e=this;this.setState(Object(y.a)(Object(y.a)({},this.state),{},{loading:!0}),function(){fetch(Re.link_to_server+"/embeddings",{method:"POST",headers:{Accept:"application/json","Content-Type":"application/json","Access-Control-Allow-Origin":"*"},body:JSON.stringify(e.state)}).then(function(e){return e.json()}).then(function(t){var n=e.ref.current.offsetHeight,a=e.ref.current.offsetWidth,o=Math.min.apply(Math,Object(T.a)(t.map(function(e){return e.x}))),i=Math.max.apply(Math,Object(T.a)(t.map(function(e){return e.x}))),r=Math.min.apply(Math,Object(T.a)(t.map(function(e){return e.y}))),s=Math.max.apply(Math,Object(T.a)(t.map(function(e){return e.y}))),l=t.map(function(e){var t=e;return t.x=.75*a*(-o+e.x)/(i-o),t.y=.75*n*(-r+e.y)/(s-r),t}),c=Ee.map(function(t){var n=l.filter(function(e){return e.UID===t.UID})[0],a=t;return a.x=50+e.applyScalingX(1,n.x),a.y=50+e.applyScalingY(1,n.y),a.selected=!1,a}),h={UID:0},m=l.filter(function(e){return 0===e.UID})[0];h.x=50+e.applyScalingX(1,m.x),h.y=50+e.applyScalingY(1,m.y),e.setState(Object(y.a)(Object(y.a)({},e.state),{},{paper_data:c,new_paper:h,loading:!1,rendered:!0}))}).catch(function(t){e.setState(Object(y.a)(Object(y.a)({},e.state),{},{loading:!1,error:!0}))})})}},{key:"applyScalingX",value:function(e,t){return e*t}},{key:"applyScalingY",value:function(e,t){return e*t}},{key:"selectNode",value:function(e,t){window.scrollTo({top:t.pageY/1.5,behavior:"smooth"});var n=this.state.imagination.neighbors.map(function(t,n){var a=t;return a.selected=!t.selected&&t.UID===e,a});this.setState(Object(y.a)(Object(y.a)({},this.state),{},{imagination:Object(y.a)(Object(y.a)({},this.state.imagination),{},{neighbors:n})}))}},{key:"render",value:function(){var e=this,t=null,n=[],a=[];return this.state.rendered&&(a=this.state.paper_data.map(function(t,n){return o.a.createElement("foreignObject",{key:n,style:{overflow:"visible"},transform:"translate(".concat(t.x,", ").concat(t.y,")")},o.a.createElement(Te.a,{id:t.UID,size:10,onClick:e.selectNode.bind(e,t.UID),renderIcon:o.a.createElement(g.ad,null),className:e.state.imagination.neighbors.map(function(e){return e.UID}).indexOf(t.UID)>-1?"selected-circle":"unselected-circle"}))}),this.state.new_paper&&(t=o.a.createElement("foreignObject",{key:0,style:{overflow:"visible"},transform:"translate(".concat(this.state.new_paper.x,", ").concat(this.state.new_paper.y,")")},o.a.createElement(Te.a,{id:0,size:20,onClick:this.selectNode.bind(this,this.state.new_paper.UID),renderIcon:o.a.createElement(g.ad,null),className:"special-circle"})),n=this.state.paper_data.filter(function(t){return e.state.imagination.neighbors.map(function(e){return e.UID}).indexOf(t.UID)>-1}).map(function(t,n){var a=JSON.parse(JSON.stringify(t)),i=JSON.parse(JSON.stringify(e.state.new_paper));return a.x=a.x+5,a.y=a.y+5,i.x=i.x+10,i.y=i.y+10,o.a.createElement(Q.a,{key:"link_".concat(n),source:a,target:i,variant:"dash-md"})}))),o.a.createElement(o.a.Fragment,null,o.a.createElement("div",{className:"bx--col-lg-16"},o.a.createElement("br",null),o.a.createElement("div",{className:"bx--row",style:{width:"75%"}},o.a.createElement(x.a,null,o.a.createElement("p",{style:{fontSize:"inherit"}},"This is a paper, described in terms of the tags in this taxonomy, that does not exist yet! It is shown below in"," ",o.a.createElement("span",{style:{color:"green"}},"green")," in"," ",o.a.createElement("em",null,'"tag space"'),"."))),o.a.createElement("br",null),this.state.imagination.key_map.map(function(e,t){var n=e.split(" > "),a=n.map(function(e,t){return o.a.createElement("div",{key:t},o.a.createElement(A.a,{size:"sm",type:t===n.length-1?"green":"gray",title:e},e),t!==n.length-1&&o.a.createElement(ye.Yb,{className:"label-connector"}))});return o.a.createElement("div",{key:t,className:"bx--row"},a)}),o.a.createElement("br",null),o.a.createElement("br",null),this.state.loading&&o.a.createElement(ve.a,{description:"Loading new paper embeddings..."}),this.state.error&&o.a.createElement(o.a.Fragment,null,o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement(k.b,{lowContrast:!0,subtitle:o.a.createElement("span",null,"There was an error rendering the new paper embedding. Please report a bug"," ",o.a.createElement(R.a,{href:Re.metadata.link_to_code+"/issues",target:"_blank"},"here"),"."),title:"ERROR"})),o.a.createElement("div",{ref:this.ref,style:{height:"30vh"}},!this.state.loading&&!this.state.error&&o.a.createElement("svg",{height:"100%",width:"100%"},n,a,t))),o.a.createElement(he.e,{ariaLabel:"Neighboring Papers"},o.a.createElement(he.c,null,o.a.createElement(he.d,null,o.a.createElement(he.b,{head:!0},"Neighboring Papers"))),o.a.createElement(he.a,null,o.a.createElement(he.d,null,o.a.createElement(he.b,null,"To get to this new paper, our AI thinks you should be looking at the following papers known to our system as the state of the art that immediately makes the new work possible. Each paper is tagged with features that need relaxation or extension to get to the new paper.",this.state.paper_data.length>0&&o.a.createElement(he.a,null,this.state.imagination.neighbors.map(function(t,n){return o.a.createElement(he.d,{key:n,className:n===e.state.imagination.neighbors.length-1?"no-bottom-border":""},o.a.createElement(he.b,{className:Boolean(t.selected)?"text-blue":"",style:{width:"20%"}},o.a.createElement(_,{paper:e.state.paper_data.filter(function(e,n){return e.UID===t.UID})[0]})),o.a.createElement(he.b,{style:{width:"80%"}},t.transforms.map(function(e,n){var a=e.key.split(" > ").map(function(e,n){return o.a.createElement(oe.a,{key:n,isCurrentPage:!t.selected},e)});return o.a.createElement(ae.a,{style:{marginBottom:"5px"},key:n,noTrailingSlash:!0},a,o.a.createElement(oe.a,null,e.value?o.a.createElement("span",{className:"text-blue"},"True"):o.a.createElement("span",{style:{color:"red"}},"False")))})))})))))))}}]),n}(o.a.Component),je={Taxonomy:be,Network:Z,Affinity:q,Insights:function(e){Object(c.a)(n,e);var t=Object(h.a)(n);function n(e){var a;return Object(s.a)(this,n),(a=t.call(this,e)).changeNumePapers=function(e){a.setState(Object(y.a)(Object(y.a)({},a.state),{},{num_papers:parseInt(e.imaginaryTarget.value)}))},a.logPaperSelection=function(e){a.setState(Object(y.a)(Object(y.a)({},a.state),{},{selected_papers:e.map(function(e){return e.id})}))},a.logTagSelection=function(e){a.setState(Object(y.a)(Object(y.a)({},a.state),{},{selected_tags:e.map(function(e){return e.text})}))},a.state={view:Re.default_view,paper_data:Ee,taxonomy_data:Me,pageID:1,pageMAX:1,new_papers:[],selected_papers:[],selected_tags:[],num_papers:1,loading:!1,error:!1},a.updateSelectedTab(),a}return Object(l.a)(n,[{key:"componentDidUpdate",value:function(e,t){this.props.props!==e.props&&this.setState(Object(y.a)(Object(y.a)({},this.state),{},{paper_data:this.props.props}))}},{key:"updateSelectedTab",value:function(e){this.props.updateSelectedTab(this.state.paper_data,[])}},{key:"imaginePapers",value:function(e){var t=this;this.setState(Object(y.a)(Object(y.a)({},this.state),{},{loading:!0,error:!1,new_papers:[]}),function(){var e={paper_data:t.state.paper_data.map(function(e,t){var n=JSON.parse(JSON.stringify(e)),a=[];return n.tags.forEach(function(e){var t=[];e.parent&&(t=[e.parent]),t.push(e.name);var n=t.join(" > ");Oe.forEach(function(e){if(e.text.endsWith(n))return a.push(e.text),!1})}),n.tag_chain=a,n}),selected_papers:t.state.selected_papers.length?t.state.selected_papers:t.state.paper_data.map(function(e){return e.UID}),selected_tags:t.state.selected_tags.length?t.state.selected_tags:Oe,num_papers:t.state.num_papers,domain:Re.metadata.acronym};fetch(Re.link_to_server+"/imagine",{method:"POST",headers:{Accept:"application/json","Content-Type":"application/json","Access-Control-Allow-Origin":"*"},body:JSON.stringify(e)}).then(function(e){return e.json()}).then(function(e){t.setState(Object(y.a)(Object(y.a)({},t.state),{},{new_papers:e,loading:!1}))}).catch(function(e){t.setState(Object(y.a)(Object(y.a)({},t.state),{},{loading:!1,error:!0}))})})}},{key:"render",value:function(){var e=this;return o.a.createElement(o.a.Fragment,null,o.a.createElement("div",{className:"bx--grid bx--grid--full-width",style:{width:"100%",minHeight:"100vh"}},o.a.createElement(se.a,{align:"start"},o.a.createElement(le.a,{title:"Tell me topics that do not have any papers!"},o.a.createElement(k.a,{lowContrast:!0,hideCloseButton:!0,kind:"error",title:"Coming soon!"})),o.a.createElement(le.a,{title:"What are topics that have the least number of papers?"},o.a.createElement(k.a,{lowContrast:!0,hideCloseButton:!0,kind:"error",title:"Coming soon!"})),o.a.createElement(le.a,{title:"What are most popular topics?"},o.a.createElement(k.a,{lowContrast:!0,hideCloseButton:!0,kind:"error",title:"Coming soon!"})),o.a.createElement(le.a,{title:"Search papers using tags"},o.a.createElement("p",null,"You can search papers interactively using tags in the"," ",o.a.createElement(R.a,{href:"/"},"home")," page.")),o.a.createElement(le.a,{className:"whats-next",title:o.a.createElement(o.a.Fragment,null,"What should I work on next?! \ud83e\udd13"),open:!0},o.a.createElement("p",{style:{fontSize:"inherit"}},"In her"," ",o.a.createElement(R.a,{href:"https://ojs.aaai.org/index.php/aimagazine/article/view/18149",target:"_blank"},"AAAI 2020 presidential address"),", Yolanda Gil asked:"," ",o.a.createElement("em",null,'"Will AI write the scientific papers of the future?"')," to put into context the outsized impact that AI is beginning to have on the scientific process. This section builds on this theme and uses an AI constraint solver to imagine new papers yet unwritten. Learn more about it"," ",o.a.createElement(R.a,{href:"",target:"_blank"},"here"),"."),o.a.createElement("br",null),o.a.createElement("br",null),xe.interactive&&o.a.createElement(o.a.Fragment,null,o.a.createElement(we.a,{helperText:"You can make the new paper search focus on papers of interest. If nothing is selected, the system will work with all the papers.",id:"multiselect-paper",itemToString:function(e){return e?e.text:""},items:this.state.paper_data.map(function(e,t){return{id:t,text:e.title+" by "+e.authors}}),label:"List of papers",titleText:o.a.createElement(o.a.Fragment,null,o.a.createElement("span",{style:{color:"red"}},"Optional")," Select list of papers you want to focus on"),initialSelectedItems:this.state.selected_papers,onChange:function(t){e.logPaperSelection(t.selectedItems)}}),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement(we.a,{helperText:"You can make the new paper search focus on tags of interest. If nothing is selected, the system will work with all the tags.",id:"multiselect-tags",itemToString:function(e){return e?e.text:""},items:Oe,label:"List of tags",titleText:o.a.createElement(o.a.Fragment,null,o.a.createElement("span",{style:{color:"red"}},"Optional")," Select list of tags you want to focus on"),initialSelectedItems:this.state.selected_tags,onChange:function(t){e.logTagSelection(t.selectedItems)}}),o.a.createElement("br",null),o.a.createElement("br",null)),o.a.createElement("div",{className:"bx--row"},o.a.createElement("div",{className:"bx--col-lg-4"},o.a.createElement(I.a,{helperText:o.a.createElement(o.a.Fragment,null,o.a.createElement("span",{style:{color:"red"}},"Optional")," Number of papers"),id:"num-papers",invalidText:"Number is not valid",max:5,min:1,step:1,value:this.state.num_papers,onChange:this.changeNumePapers.bind(this)})),o.a.createElement("div",{className:"bx--col-lg-4"},o.a.createElement(S.a,{kind:"primary",size:"field",onClick:this.imaginePapers.bind(this)},"What's Next"))),this.state.loading&&o.a.createElement("div",{style:{padding:"50px"}},o.a.createElement(Ie.a,{style:{margin:"0 auto"},description:"Active loading indicator",withOverlay:!1})),this.state.error&&o.a.createElement(o.a.Fragment,null,o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement(k.b,{lowContrast:!0,subtitle:o.a.createElement("span",null,"There was an error contacting the server. Please report a bug"," ",o.a.createElement(R.a,{href:Re.metadata.link_to_code+"/issues",target:"_blank"},"here"),"."),title:"ERROR"})),this.state.new_papers.length>0&&o.a.createElement("div",null,o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement(ke.a,{backwardText:"Previous paper",forwardText:"Next paper",itemsPerPageText:"Papers per page:",page:1,pageSize:1,pageSizes:Object(T.a)(Array(this.state.new_papers.length).keys()).map(function(e){return e+1}),size:"md",totalItems:this.state.new_papers.length,onChange:function(t){e.setState(Object(y.a)(Object(y.a)({},e.state),{},{pageID:t.page,pageMAX:t.pageSize}))}}),Object(T.a)(Array(this.state.new_papers.length).keys()).map(function(t,n){return n>=(e.state.pageID-1)*e.state.pageMAX&&n<e.state.pageID*e.state.pageMAX?o.a.createElement(Ue,{key:n,paper_data:e.state.paper_data,data:e.state.new_papers[n]}):null})),o.a.createElement("br",null),o.a.createElement("br",null)))))}}]),n}(o.a.Component)},Ce=n(108),ze=(new Date).getFullYear(),_e=function(e){Object(c.a)(n,e);var t=Object(h.a)(n);function n(e){var a;return Object(s.a)(this,n),(a=t.call(this,e)).refreshData=function(e){var t=a.state.cached_paper_data.filter(function(e){return a.state.tags.filter(function(t){return new Set(e.tags.map(function(e){return j(e)})).has(j(t))}).length>0||0===a.state.tags.length}).filter(function(e){if(!a.state.search)return!0;var t=a.state.search.trim().toLowerCase().split("||"),n=[e.title,e.authors,e.venue,e.year,e.abstract].join(" ").toLowerCase().replaceAll(","," ").replaceAll("."," "),o=new Set(n.split(/\s+/));return t.reduce(function(e,t){var a=t.trim().split(/\s+/).map(function(e){return e.trim()});return t&&(e=e||n.indexOf(t)>-1||a.filter(function(e){return o.has(e)}).length===a.length),e},!1)}).filter(function(e){return parseInt(e.year)>=a.state.years.min_val&&parseInt(e.year)<=a.state.years.max_val&&parseInt(e.year)<=a.state.years.cur_val}),n=t.length;a.setState(Object(y.a)(Object(y.a)({},a.state),{},{paper_data:t.map(function(e){return e.selected=!0,e}),number:n}))},a.updateSelectedTab=function(e,t){var n=e.reduce(function(e,t){return t.year<e&&(e=t.year),e},a.state.years.max_max),o=e.reduce(function(e,t){return t.year>e&&(e=t.year),e},a.state.years.min_min);a.setState(Object(y.a)(Object(y.a)({},a.state),{},{taxonomy_data:t,paper_data:e,cached_paper_data:e,search:"",tags:[],number:e.length,years:Object(y.a)(Object(y.a)({},a.state.years),{},{min_min:n,max_max:o,min_val:n,max_val:o})}))},a.updateSelectedTags=function(e){var t=a.state.tags,n=t.map(function(e){return j(e)}).indexOf(j(e));n>-1?t.splice(n,1):t.push(e),a.setState(Object(y.a)(Object(y.a)({},a.state),{},{tags:t}),function(){a.refreshData()})},a.handleInputChange=function(e){a.setState({search:e.target.value},function(){a.refreshData()})},a.sortYear=function(e){var t="decreasing"===e.currentTarget.name?-1:1,n=a.state.paper_data;n.sort(function(e,n){return e.year<=n.year?-1*t:1*t}),a.setState(Object(y.a)(Object(y.a)({},a.state),{},{paper_data:n}))},a.state={taxonomy_data:[],paper_data:[],active_view:e.props,config:Ce,search:"",tags:[],number:0,years:{min_min:1984,min_val:Ce.min_year,max_max:ze,max_val:ze,cur_val:ze}},a}return Object(l.a)(n,[{key:"componentDidUpdate",value:function(e,t){this.props.props!==e.props&&this.setState(Object(y.a)(Object(y.a)({},this.state),{},{active_view:this.props.props,search:"",tags:[]}))}},{key:"handleSimulate",value:function(e){var t=this;this.setState(Object(y.a)(Object(y.a)({},this.state),{},{years:Object(y.a)(Object(y.a)({},this.state.years),{},{cur_val:e})}),function(){t.refreshData()})}},{key:"render",value:function(){var e=this;return o.a.createElement("div",{className:"bx--container"},o.a.createElement("div",{className:"bx--row"},o.a.createElement("div",{className:"bx--col-lg-10"},o.a.createElement(w.a,{light:!0,labelText:"",id:"search",placeholder:"Search",size:"sm",value:this.state.search,onChange:this.handleInputChange.bind(this)}),o.a.createElement("div",{className:"label-text",style:{paddingTop:"5px"}},'The search is case insensitive and looks for an AND of all keywords. Use an "||" for OR semantics.')),o.a.createElement("div",{className:"bx--col-lg-3"},o.a.createElement(I.a,{light:!0,onChange:function(t){e.setState(Object(y.a)(Object(y.a)({},e.state),{},{years:Object(y.a)(Object(y.a)({},e.state.years),{},{min_val:parseInt(t.imaginaryTarget.value)})}),function(){e.refreshData()})},size:"sm",id:"min-year",min:this.state.years.min_min,max:this.state.years.max_val,value:this.state.years.min_val,helperText:o.a.createElement("div",{className:"label-text"},"Earliest date"),invalidText:"Invalid"})),o.a.createElement("div",{className:"bx--col-lg-3"},o.a.createElement(I.a,{light:!0,onChange:function(t){e.setState(Object(y.a)(Object(y.a)({},e.state),{},{years:Object(y.a)(Object(y.a)({},e.state.years),{},{max_val:parseInt(t.imaginaryTarget.value)})}),function(){e.refreshData()})},size:"sm",id:"max-year",min:this.state.years.min_val,max:this.state.years.max_max,value:this.state.years.max_val,helperText:o.a.createElement("div",{className:"label-text"},"Latest date"),invalidText:"Invalid"}))),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("div",{className:"bx--row"},o.a.createElement("div",{className:"bx--col-lg-12"},this.state.config.views.map(function(t,n){if(e.state.active_view===t.name){var a=je[t.name];return t.disabled?o.a.createElement(k.b,{lowContrast:!0,hideCloseButton:!0,key:n,type:"error",subtitle:o.a.createElement("span",null,"The authors have disabled the ",t.name," view. Please check out the other viewing options on the left."),title:"DISABLED"}):o.a.createElement(a,{props:e.state.paper_data,updateSelectedTags:e.updateSelectedTags.bind(e),updateSelectedTab:e.updateSelectedTab.bind(e),handleSimulate:e.handleSimulate.bind(e),years:e.state.years,key:n})}return null})),o.a.createElement("div",{className:"bx--col-lg-4"},o.a.createElement("p",null,"Showing all ",o.a.createElement("span",{className:"text-blue"},this.state.number)," ","papers",Boolean(this.state.search||this.state.tags.length>0)&&o.a.createElement("span",null," with "),this.state.tags.length>0&&o.a.createElement("span",{className:"text-blue"},"selected tags"," ",this.state.tags.map(function(e){return e.parent+":"+e.name}).join(", ")),Boolean(this.state.search&&this.state.tags.length)>0&&o.a.createElement("span",null," and "),this.state.search&&o.a.createElement(o.a.Fragment,null," ","one or more of keywords"," ",o.a.createElement("span",{className:"text-blue"},this.state.search)," in their metadata"),"."),o.a.createElement("br",null),o.a.createElement(S.a,{onClick:this.sortYear.bind(this),name:"decreasing",kind:"ghost",className:"navigation-buttons",renderIcon:v.ub,iconDescription:"Sort down by year",size:"sm",hasIconOnly:!0}),o.a.createElement(S.a,{onClick:this.sortYear.bind(this),name:"increasing",kind:"ghost",className:"navigation-buttons",renderIcon:v.T,iconDescription:"Sort up by year",size:"sm",hasIconOnly:!0}),o.a.createElement(S.a,{kind:"secondary",size:"sm",onClick:this.props.logChange.bind(this,{name:"Insights"})},"Insights"),this.state.paper_data.map(function(e,t){return o.a.createElement(W,{key:t,paper:e})}))))}}]),n}(o.a.Component),We=n(473),He=n(474),Le=n(482),Fe=n(471),Pe=n(108),Ne=function(e){Object(c.a)(n,e);var t=Object(h.a)(n);function n(e){var a;return Object(s.a)(this,n),(a=t.call(this,e)).logChange=function(e){a.setState({view:e.name})},a.state={view:Pe.default_view},a}return Object(l.a)(n,[{key:"render",value:function(){var e=this;return o.a.createElement(o.a.Fragment,null,o.a.createElement("div",{className:"bx--grid bx--grid--full-width",style:{width:"100%",minHeight:"100vh"}},o.a.createElement("div",{className:"bx--row"},o.a.createElement("div",{className:"bx--col-lg-4 sidebar"},o.a.createElement("div",{className:"bx--container"},o.a.createElement(We.a,{onChange:function(t){return e.logChange(t)},size:"sm",selectedIndex:Pe.views.map(function(e){return e.name}).indexOf(this.state.view)},Pe.views.map(function(e,t){return o.a.createElement(He.a,{key:t,name:e.name,text:e.name})})),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("h3",null,Pe.metadata.title_text),o.a.createElement("br",null),o.a.createElement(Le.a,{type:"multi"},Pe.metadata.citation_text),o.a.createElement("br",null),Pe.metadata.info_tile&&o.a.createElement(x.a,null,o.a.createElement("p",{style:{fontSize:"small"}},Pe.metadata.info_text),o.a.createElement("div",{style:{paddingTop:"3px"}},Pe.metadata.info_link.map(function(e,t){return o.a.createElement("span",{key:t}," ",t>0&&"|"," ",o.a.createElement(R.a,{href:e.link,target:"_blank"},e.text))}))),o.a.createElement("br",null),o.a.createElement(Fe.a,{stacked:!0},o.a.createElement(S.a,{kind:"primary",className:"buttonset",size:"field",renderIcon:g.Z,href:Pe.metadata.primary_link,target:"_blank"},"Read"),o.a.createElement("br",null),1===Pe.metadata.secondary_links.length&&o.a.createElement(o.a.Fragment,null,o.a.createElement(S.a,{kind:"tertiary",className:"buttonset",size:"small",renderIcon:g.Z,href:Pe.metadata.secondary_links[0].link,target:"_blank"},"See Also"),o.a.createElement("br",null)),o.a.createElement(S.a,{kind:"tertiary",className:"buttonset tertiary-secondary",renderIcon:f.g,size:"small",href:Pe.metadata.link_to_contribute,target:"_blank"},"Contribute"),o.a.createElement("br",null),Pe.metadata.community_link&&o.a.createElement(o.a.Fragment,null,o.a.createElement(S.a,{kind:"tertiary",className:"buttonset tertiary-danger",renderIcon:f.ob,size:"small",href:Pe.metadata.community_link,target:"_blank"},"Community"),o.a.createElement("br",null)),Pe.metadata.secondary_links.length>1&&o.a.createElement("div",{className:"bx--col-lg-8",style:{padding:0,margin:0,maxWidth:"12.25rem"}},o.a.createElement(se.a,{align:"start"},o.a.createElement(le.a,{className:"see-also-accordion",title:"See also",onClick:function(e){window.scrollTo({top:e.currentTarget.offsetHeight>e.pageY/4?0:e.pageY/2,behavior:"smooth"})}},o.a.createElement(Fe.a,{stacked:!0},o.a.createElement("br",null),o.a.createElement("br",null),Pe.metadata.secondary_links.map(function(e,t){return o.a.createElement("div",{key:t},o.a.createElement(S.a,{target:"_blank",href:e.link,kind:"ghost",renderIcon:b.R},e.name),o.a.createElement("br",null),o.a.createElement("br",null))}))))))),!u.isMobile&&o.a.createElement("div",{className:"footer bx--col-lg-4",style:{backgroundColor:"rgba(255,255,255,0.9)",zIndex:"999"}},o.a.createElement("div",{className:"bx--row"},o.a.createElement("div",{className:"bx--col-lg-16"},o.a.createElement("div",{className:"bx--container"},o.a.createElement("p",{style:{fontSize:"small",marginBottom:"10px",maxWidth:"75%"}},"Follow us on GitHub. Your love",o.a.createElement("br",null)," keeps us going!"," ",o.a.createElement("span",{role:"img","aria-label":"hugging face"},"\ud83e\udd17")),o.a.createElement(p.a,{href:Pe.metadata.link_to_code,"data-size":"small","data-show-count":"true","aria-label":"Star survey-visualizer on GitHub"},"Star")),o.a.createElement("div",{className:"bx--container"},"App built by"," ",o.a.createElement(R.a,{href:"https://twitter.com/tchakra2",target:"_blank"},"tchakra2")))))),o.a.createElement("div",{className:"bx--col-lg-12"},u.isMobile?o.a.createElement("div",{className:"bx--container"},o.a.createElement(k.b,{lowContrast:!0,hideCloseButton:!0,type:"error",subtitle:o.a.createElement("span",null,"This application only runs on a desktop."),title:"Please switch to widescreen."})):o.a.createElement(_e,{props:this.state.view,logChange:this.logChange.bind(this)})))))}}]),n}(o.a.Component),Be=function(e){Object(c.a)(n,e);var t=Object(h.a)(n);function n(){return Object(s.a)(this,n),t.apply(this,arguments)}return Object(l.a)(n,[{key:"render",value:function(){return o.a.createElement(o.a.Fragment,null,o.a.createElement(m.Content,null,o.a.createElement(d.c,null,o.a.createElement(d.a,{exact:!0,path:"/",component:Ne}),o.a.createElement(d.a,{exact:!0,path:"/home",component:Ne}))))}}]),n}(a.Component),qe=n(184);r.a.render(o.a.createElement(qe.a,null,o.a.createElement(Be,null)),document.getElementById("root"))}},[[337,1,2]]]);
//# sourceMappingURL=main.9b5d3d1b.chunk.js.map